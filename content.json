{"meta":{"title":"Vincent's Home","subtitle":"欢迎访问！","description":"本网站用于整理个人学习笔记，发布日记，杂文，所见所想","author":"吴勇胜（Vincent）","url":"http://wysh.site"},"pages":[{"title":"","date":"2076-11-29T08:54:34.955Z","updated":"2018-04-20T15:25:12.000Z","comments":true,"path":"404.html","permalink":"http://wysh.site/404.html","excerpt":"","text":"L2Dwidget.init({\"pluginRootPath\":\"live2dw/\",\"pluginJsPath\":\"lib/\",\"pluginModelPath\":\"assets/\",\"model\":{\"scale\":1,\"hHeadPos\":0.5,\"vHeadPos\":0.618,\"jsonPath\":\"/live2dw/assets/wanko.model.json\"},\"display\":{\"superSample\":2,\"width\":120,\"height\":240,\"position\":\"right\",\"bottom\":-200,\"hOffset\":0,\"vOffset\":-20},\"mobile\":{\"show\":false,\"scale\":0.1},\"react\":{\"opacityDefault\":0.4,\"opacityOnHover\":0.2}});"},{"title":"分类","date":"2018-04-22T12:47:46.000Z","updated":"2018-04-22T12:49:40.000Z","comments":false,"path":"categories/index.html","permalink":"http://wysh.site/categories/index.html","excerpt":"","text":""},{"title":"音乐收藏","date":"2018-04-29T10:43:44.000Z","updated":"2018-09-16T17:02:24.000Z","comments":false,"path":"music/index.html","permalink":"http://wysh.site/music/index.html","excerpt":"","text":""},{"title":"个人简历","date":"2076-11-29T08:54:34.955Z","updated":"2018-09-16T17:03:14.000Z","comments":false,"path":"resume/index.html","permalink":"http://wysh.site/resume/index.html","excerpt":"请输入密码.","text":"欢迎来到我的博客，请输入密码. 密码错误！ No content to display! U2FsdGVkX19YvGdOoJjSfeyg6Q0Q0DQOAq4pveWSTJQDk9KKSo4tiAMqjSwNt5neMxidblsVxsN0C8YXGFxEMO6osuO5kYHXLjmbE/vwbnXnaZaV4shxVD7B1g1Km8CXjYVxcKMfyUfcP0V4Xs57jJyNX+e1XMdEYL78RWXM2SOK0ZAGZPPPN3bl1ysexgk1HNp6+sfIDTeIQAMQVwZSHDOEISCodux+ZnTSn7pMNBedvhbFEWt+UbsGu6t3a7w0BgtpIMqSzrrwGvUfyLdWngP/fvQMbdbfH8gGROBI+s54ueytXzVEFn4f5qLS37uFiUBLi7FOO914485HP012ZOBz1eLi+RnDarL7hUEyNtHXzeCB3aS1MFh5qdRQz3kpTfGpz7NNqvl6JxAz+zy1gjbjynkMUvkX9uQxSzjtp7VuNoiWz7K6lo2zTt1wbQE6gFyG/j4zHZ68FLKk0CUPDiC2zvin8v7MRZYDNQJYk/kXcvFFVpqijZptDiuSrGDAOka/r27llwg3IHy2vkOuEqibjWHWj17XvcvYh6752Z6sxE9W/K6KMcxe7UEM0uem989NeHiPLuLDVi6Z/cr+UqtDw0GlbdKFNresTpAqhdAeIGgk9d9x+eJSnVbdPtJbkp5lbGVTw4ELS9ZrcWfM2zQ49omOGds6mIeiS+KW+bTEbt4ODyROX6iN8k3F8E0qfuPmE2CWOHiCp9A1E+jlZ0NBL0ZpUruW3qL8akieSzR5hhinQS3a0+rYYS47Lre+"},{"title":"标签","date":"2018-04-22T12:45:09.000Z","updated":"2018-04-22T12:46:54.000Z","comments":false,"path":"tags/index.html","permalink":"http://wysh.site/tags/index.html","excerpt":"","text":""},{"title":"关于","date":"2076-11-29T08:54:34.955Z","updated":"2018-09-16T19:33:30.000Z","comments":false,"path":"about/index.html","permalink":"http://wysh.site/about/index.html","excerpt":"欢迎访问，请输入密码.","text":"欢迎访问，请输入密码. 密码错误！ No content to display! U2FsdGVkX19pDTT/m4oBCvI5FED8tA/aAkQrJs4wupk3zbbjuaSwn5sbxBOK6kwliZ2mmNpG0aoJhICtSMXxAgSGXADZj16kZMEZOM8XGdoSOI/3LndDu6T1v8Et2p8kvA5Wbg5mLkvTWDESz5Ot6xbSyqBXDZLyEkmhnmJIS1IU5I5isHLSCHQbd9KgI7tXK3CnkvZVws1rklzgz31EddVfxMe4XZ5cHROre3BgqkowF0ZErtBehymMgVkBgi/yUP7aeSa9SJpEUVOsTNuG15gvMqztqYMyHYAI3bkYFuvU5kMepP0YCCk9nKse037QeC9NqgdOeE0Qo6qgrQn6NxAAan532qCkbXCj0clzVEk9sMGYe3bssbcY1PLvv7bcSCkm0t87H/4MjxXiI59HIlKqf3jJgnK7zoGk/KqsD/Kn7W1xS+FAHzTtfRr7Q69+e1t97ZeZ47XJ7H/6lu76xcDmSivXNDeBaQ83cp9rpBosLSBcNczW3y5e7nVgHIyyggvGlwQS/PUUcv2uarw4m6/mVqZiLuPA+AULhN3N2l9IwD9PPDa4DoYZ1cGHrXEopqqLwaZA0GB9uUkXtx7/nfe9vH6PMPhTb749T3isD1OsBQfUrqS082/4N8+cgRSNUC/vqaYiSmdMEdCLezTrjmUcOR2lfFSUDBZdwLW+XtwWWIvZVpwufDZLbppN7k+3Ep5u8djGz8e7yFtulZ+VvWhzKY8VqKjpPmEvsF7t7yECaF4GPQQBa2T68D8yV6dM"}],"posts":[{"title":"详解服务器性能测试的全生命周期？——从测试、结果分析到优化策略","slug":"详解服务器性能测试的全生命周期从测试结果分析到优化策略","date":"2017-06-24T06:20:17.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2017/06/24/详解服务器性能测试的全生命周期从测试结果分析到优化策略/","link":"","permalink":"http://wysh.site/2017/06/24/详解服务器性能测试的全生命周期从测试结果分析到优化策略/","excerpt":"","text":"服务器性能测试是一项非常重要而且必要的工作，本文是作者Micheal在对服务器进行性能测试的过程中不断摸索出来的一些实用策略，通过定位问题，分析原因以及解决问题，实现对服务器进行更有针对性的优化，提升服务器的性能。 1. 服务器性能测试小结 讲到服务器性能大部分人会想到这个服务器的架构是什么样子的，用的什么epoll，select，spring，tornado之类的。其实从本质上来看的话目前大部分的服务器主要包括逻辑层以及DB层，我们采用的各种框架组件处于逻辑服务器中，如下图所示。 服务器性能测试是一项比较繁琐的事情，作为没有做过性能测试的同学可能需要理清楚以下几个事情。 1.1. 协议分析首先是协议分析，性能测试本质上是我们用代码来模拟真实的用户请求，所以我们必须要知道发送出去的请求内容才能模拟。在典型的CS服务器中很多使用了protobuf，thrift，tdr（腾讯自研）来序列化以及反序列号请求内容。 序列化之后一方面可以对数据进行压缩处理，另一方面也避免请求内容明文传输造成被抓包·泄漏数据的危险。之前有过服务器传输数据的时候使用的是明文直接发送，而且这个数据是一些敏感的sql语句，这样首先暴露了数据库的表结构，同时不法分子可以通过模拟发包造成“脱裤”甚至是数据被清空。 1.1.1. Protobuf谷歌出品，必属精品。Protobuf使用起来很方便，学习成本非常低，而且序列化和反序列号的接口很容易使用。同时它相对于xml以及json，极大的的减小了数据占用的空间，减少了传输成本。目前支持包括C，java以及python等多个语言。Protobuf目前用的比较多，打解包也很方便，比较推荐使用。 1.1.2. ThriftThrift是一个跨语言的轻量级rpc消息和数据交换框架。Thrift支持几乎绝大部分主流的语言，包括C, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, Smalltalk, and OCaml，虽然大部分我都没有用过。相对于protobuf，thrift提供了全套RPC解决方案，包括序列化机制、传输层、并发处理框架等，也因为如此thrift的学习成本比较高。 1.1.3. 腾讯自研协议tdrTdr是腾讯自研跨平台多语言数据表示组件，主要用于基于数据的序列化反序列化（支持二进制方式和XML文本方式）以及ORM数据存储。广泛用于互娱自研游戏和部分代理游戏。在性能上基本和protobuf差不多，主要应用在C程序中。在做服务器性能测试之前，我们需要了解它的协议是怎么定义的。 1.2. 机器人管理本质上机器人管理就是一个调度控制器，在获取需要发送的请求协议之后，需要有一个框架来管理所有的机器人，控制机器人的启动，发送请求以及停止的动作。框架的选择需要根据服务器的实际情况来，不同的业务场景，使用不同的框架产生的压力上限也会不一样。 以一个简单的多线程框架为例，主线程负责控制逻辑，管理所有的机器人状态信息。子线程执行每个机器人的任务，包括连接服务器，发送数据，接收数据，断开连接等。 1.3. 结果统计机器人发送请求包之后，一般是要等待服务器的响应回包。服务器那边可以计算本次压测过程中各项业务数据，包括TPS，总的收发包量等。 不可能在测试过程中一直盯着各个数据看，我们需要把每项数据记录下来，后续综合各项结果进行分析。这里的结果统计除了需要统计每个机器人收到回包的结果，还需要统计服务器在压测过程中的各项性能数据变化。一旦客户端的压力上到一定值时，服务器某项资源支撑不了的话，说明这个资源可能存在短板，存在可以优化的空间。 性能结果分析是一个比较复杂的过程。需要综合硬件、操作系统、应用程序等多方面来定位。 2.1. 硬件的影响硬件对服务器性能影响还是蛮大的，如果是土豪的话，可以直接买最好的。我们分析硬件主要是希望选择合适的配置，节约资源，避免出现高射炮打蚊子的情况。 2.1.1. CPU在资金的充足下，一般来说CPU的数量越多，主频越高，那么服务器的性能也就会越好。在实际测试过程中，如果在大压力下持续观察CPU的使用率很低，那么CPU的资源基本上是可以满足服务器要求的。这样的情况其实是有点浪费CPU资源的，比较理想的情况是压力不大的时候CPU利用率比较低，压力上来之后CPU利用率保持在60%-70%。 大部分的CPU在同一时间内只能运行一个线程，但是超线程的处理器可以在同一个时间运行多个线程，我们可以利用处理前超线程特性提高系统性能。虽然采用超线程技术能同时执行两个线程，但它并不象两个真正的CPU那样，每个CPU都具有独立的资源。当两个线程都同时需要某一个资源时，其中一个要暂时停止，并让出资源，直到这些资源闲置后才能继续。因此超线程的性能并不等于两颗CPU的性能。 2.1.2. 内存内存的大小也是影响服务器性能的一个重要因素。内存太小，系统进程要被阻塞，应用程序会变得缓慢，甚至是失去响应；如果太大的话，也是造成一种浪费。Linux系统中采用物理内存和虚拟内存两种方式，使用虚拟内存可以缓解物理内存的不足，但是如果占用过多的虚拟内存的话，应用程序的性能会明显的下降。 2.1.3. 网络带宽网络带宽的大小限制了客户端与服务器交互的流量，相对其他硬件资源，网络带宽在价格上更贵。这需要我们合理预估服务器的可服务器能力，需要占用的带宽资源。 2.1.4. 磁盘IO目前磁盘都是机械方式运作的，主要体现在磁盘读写前寻找磁道的过程。磁盘自带的读写缓存大小，对于磁盘读写速度至关重要。读写速度快的磁盘，通常都带有较大的读写缓存。磁盘的寻道过程是机械方式，决定了其随机读写速度将明显低于顺序读写。在我们做系统设计和实现时，需要考虑到磁盘的这一特性 2.2. 操作系统及软件2.2.1. 版本不同的操作系统在内核实现上可能各不相同，因而对运行在上面的应用程序来说可能影响比较大。 笔者并没有做过分析不同操作系统对服务器性能的影响，因为只用过Linux开发服务器程序。Linux操作系统在这十几年发展的异常迅猛，目前大部分的服务器都是运行在Linux操作系统上的。Linux目前具有最好的生态系统，服务器端的各种软件都为它而设计，默认都认为你是在 Linux 上跑，你要是整一个非 Linux 的服务器，你得有足够的心理准备，因为出现任何问题，你可能未必能找到能帮你解决问题的人。 2.2.2. 参数配置先说一个小故事。福特公司一套重要设备出现故障，找了很多人来维修，结果都没有维修好，没办法了，就在购买设备的英国公司高价聘请一位工程师过来维修，工程师来到之后，反复查找原因，最后在一个小零件上划了一条线，然后对旁边福特公司的人说，在划线的地方切掉就好了，果不其然，切掉之后故障真的解除了，按照合约，福特公司应支付公司一万美元，周围的人都唏嘘不已，感叹一条线就可以价值一万美元，工程师回答到：那条线只值一美元，而怎样找到那条线值9999美元。 我们在测试服务器的过程中，经常会遇到性能上不去。查看CPU，网络，IO消耗都挺低的，就是定位不到问题的原因。有经验的程序员可能会告诉你你把某个参数修改一下，立马性能噌噌噌上去了。比如mysql相关设置，系统文件描述符，缓冲区大小，time_wait快速回收设置等，甚至是线程池配置的线程个数也会对服务器的性能产生较大的影响。 关于数据库参数的设置，比如mysql的配置文件my.cnf文件中，修改不同的配置（比如innodb_flush_log_at_trx_commit 设置为0，1还是2 ）可能会对数据库的读写性能影响很大。 2.2.3. 应用程序本身实现比如程序中需要频繁申请内存，使用bzero和memset对服务器性能影响差距可能会很大。 另外程序中的一些查询操作，采用不同的数据结构，可以实现时间和空间上的相互转化，从而影响服务器的性能。 3. Linux下的数据监控工具3.1. VmstatVmstat，virtual memmory statistics（虚拟内存统计），主要是对操作系统的内存信息、进程状态、cpu活动等进行监视，但是它不能对某个进程进行深入的分析。 Procs中r列表示运行和等待CPU时间片的进程数，如果r值长期大于CPU个数，说明CPU资源不够用啦，可以适当增加CPU数量。 Procs中b列表示当前等待资源的进程数，包括等待I/O，内存等。 Swpd列表示切换到内存交换区的KB数，一般si，so为0的话基本不影响系统的性能。 Cache是page cache的内存数量，Linux会把空闲的物理内存的一部分拿来做文件和目录的缓存，以便提高程序执行的性能。如果cache的值较大的话，说明缓存了太多的文件，如果bi值小的话，说明文件系统效率比较高。 Si是每秒从磁盘读入虚拟内存的大小，如果这个值一直大于0，表示物理内存不够用或者内存泄露了，需要查找耗内存进程解决掉。 Bi，bo是表示从块设备读入数据的总量以及写到块设备的数据总量。如果bi+bo值比较大，而且wa值也比较大的话，说明系统磁盘I/O可能有问题，性能不高。 In和cs是每秒钟的设备中断数以及上下文切换数。它们很大的话，表面内核消耗的CPU时间较多。 3.2. TopTop是一个动态显示过程，即可以通过用户按键来不断刷新当前状态。它可以按照系统中当前进程的CPU利用率以及占用的内存大小进行排序，可以比较快速定位出系统响应迟钝的原因。如果在前台执行该命令，它将独占前台，直到用户终止该程序为止。 top是一个显示数据较多的工具，第一行显示的是系统的开机运行时间，机器的CPU负载信息；第二行显示当前系统任务的总数，以及各个状态的进程数；第三行显示的是CPU资源的使用情况总览；第四行显示内存的使用情况总览；第五行显示的是内存交换区的使用情况总览；后面开始是每个进程对资源使用的情况。 3.3. NmonNmon提供对CPU、内存、网络、磁盘等系统资源占用情况分析，相比其他Linux命令获取到的数据，nmon的功能更为集中、配置性更强。通过nmon采集到数据之后可以在windows系统中使用nmon_analyser做数据的展示以及分析工作，可视化效果比较好。 由于一般Linux系统都不自带nmon，使用之前需要下载安装。 3.4. UptimeUptime命令显示系统已经运行了多长时间，它依次显示当前时间、系统已经运行了多长时间、目前有多少登陆用户、系统在过去的1分钟、5分钟和15分钟内的平均负载。 关于系统平均负载，它表示在特定时间间隔内运行队列中的平均进程数。如果一个进程满足以下条件则其就会位于运行队列中：没有在等待I/O操作的结果；没有主动进入等待状态；没有被停止。 3.5. NetstatNetstat命令可以显示本机的网络连接情况，监听端口以及路由表等各种网络相关信息。Netstat用于显示与IP、TCP、UDP和ICMP协议相关的统计数据，一般用于检验本机各端口的网络连接情况。 比较常用的可以用次命令查看当前开启监听的服务器进程信息以及端口信息。 3.6. FreeFree是监控Linux使用情况最常用的命令。 “Free -m”可以查看以M为单位的使用情况，这里主要观察free和cached两列。 一般来说，如果应用程序可用内存/系统物理内存&gt;70%时，表明目前系统内存资源比较充足，不影响系统性能；如果应用程序可用内存/系统物理内存&lt;20%时，表明目前系统内存资源比较紧缺，需要释放其他程序内存或者增加内存；如果应用程序可用内存/系统物理内存在20%-70%之间，表明目前系统的内存资源基本满足应用需求，暂时不影响系统的性能 3.7. SarSar也是一个强大的分析系统性能的工具，它可以比较全面的获取系统的CPU，运行队列，磁盘IO，分页，内存，CPU中断，网络等多项数据。上图是使用sar获取系统CPU的整体负责情况，每隔1秒统计一次，统计3次，最后会给出3次的平均值。需要查看其他的数据可以查看手册使用。 3.8. IostatIostat是I/O statistics的缩写，主要功能是对系统的磁盘I/O操作进行监控。它的输出主要显示磁盘读写操作的统计信息，同时也会给出CPU的使用情况。 这里显示的是查看CPU和磁盘的信息，统计间隔2秒，共3次。 3.9. ValgrindValgrind是一款广泛用于监控程序运行过程进行内存调试、内存泄漏检测以及性能分析的工具。它会给出内存泄漏的统计，包括definitely lost,indirectly lost,possibly lost,still reachable ,suppressed等，我们可以使用valgrind来测试程序中内存不规范使用的部分。同时对于地址越界问题也可以通过valgrind扫出来，它会统计invalid write的情况。 4. 服务器的性能优化4.1. 存储的优化IO相对来说比较耗时，我们都知道越靠近CPU的存储，其访问速度越快，但是其价格越贵。下图来展示了不同存储的容量以及访问时间。 目前很多同学在优化服务器性能的时候都会从存储这方面入手。 4.1.1. 用内存换时间4.1.1.1. 增加缓存很多web应用是有大量的静态内容，这些静态内容主要都是一些小文件，并且会被频繁的读，采用Apache以及nginx作为web服务器。在web访问量不大的时候，这两个http服务器可以说是非常的迅速和高效，如果负载量很大的时候，我们可以采用在前端搭建cache服务器，将服务器中的静态资源文件缓存到操作系统内存中直接进行读操作，因为直接从内存读取数据的速度要远大于从硬盘读取。这个其实也是增加内存的成本来降低访问磁盘带来的时间消耗。 4.1.1.2. 内存数据库内存数据库，其实就是将数据放在内存中直接操作的数据库。相对于磁盘，内存的数据读写速度要高出几个数量级，将数据保存在内存中相比从磁盘上访问能够极大地提高应用的性能。内存数据库抛弃了磁盘数据管理的传统方式，基于全部数据都在内存中重新设计了体系结构，并且在数据缓存、快速算法、并行操作方面也进行了相应的改进，所以数据处理速度比传统数据库的数据处理速度要快很多。 但是安全性的问题可以说是内存数据库最大的硬伤。因为内存本身有掉电丢失的天然缺陷，因此我们在使用内存数据库的时候，通常需要，提前对内存上的数据采取一些保护机制，比如备份，记录日志，热备或集群，与磁盘数据库同步等方式。 对于一些重要性不高但是又想要快速响应用户请求的部分数据可以考虑内存数据库来存储，同时可以定期把数据固化到磁盘。 4.1.1.3. RDD这里图个新鲜，说说内存换时间在大数据云计算相关领域的一些应用。Spark最近很火，它的核心要数RDD了，RDD最早来源与Berkeley实验室的一篇论文《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》。现有的数据流系统对两种应用的处理并不高效：一是迭代式算法，这在图应用和机器学习领域很常见；二是交互式数据挖掘工具。这两种情况下，将数据保存在内存中能够极大地提高性能。这里不详细说RDD了，只是想说程序员一直是觊觎内存的读取速度的。 4.1.2. 使用SSD等除了对内存方面的优化，还可以对磁盘这边进行优化。跟传统机械硬盘相比，固态硬盘具有快速读写、质量轻、能耗低以及体积小等特点。但是ssd的价格相比传统机械硬盘要贵，有条件的可以使用ssd来代替机械硬盘。 4.2. 数据库优化大部分的服务器请求最终都是要落到数据库中，随着数据量的增加，数据库的访问速度也会越来越慢。想要提升请求处理速度，必须要对原来的单表进行动刀了。目前主流的Linux服务器使用的数据库要属mysql了，如果我们使用mysql存储的数据单个表的记录达到千万级别的话，查询速度会很慢的。 根据业务上合适的规则对数据库进行分区分表，可以有效提高数据库的访问速度，提升服务器的整体性能。 另外对于业务上查询请求，在建表的时候可以根据相关需求设置索引等，以提高查询速度。 4.3. 利用多核优势现在运行服务器的主流机器配置都是多核CPU的，我们在设计服务器的时候可以利用多核心的特点，采用多进程或者多线程的框架。 关于选择多线程还是多进程可以根据实际的需求，结合各自的优缺点进行选择。 对于多线程的使用，特别是使用线程池的时候可以通过测试不同线程池服务器的性能来设置合适的线程池。 4.4. 选择合适的IO模型《UNIX网络编程卷1:套接字联网API》中有一幅图比较经典。 信号驱动：首先开启套接口信号驱动I/O功能,并通过系统调用sigaction安装一个信号处理函数。当数据报准备好被读时，就为该进程生成一个SIGIO信号。随即可以在信号处理程序中调用recvfrom来读数据报，井通知主循环数据已准备好被处理中。也可以通知主循环，让它来读数据报。 异步的IO模型：告知内核启动某个操作，并让内核在整个操作完成后(包括将数据从内核拷贝到用户自己的缓冲区)通知我们。 这里并不是说一定要用某个模型，epoll也并不是在所有情况下都比select性能要好的，在选择的时候还是要结合业务需求来。 4.5. 分布式部署程序当单机服务器已经找不到合适的优化点时，我们可以通过分布式部署来提高服务器的响应能力。优秀的服务器开发都会为自己的服务器的扩容，容灾提出一些解决方案。个人觉得服务器设计的时候简单点比较好，这样后期扩容的时候会很方便。 总结服务器性能测试是一项比较繁琐的事情，作为没有做过性能测试的同学需要事先了解服务器的协议是如何定义的，建立框架管理机器人、统计测试中机器人收到回包的结果以及压测过程中各项性能数据的变化。在完成了测试的过程后，可以从硬件、操作系统以及应用程序等多个方面进行对性能结果进行定位。最后在明确业务需求的前提下，通过存储优化、数据库优化以及分布式部署程序等手段完成服务器的性能优化。","categories":[{"name":"测试文档","slug":"测试文档","permalink":"http://wysh.site/categories/测试文档/"}],"tags":[{"name":"测试文档","slug":"测试文档","permalink":"http://wysh.site/tags/测试文档/"}]},{"title":"Web安全测试基础一 （一：跨SQL注入，二：跨站脚本攻击，三，命令执行漏洞，四，跨站请求伪造，五，文件上传漏洞，六，文件包含漏洞）","slug":"Web安全测试基础","date":"2017-06-23T00:45:34.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2017/06/23/Web安全测试基础/","link":"","permalink":"http://wysh.site/2017/06/23/Web安全测试基础/","excerpt":"","text":"一、Web安全漏洞概念及原理分析1.1 跨SQL注入概念：SQL Injection按照字面意思来翻译就是“SQL注射”，常被叫做“SQL注入”，它的含义就是利用某些数据库的外部接口把用户数据插入到实际数据库操作语言当中，从而达到入侵数据库乃至操作系统的目的。SQL注入漏洞形成原因是：用户输入的数据被SQL解释器执行。 原理分析： 假设如下URL存在漏洞 http://www.xxx.com/xx.asp?id=666 构造数据库原型： select * from table_name where id=1 那么提交： http://www.xxx.com/xx.asp?id=666 and [查询语句] select * from table_name where username=‘or’=‘or’ And pass=‘or’=’or’ 这样，如果页面和id=666返回相同的结果，就说明附加查询条件成立，如果不同，就说明不成立。 手工判断能否进行SQL注入方法： 第一步先把IE菜单=&gt;工具=&gt;Internet选项=&gt;高级=&gt;显示友好 HTTP 错误信息前面的勾去掉。否则，不论服务器返回什么错误，IE都只显示为HTTP 500服务器错误，不能获得更多的提示信息。 简单判定有无漏洞： 粗略型：提交单引号’ 逻辑型（数字型注入）：and 1=1 / and 1=2 逻辑型（字符型注入）：’ and ‘1’=’1 / ‘ and ‘1’=’2 逻辑型（搜索型注入）： 关键字%’ and 1=1 and ‘%’=’% / 关键字%’ and 1=2 and ‘%’=’% 危害：获取数据库信息，脱裤，获取服务器权限，植入webshell，获取服务器后门，读取服务器敏感文件等。 示例： 如下，user id查询框中输入1，则正常显示结果如下： 但是，当user id查询框中输入1’时，则显示输出如下： 则该处存在sql注入漏洞。 目前流行的注入工具有：SQLMap、Pangolin（穿山甲）、Havij等。 1.2 跨站脚本攻击（XSS）概念： 通常指黑客通过“HTML注入”篡改了网页，插入了恶意的脚本，从而在用户浏览网页时，实现控制用户浏览器行为的一种攻击方式。 全称： Cross Site Script（本来缩写是CSS，但是为了和层叠样式表CSS有所区别，所以在安全领域叫做“XSS”） 危害： 盗取用户信息、篡改页面钓鱼、制造蠕虫等。 XSS分类： 存储型、反射型、DOM型 反射型XSS 反射型XSS只是简单地把用户输入的数据“反射”给浏览器。也就是说，黑客往往需要诱使用户“点击”一个恶意链接，才能攻击成功 如下，查询name信息，正常用户请求： 如果那name参数1修改成，则显示结果： 存储型XSS 如下，正常留言或者评论，显示如下： 如果将message信息写成，则显示 DOM XSS 基于DOM型的XSS是不需要与服务器端交互的，它只发生在客户端处理数据阶段。 下面一段经典的DOM型XSS示例。 上述代码的意思是获取URL中content参数的值，并且输出，如果输入http://www.xxx.com/dom.html?content=，就会产生XSS漏洞。 各种类型原理分析 1.3 命令执行漏洞漏洞产生原因: 程序中因为某些功能需要执行系统命令，并通过网页传递参数到 后台执行。然而最根本的原因是没有对输入框的内容做代码过滤，正常情况下输入框只能接收指定类型的数据。 漏洞影响: 命令注入漏洞可以使攻击者在受攻击的服务器上执行任意的系统命令。 示例: 正常情况下，在 ip 地址输入框中输入 127.0.0.1，结果如下: 但是，当我们输入 127.0.0.1&amp;&net; user 时，输出结果如下: 结果显示不仅仅执行了 ping 127.0.0.1 操作，而且也执行了 net user 命令，我们可 以把 net user 换成其他任意命令进行攻击。 1.4 跨站请求伪造(CSRF)概念: Cross-Site request forgery，利用用户已登录的身份，在用户毫不知情的情 况下，以用户的名义完成非法操作。 CSRF 攻击迫使终端用户在通过验证后 web 应用中执行不必要的操作。在社会工 程帮助下(如通过电子邮件/聊天发送的链接)，攻击者可能会迫使 Web 应用程序 用户执行攻击者所选择的行动。 危害: 执行恶意操作(“被转账”、“被发表垃圾评论”等)、制造蠕虫等 漏洞影响:当一个成功的 CSRF 漏洞的目标是普通用户时，它能够危害终端用户 的数据操作。但如果最终的目标用户是管理员账户，一个 CERF 攻击可以损害整 个 Web 应用程序。 示例: 如下，正常修改密码页面，New password 输入 test，Confirm new password 输入 test，然后提交，密码修改成功。 我们发现，这个修改密码的请求为 “ http://10.4.70.188/DVWA-1.9/vulnerabilities/csrf/?password_new=test&password;_ conf=test&Change;=Change ”， 此 时 ， 我 们 打 开 新 的 窗 口 ， 修 改 password_new=password 和 password_conf=password，访问显示如下: 此时，密码已经被修改成功，如果该 URL 被黑客通过电子邮件或其他途径精心 伪造，诱惑你触发点击，则可直接修改当前用户配置，后果不堪设想。 1.5 文件上传漏洞文件上传漏洞，是指用户上传了一个可执行的脚本文件，并通过此脚本文件获取了执行服务器端命令的能力。这种攻击方式是最为直接和有效的，有时候几乎没有什么技术门槛。 如下，选择正常的图片文件test.jpg上传，上传成功后显示： 我们根据显示的路径信息，推测上传图片的地址为“http://10.4.70.188/DVWA-1.9/hackable/uploads/test.jpg”，在浏览器中打开： 那么，我们写一个文件phpinfo.ini，内容为，上传成功后，我们访问该文件显示： 如果我们编辑一句话shell：” “为cmd.php文件，上传成功后访问文件url并且传参cmd=phpinfo()，查看结果如下： 我们修改cmd参数为任意命令，如cmd= system(‘dir’); ，显示如下： 一旦被黑客利用，后果不堪设想。 1.6 文件包含漏洞如下页面，点击file1、file2、file3，页面地址都会随之变化为 由此，page参数就是我们可以利用的地方，我们修改page参数为随意字段，如abc.php访问，则显示： 报错信息中，我们获取到文件的物理路径，可以利用该漏洞读取本地文件，如读取dvwa目录下的php.ini文件，将文件路径进行如下修改： 也就是page参数为”../../php.info”，访问显示如下： 成功读取服务端文件信息，当然，攻击者肯定不满足读取本地文件，攻击者可以修改成可执行php代码进行攻击。 1.7 点击劫持（ClickJacking）点击劫持是一种视觉上的欺骗手段。攻击者使用一个透明的、不可见的iframe，覆盖在一个网页上，然后诱使用户在该网页上进行操作，此时用户将在不知情的情况下点击透明的iframe页面。通过调整iframe页面的位置，可以诱使用户恰好点击在iframe页面的一些功能性按钮上。也就是通过覆盖不可见的框架误导受害者点击而造成的攻击行为。 隐蔽性高，骗取用户操作，也称UI-覆盖攻击，是利用iframe或者其他标签的属性，如flash也可以。 1.8 URL跳转漏洞与钓鱼借助未验证的URL跳转，将应用程序引导到不安全的第三方区域，从而导致的安全问题。","categories":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/categories/产品测试/"}],"tags":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/tags/产品测试/"}]},{"title":"产品测试规范（十二:Bug管理规范）","slug":"产品测试规范十二Bug管理规范","date":"2016-11-23T00:34:34.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/11/23/产品测试规范十二Bug管理规范/","link":"","permalink":"http://wysh.site/2016/11/23/产品测试规范十二Bug管理规范/","excerpt":"","text":"1.11 Bug管理规范1.11.1 bug提交规范Bug的报告要求描述内容清晰、简介、易懂，让用根据简要描述就可以大致了解问题所在： 缺陷ID BUG的唯一标识，由BUG管理工具自动生成。 项目名称 每个要测试的软件项目都有唯一的名称。 问题类型（严重程度） BUG所属的类型（即严重程度），包括致命问题、严重问题、一般 问题、优化建议等。缺陷标题简明的对BUG进行概要描述。 缺陷标题 简明的对BUG进行概要描述。 优先级 BUG解决的优先级。 所属模块 项目的各个组成模块。 测试版本 提交BUG时，一定要正确填写产生BUG的软件版本号。 分派人 BUG需要指派处理的人员，如果不清楚统一给项目负责人。 报告人 报告BUG的人员。 测试环境 可根据实际描述当前测试的软硬件环境，以作为参考。 详细描述 在详细描述中，可对BUG产生的前提条件、操作的步骤、实际结 果、预期结果等进行描述。 文字注释与附图 在提交BUG时，可上传必要的附图，便于确认错误的表现形式和 错误位置等。 在提交BUG时，提交人可根据提交BUG的紧急程度，选择对应的“优先级”，同时建议开发人员在处理BUG的时候能够根据优先级进行处理，优先级别较高的可以最先进行处理。 在BUG详细描述中，可在从BUG产生的前提条件、操作的步骤、实际结果、预期结果等方面进行描述： 前提条件：有些BUG的产生是需要在一定条件下才会出现，例如浏览器、分辨率、Office版本等，所以就要求在描述时描述清楚前提条件； BUG的操作步骤：详细的、有次序的、每一步的操作步骤，包括输入的数据，尽可能的重新操作的步骤； 实际结果：指的我按照以上的操作步骤，最后得出的结果是什么， 例如我点击“增加”按钮后出现白页，这就是实际结果； 预期结果：指的我按照以上的操作步骤，我想要得到的结果是什么，例如我点击“增加”按钮想要得到的预期结果是提示我“增加成功”提示； 图文描述：在必要的情况下可上传截图并注释文字，这样更便于确认错误的表现形式和错误位置等。 一般情况下，开发人员在提交BUG时，“分派人”可指定对应的处理人员，如果无法确定“分派人”，可分派给项目的负责人，然后由项目负责人进行二次分派给对应的开发人员进行处理。在分派时可以添加一些对应的批注信息。 1.1.2 bug级别定义具体的优先级别有以下几种 1 致命问题(一级bug) 致命问题：不能完全满足系统正常的功能操作要求，系统停止运行，系统的重要部件无法运行，系统崩溃或挂起等导致系统不能继续运行。 常规操作下因程序问题导致系统崩溃，迫使整个系统无法使用（其中非程序问题有：系统配置、数据结构变动、session超时、网络中断、人为变更数据库中的数据、系统缺少相应文件或目录等）。 常规操作下因程序问题导致程序重启、死机或非法退出。 常规操作下系统出现死循环。 数据丢失或异常。 模块间数据传递及取值错误（如：输入A，预期结果应该是B，但实际结果不是B等）。 流程输出错误（包括业务流程和事件流程。如：输入流程A，但实际流程处理中未能按A流程处理数据；点击某按钮，应跳转增加页面，结果跳转成修改页面等）。 按照需求文档，功能未在程序中体现出来，即系统无此功能（据项目经理及相关负责人确认此功能必须具备的）；功能不符合用户需求，功能实现不正确（由项目经理及相关负责人确认此功能必须具备的）。 2 严重问题(二级bug) 严重问题：严重地影响系统要求或基本功能的实现，且没有更正办法（重新安装或重新启动该软件不属于更正办法）。使系统不稳定、或破坏数据、或产生错误结果，或部分功能无法执行，而且是常规操作中经常发生或非常规操作中不可避免（不能用其他操作修复问题）的主要问题，系统无法满足主要的业务要求，性能、功能或可用性严重降低。 数据计算错误。 因程序问题迫使正在操作的流程无法继续且无其他操作可以修复问题的（其中非程序问题有：系统配置、数据结构变动、Session超时、网络中断、人为变更数据库中的数据、系统缺少相应文件或目录等）。 常规操作下功能异常，如：结果与实际查询条件不一致、页面按钮点击没反应等。 功能项的某些项目（可为所有控件）使用无效（对系统非致命的）。 因程序问题迫使正在操作的流程无法继续且有其他操作可以修复问题的（其中非程序问题有：系统配置、数据结构变动、Session超时、网络中断、人为变更数据库中的数据、系统缺少相应文件或目录等）。 多余功能，且该功能影响了程序的正常使用（需项目经理及相关负责人确认），如客户名称录入项需要录入汉字和英文，但程序限制了只能输入汉字等。 常规操作下，程序打印、导出的内容错误。 在程序安装配置无误的情况下相关功能js报错，且该功能影响业务流的正常进行。 在1024*768分辨率下，页面严重变形，使数据无法浏览。 在Session超时，无友情页面提示 3 中级问题（三级bug） 系统可以满足业务要求，系统性能或响应时间变慢、产生错误的中间结果但不影响最终结果等影响有限的问题，另外，还包括系统健壮性方面的测试。 对于一些重要数据的操作、重要环节的变动且相关的操作和变动不可挽回时，系统应给出相应的操作确认提示，防止误操作，如数据删除、审批等。 常规操作下页面跳转至错误友情提示页面，且操作其他模块，程序可正常运行（其中非程序问题有：系统配置、数据结构变动、Session超时、网络中断、人为变更数据库中的数据、系统缺少相应文件或目录）。 功能实现不完整，如删除时没有考虑数据关联。 因错误操作且因程序问题导致系统崩溃，迫使整个系统无法使用（其中非程序问题有：系统配置、数据结构变动、Session超时、网络中断、人为变更数据库中的数据、系统缺少相应文件或目录等）。 数据添加、修改、查看界面中控件没有一一对应或对应控件长度、格式、验证性提示信息内容等不一致，但又不影响程序功能的进一步的操作（最终以需求规格说明书中内容规定为准）。 响应时间较慢。（不可超过1分钟） 功能性建议。 操作界面错误（包括数据窗口内列名定义、含义是否一致）。 简单的输入限制未放在前台进行控制。 虽然正确性不受影响，但系统性能和响应时间受到影响。 常规操作下，程序显示、打印、导出的内容格式错误，如页面变形、金额类数据未加货币符号等。 在程序安装配置无误的情况下相关功能js报错，且该功能不影响业务流的正常进行。 页面验证提示信息位置或内容错误，如空值验证对应位置或内容错误、提示对话框内容错误等（最终以需求规格说明书中内容规定为准）。 在1024*768分辨率下，页面变形，但不影响数据的浏览。 输入超长数据或特殊字符导致程序报黄页或跳转到友情提示页面等影响程序进一步的操作（需跳转友情页面）。 在Session超时（需友情页面）、网络中断时，出现浏览器卡死、报黄页等异常情况，且没有对应的错误捕获机制并给出友情提示。 滚动条无效，但不影响数据的显示与浏览。 界面不规范，页面表现形式、样式与其他类似功能模块不一致，且差异明显的。 必填项与非必填项应加以区别。 4 轻微问题 轻微问题：使操作者不方便或遇到麻烦，但它不影响执行工作功能或重要功能。界面拼写错误或用户使用不方便等小问题或需要完善的问题。 页面表现建议。 功能操作建议。 非程序代码导致黄页（如：手动删除、修改、增加数据库中的数据；缺少相应的系统配置；项目缺少目录或文件、因不明操作导致数据库中数据不符合正常逻辑关系）。 辅助说明字体大小、颜色明显与页面整体表现形式不协调或者文字描述不清楚。 长时间操作未给用户提示（不可超过1分钟），但程序一直在正常运行的，没有出现卡死等情况，如给出旋转的loading图标或程序后台操作进度条或显示进度百分比等。 提示窗口文字未采用行业术语。 可输入区域和只读区域没有明显的区分标志，如只读区域置灰显示等。 键盘支持不好，如在可输入多行的字段中不支持回车换行，输入查询条件后不支持回车触发查询。 界面不能及时刷新，如需要重新执行查询或加载页面等（最终以需求规格说明书中内容为准）。 以上就是产品的测试规范，囊括了从需求到测试计划、测试准备、测试执行、结果分析、上线准备、跟踪测试到项目总结的整个流程，规范了产品测试流程。 产品测试规范纲要 目录 第1章 产品测试规范 1.1 产品测试流程 1.1.1 测试流程图 1.1.2 测试流程说明 1.2 需求梳理 1.2.1 需求梳理 1.3 测试计划 1.3.1 测试工具选取 1.3.2 测试人员分配 1.3.3 测试业务场景选取 1.3.4 测试环境梳理 1.3.5 测试数据梳理 1.4 测试准备 1.4.1 代码管理 1.4.2 测试环境搭建 1.4.3 测试数据脚本编写 1.5 测试用例编写(功能测试框架) 1.5.1 界面友好性测试 1.5.2 功能测试 1.5.3 业务流程测试(主要功能测试) 1.5.4 链接测试 1.5.5 容错测试 1.5.6 稳定性测试 1.5.7 常规性能测试 1.5.8 易用性测试 1.5.9 兼容性测试 1.6 测试执行 1.6.1 接口自动化测试 1.6.2 探索式测试 1.6.3 传统测试用例测试 1.6.4 Bug跟踪 1.7 测试结果分析 1.7.1 结果收集 1.7.2 结果分析 1.7.3 测试分析报告 1.8 上线准备 1.8.1 版本发布 1.8.2 数据准备 1.9 上线测试跟踪 1.9.1 跟踪测试 1.10 BUG预防体系 1.10.1 web常见产品问题及预防 1.10.2 app常见产品问题及预防 [TOC]","categories":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/categories/产品测试/"}],"tags":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/tags/产品测试/"}]},{"title":"产品测试规范（十一:Bug预防体系）","slug":"产品测试规范十一Bug预防体系","date":"2016-11-07T11:23:38.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/11/07/产品测试规范十一Bug预防体系/","link":"","permalink":"http://wysh.site/2016/11/07/产品测试规范十一Bug预防体系/","excerpt":"","text":"1.10.2 app常见产品问题及预防11 网络机制(2) a：未加载完图片时切换到相似tab，切回不再加载图片； b：进入一个tab，该页面已经加载完成，选择点击某个详细信息页面返回时，页面会闪一下。 预防方法： a：一个页面有多个tab页时，用户切换tab可不轻易取消线程，取而代之使用暂停线程，退出页面时才回收清除； b：启动负载分摊机制的请求，可先保存请求地址，供返回时判断避免重复加载。 12 网络机制(3) a：iOS弱网络下获取不到配置，导致启动卡死； b：sim卡未激活，无移动网络，某些功能卡死； c：断网下启动，登录状态丢失，某些功能信息未正确显示。 预防方法： a：启动逻辑中的网络类请求不能阻塞UI主线程，即网络请求数据可不即时响应（可在下次启动时生效）； b：按钮的点击事件不跟接口关联，做成异步处理不管是否有返回，都可以正常进行点击操作； c：离线操作类，不因与当前网络状态有影响。 13 下载空间有效性判断 a：空间不足时，无法保存信息时，没有提示和提前判断； b：本地存储空间不足时，保存文件时没有相应提示； c：空间不足时，文件下载不成功，导致重复不停下载，浪费用户流量。 预防方法： a：对磁盘剩余空间的判断和自动清理逻辑可以做统一封装，提供各不同下载业务使用 b：可结合系统硬件配置的10%作为有效剩余空间阀值； c：针对手机内外置SDCard，可以在空间不足情况下做分区切换机制。 14 下载文件完整性判断(1) a：换肤图片未下载完，就触发换肤操作，导致换肤效果错误； b：图片无法下载完全，导致图片展示不完整； c：文件下载完成后，由于网络错误与源文件不符，导致下载后无法播放； d：上传文件功能，目标物理文件不存在（界面缺显示存在），导致传送文件页面一直处于等待中。 预防方法： a：通过判断下载前后文件的size或者文件内容签名，确保下载文件完整后再触发文件使用相关的逻辑； b：文件传输时检查文件是否存在，若不存在则视为传输失败，不阻塞后续传输。 15 阻断连续操作 a：连续快速切换界面，或者频繁触发某些功能操作，导致程序卡死； b：连续多次点击同一张图片，导致该图片下载错误。 预防方法： a：使用间隔响应、延迟响应的方式，达到多次相同操作只的触发一次有效逻辑。 b：操作一次后，可将按钮等元素设定为禁用状态，防止用户多次点击和请求。 16 有效统计逻辑 a：操作页面某些元素，也会导致发送页面使用的统计信息。 预防方法： a：为确保统计数据上传的有效性，只针对真正展示的界面做上报统计，对于展示不完整、非针对性展示不做统计上报。 17 程序健壮性判断(1) a：分享到新浪微博（手机未装新浪微博客户端） ,app崩溃； b：后台接口变更（返回值和类型发生变化），客户端不兼容新格式判断，抛出崩溃异常； c：搜索默认操作崩溃； d：使用外部第三方数据，出现空数据或者非标准格式，则app崩溃 e：输入框没有限制字符长度，保存时导致溢出崩溃。 预防方法： a：客户端针对接口返回需做容错处理，如返回为空、返回数据类型不一致； b：任何文本框类型的需要限制输入长度。 18 程序健壮性判断(2) a：某些功能的初始化逻辑没有加入启动逻辑，导致功能使用失败； b：退出重启app，无法自动登录。 预防方法: a：制定启动加载逻辑规范； b：对于重要的业务建议加入启动逻辑，并在业务实际使用时再根据状态多一层判断和加载； c：产品人员需要考虑是否需要保存自动登录功能，并明确告之开发和测试人员。 19 安全机制 a：在URL中不要带有明文的用户信息写代码的时候，不要把密码等敏感的用户信息明文的显示在url中； b：即使要传递密码参数也不要使用pwd、passpord这样的参数名称来进行传递，防止被截获； c：要在传递参数的操作中使用NoCache参数，防止将url参数进行缓存。 预防方法： a：建立标准的数据传输和命名规范，并制作一些网页开发模板或者规范供参考。 20 日志调试管理 a：上线以后，调试日志没有关闭，影响程序性能。 预防方法： a：日志统一开关，编译正式包需要关闭； b：再程序界面有入口可以检查是否关闭，方便及时校验； c：方便定位问题，可以做日志动态开启的隐藏开关； d：方便收集问题，可以对问题类型做上报处理（典型如崩溃日志上报）。 产品测试规范纲要 目录 第1章 产品测试规范 1.1 产品测试流程 1.1.1 测试流程图 1.1.2 测试流程说明 1.2 需求梳理 1.2.1 需求梳理 1.3 测试计划 1.3.1 测试工具选取 1.3.2 测试人员分配 1.3.3 测试业务场景选取 1.3.4 测试环境梳理 1.3.5 测试数据梳理 1.4 测试准备 1.4.1 代码管理 1.4.2 测试环境搭建 1.4.3 测试数据脚本编写 1.5 测试用例编写(功能测试框架) 1.5.1 界面友好性测试 1.5.2 功能测试 1.5.3 业务流程测试(主要功能测试) 1.5.4 链接测试 1.5.5 容错测试 1.5.6 稳定性测试 1.5.7 常规性能测试 1.5.8 易用性测试 1.5.9 兼容性测试 1.6 测试执行 1.6.1 接口自动化测试 1.6.2 探索式测试 1.6.3 传统测试用例测试 1.6.4 Bug跟踪 1.7 测试结果分析 1.7.1 结果收集 1.7.2 结果分析 1.7.3 测试分析报告 1.8 上线准备 1.8.1 版本发布 1.8.2 数据准备 1.9 上线测试跟踪 1.9.1 跟踪测试 1.10 BUG预防体系 1.10.1 web常见产品问题及预防 [TOC] 1.11 BUG管理规范 1.11.1 bug提交规范 1.11.2 bug级别定义","categories":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/categories/产品测试/"}],"tags":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/tags/产品测试/"}]},{"title":"产品测试规范（十:Bug预防体系）","slug":"产品测试规范十Bug预防体系","date":"2016-11-07T11:23:38.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/11/07/产品测试规范十Bug预防体系/","link":"","permalink":"http://wysh.site/2016/11/07/产品测试规范十Bug预防体系/","excerpt":"","text":"1.10.2 app常见产品问题及预防01 界面适配 a：手机分辨率为1920x7080的高分辨率手机，在调整手机字体大小时，会导致页面显示出现变形； b：因用户设置的特殊字体导致列表的字母条不显示； c：某些 banner 图片在部分机型只能显示一半。 预防方法： a：文字或者图片需要适配不同分辨率的机型时，建议使用dp方式进行开发，即使是使用dp,也需要考虑特殊分辨率的机型显示； b：适应宽度/适应高度/高宽均适应的； c：针对程序需求，设定合适的适配机制。 02 系统适配 a：调用高版本API，导致某些机型进入主页显示空白页面。 预防方法： a：调用高版本API，需要考虑兼容性，开发团队需要制定程序API调用规范。 03 交互适配（1） a：在输入框操作时，调出系统输入法软键盘后，没有有效启用键盘上的 “下一项”、“确定”、“搜索”等按键； b：系统软键盘，在关闭当前页面时没有及时收起软键盘。 预防方法： a：需求设计过程中需要考虑输入法操作键的使用细节，确保所有软键盘的输入键可使用； b：设计规范：程序/页面设计针对输入法操作键的使用制定规范 04 交互适配（2） a：APP界面的“返回”操作与手机系统的“返回”按键操作效果不一致；或界面未提供“返回”，在无系统“返回”按键的手机上，无法返回。 预防方法： a：设计规范：程序设计针对手机返回键制定使用规范； b：在设计中要综合界面需求设定是否提供 “返回”操作。 05 界面风格 a：对话框标点、英文字符出现全角、半角的不统一； b：对话框、提示浮动框提示语风格不同，显示位置均不同，产品友好度下降； c：字体和字号要在app中是不同的风格。 预防方法：-语言文字提示规范 a：全角字符和半角字符都要使用一个空格分开； b：英文和数字之间要有空格分开； c：汉字和英文、数字要有空格分开； d：带有汉字的话要使用全角字符； e：语言中不要混用全角和半角标点； f：字体和字号要保持统一的风格。 06 性能优化(1) a：进入一些列表，若数量较多则会出现卡死:； b：界面显示对象数量较多，某些会导致页面操作卡顿，用户体验很差； c：处理大量数据时，用户等待时间过长，无进度条提示进度。 预防方法： a：程序对耗时较多的操作逻辑、判断逻辑，不放入UI主线程； b：对数据库记录较多的操作，可以改成数据库批量操作，或者 调用批量接口；c：程序在后台处理用户的输入，则提供进度条或对话框。 07 性能优化(2) a：后台播放内存泄露； b：程序后台运行的时候，手机一直处于占用CPU的运行状态； c：页面中的动态效果（如：马灯滚动）次数无限制，导致界面不断刷新消耗资源。 预防方法： a：使用静态分析工具或代码检查方式检查内容的分配和释放； b：WakeLock机制是防回收技术，当没有播放、下载等操作时，应该主动关闭后台的唤醒锁，减少耗电。当再次需要使用播放、下载功能时才去开启唤醒； c：对刷新消耗资源类操作，要有次数限制。 08 多服务、多进程 a：某些功能操作后， app 无法连接网络； b：进程被杀死后重启，通知栏中显示的信息不正确，没有显示正确的信息； c：app未启动，通过其他第三方app的调用入口调用app,无法正常使用某些功能； d：服务停止后，无法被启动； f：程序被手动退出后，进程仍然在后台存在。 预防方法： a：重新初始化时获取值时读取到空值，因此赋予一个默认值； b：服务重启被回收重启时，初始化对象时要判断当前是否已存在，若存在则复用并更新内容 c：任务独立，需要创建不同的服务，生命周期不会互相影响，服务独立可以避免某个服务结束会影响到其他功能的正常使用。 总体，对有启用多服务、多进程的程序，有需要做好服务、进程的一致性管理。 09 外部调用 a：某些机型启动app之后一直在调用某些外部服务（通过后台服务可以看到其他服务进程，退出app后，有些服务进程消失） b：某些功能模块被扫描成存在木马病毒； c：安全管家告警程序获取绝密权限（通讯录权限）。 预防方法： a：调用第三方功能作为统计或者监控作用时，需要考虑该sdk是否会一直唤醒app导致耗电或者程序无法真正关闭问题； b：调用外部第三方SDK，要考虑被安全工具（上次有广告被扫描到病毒）扫描的设计需求； c：及时关闭不需要的服务进程，在能满足需求的情况下，尽量减少使用敏感的系统权限。 10 网络机制(1) a：网络重试操作机制不统一，导致页面超时体验风格不统一； b：某些应用页面，访问响应慢。 预防方法： a：对底层网络重试机制做统一封装后，供上层调用； b：固定好每次重试间隔（建议10s重试）和重试总次数（建议3次）； c：为使页面提示可以区分网络层与业务解析层不同错误，需对不同错误类型做分类的异常处理，并提示用户原因或让用户重试； d：对多个网络请求的界面，网络接口并行请求有利于提高响应速度。 产品测试规范纲要 目录 第1章 产品测试规范 1.1 产品测试流程 1.1.1 测试流程图 1.1.2 测试流程说明 1.2 需求梳理 1.2.1 需求梳理 1.3 测试计划 1.3.1 测试工具选取 1.3.2 测试人员分配 1.3.3 测试业务场景选取 1.3.4 测试环境梳理 1.3.5 测试数据梳理 1.4 测试准备 1.4.1 代码管理 1.4.2 测试环境搭建 1.4.3 测试数据脚本编写 1.5 测试用例编写(功能测试框架) 1.5.1 界面友好性测试 1.5.2 功能测试 1.5.3 业务流程测试(主要功能测试) 1.5.4 链接测试 1.5.5 容错测试 1.5.6 稳定性测试 1.5.7 常规性能测试 1.5.8 易用性测试 1.5.9 兼容性测试 1.6 测试执行 1.6.1 接口自动化测试 1.6.2 探索式测试 1.6.3 传统测试用例测试 1.6.4 Bug跟踪 1.7 测试结果分析 1.7.1 结果收集 1.7.2 结果分析 1.7.3 测试分析报告 1.8 上线准备 1.8.1 版本发布 1.8.2 数据准备 1.9 上线测试跟踪 1.9.1 跟踪测试 1.10 BUG预防体系 1.10.1 web常见产品问题及预防 [TOC] 1.11 BUG管理规范 1.11.1 bug提交规范 1.11.2 bug级别定义","categories":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/categories/产品测试/"}],"tags":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/tags/产品测试/"}]},{"title":"产品测试规范（九: Bug预防体系）","slug":"产品测试规范九Bug预防体系","date":"2016-10-28T08:23:52.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/10/28/产品测试规范九Bug预防体系/","link":"","permalink":"http://wysh.site/2016/10/28/产品测试规范九Bug预防体系/","excerpt":"","text":"1.10.1 web常见产品问题及预防11 多个ie同时访问  用户可能打开不同的IE使用相同的用户登录后进行操作，程序处理的时候要考虑到数据的一致性和同步问题  多个IE使用不同用户，则cookie操作不会出现用户信息混乱的问题 预防方法：  开发：提前考虑到多个IE操作和多用户操作的使用场景，在使用cookie本地信息时需要做好针对性的程序处理，依据以往出现的问题设计开发规范  测试：按照多浏览器和多用户的使用情况，进行更多场景的测试 12 安全考虑  在URL中不要带有明文的用户信息写代码的时候，不要把密码等敏感的用户信息明文的显示在url中  即使要传递密码参数也不要使用pwd、passpord这样的参数名称来进行传递，防止被截获  要在传递参数的操作中使用NoCache参数，防止将url参数进行缓存 预防方法：  开发： 建立数据传输技术规范和参数命名规范标准，严格参照执行，防止信息被拦截，造成应用系统的信息泄露  测试：在缓存目录验证缓存信息是否有敏感信息，通过抓包方式验证是否暴露了敏感信息 13 直接URL链接检查 在Web系统中，匿名在地址栏直接输入各个功能页面的URL地址，检查系统是否处理了权限控制 预防方法：  开发：代码走查的方式确认所有页面的具有权限验证逻辑  测试：获取所有系统url，在非登录情况下进行遍历截图，或关键字判断，验证非登录状态下无法访问具有访问权限限定的 14 防止sql注入和跨站攻击  不要把数据库或者程序的任何报错信息显示在页面上。  数据库中设计到操作权限的表名和字段名不要使用过于通俗易懂的命名，尤其是用户和密码之类的信息，禁止使用明文存储密码  页面回显的input text, input hidden中的文本内容需过滤 “ &lt;、 &gt;、 ”、 ’等字符（半角转换为全角或者删除掉），防止 Javascript 的跨站攻击 预防方法：  开发：出错的时候使用错误处理页面，建立标准的过滤关键字程序，统一数据库设计命名规范将敏感的表名做特殊命名处理，密码使用Md5或其他加密方式保存  测试：验证所有页面不会暴露系统的任何出错信息使用安全工具appscan 或其他工具扫描系统的sql注入漏洞和跨站攻击漏洞 15 关于cookie Cookie没有设定过期时间IE不支持Cookie的时候没有任何提示信息Cookie中的敏感信息没有进行加密 预防方法：  开发：明确cookie生存期，并对生成的cookie进行检查，建立标准的检查浏览器对cookie支持的程序函数  测试：检查cookie的生存周期，以及是否存在敏感内容 16 各种资源链接的释放 有的时候，系统莫名访问不了，有可能是数据库连接没有释放压力测试的时候，连接释放如果效率不高，则有可能出现大量连接超时失败内存泄露，长时间工作内存被占满了。 预防方法：  开发：系统资源的释放过程，最好通过代码review的方式来互相监督  测试：进行稳定性测试，验证长时间工作情况下的资源是否可以释放 关于keepalive的设置： 如果需要在一个连接同时获取多个资源，则需要打开apache或者resin的Keepalive参数为On，来提高系统的处理能力，减少多次建立连接所消耗的资源。如果大量的处理只是一次性连接，则不要打开Keepalive设置。在实际工作中，需要将keepalive分别设置On或者Off来验证哪个设置的性能更好。 17 系统上线的log配置 上线以后，要关闭无用大量调试log信息不要打开过多的log 预防方法： 运维和开发：系统管理员对所有打开log级别进行确认，并群发相关人确认 18 用户易用性 用户删除某个数据前，要明确提示用户是否要删除，默认把焦点选择为“否”。 预防方法：  开发：按照上述要求进行焦点设定  测试：进行测试确认 19 文档 程序实现和接口文档描述不一致 预防方法：  开发：团队中专人定期对接口文档进行审核和更新，保证文档、需求变更和程序实现保持一致  测试：仅参照文档进行测试 20 多表操作 详细设计文档缺失，接口对多表进行操作时候，经常会发生有些表的数据没有被更新的情况 预防方法：  开发：审核设计文档是否覆盖必要的逻辑，加强代码审查  测试：通过查询接口判断所有插入接口的数据库操作是否正确 等等，这些我们完全可以在不断测试过程中进行总结和积累，可以给开发进行培训，让他们了解这些常见的问题，在自测时注意这些问题，提高送测产品的质量。 产品测试规范纲要 目录 第1章 产品测试规范 1.1 产品测试流程 1.1.1 测试流程图 1.1.2 测试流程说明 1.2 需求梳理 1.2.1 需求梳理 1.3 测试计划 1.3.1 测试工具选取 1.3.2 测试人员分配 1.3.3 测试业务场景选取 1.3.4 测试环境梳理 1.3.5 测试数据梳理 1.4 测试准备 1.4.1 代码管理 1.4.2 测试环境搭建 1.4.3 测试数据脚本编写 1.5 测试用例编写(功能测试框架) 1.5.1 界面友好性测试 1.5.2 功能测试 1.5.3 业务流程测试(主要功能测试) 1.5.4 链接测试 1.5.5 容错测试 1.5.6 稳定性测试 1.5.7 常规性能测试 1.5.8 易用性测试 1.5.9 兼容性测试 1.6 测试执行 1.6.1 接口自动化测试 1.6.2 探索式测试 1.6.3 传统测试用例测试 1.6.4 Bug跟踪 1.7 测试结果分析 1.7.1 结果收集 1.7.2 结果分析 1.7.3 测试分析报告 1.8 上线准备 1.8.1 版本发布 1.8.2 数据准备 1.9 上线测试跟踪 1.9.1 跟踪测试 1.10 BUG预防体系 [TOC] 1.10.2 app常见产品问题及预防 1.11 BUG管理规范 1.11.1 bug提交规范 1.11.2 bug级别定义","categories":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/categories/产品测试/"}],"tags":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/tags/产品测试/"}]},{"title":"产品测试规范（八: Bug预防体系）","slug":"产品测试规范八Bug预防体系","date":"2016-10-13T01:38:02.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/10/13/产品测试规范八Bug预防体系/","link":"","permalink":"http://wysh.site/2016/10/13/产品测试规范八Bug预防体系/","excerpt":"","text":"1.10 Bug预防体系1.10.1 web常见产品问题及预防测试人员在每次版本迭代中，会对项目的整体质量有一个把控，对于项目常见的问题，开发经常犯的错误都会有所了解，为了避免或者减少这样的错误或不规范的事情在发生，测试人员可以整理构建属于产品的bug预防体系，总结项目经常出现bug的种类、位置、以及可以提出针对性的规避措施，提高产品质量。 1F 分辨率兼容性 产品的网页通常保证在1024768的分辨率下显示正常，但是常常忽略800600分辨率下的显示情况，还有其他特殊要求的分辨率 如果页面设计明确只考虑1024768的需求，则只在1024768下验证各个产品页面的显示正确无误 预防方法： 产品：需要明确产品需要兼容的常见屏幕分辨率 开发：网页页面的设计需要针对多种屏幕分辨率制定设计规范，并依据设计规范进行开发 测试：在不同分辨率下验证页面显示的兼容正确性 2F 浏览器兼容性 目前市场上的主流浏览器如下： a. IE 6.0-11 b. 360 浏览器 c. 猎豹浏览器 d. QQ 浏览器 e. Chrome 浏览器 f. FireFox 浏览器 通常情况下要保证IE 6-11和360 浏览器下的兼容性，需要保证页面不变型，Js执行均正确 预防方法： 产品：依据主流的浏览器市场占比，评估你需要兼容的浏览器 开发：针对需要兼容的浏览器类型和版本，指定浏览器兼容设计开发规（ CSS和Js 为主），并不断总结兼容性的经验教训 测试：在产品要求兼容的浏览器类型和版本下，进行兼容性测试 3F Link问题 所有链接是否按指示那样确实链接到了该链接的页面  所链接的页面是否存在  保证Web应用系统上没有孤立的页面，所谓孤立页面是指没有链接指向该页面  链接的打开方式是否合理（在当前窗口中打开、打开新窗口）  有死链 预防方法：  产品：提供的需求中明确是否需要链接以及链接的位置以及链接的打 开方式  测试：死链测试可以采用工具自动进行 4F 快捷键和焦点 Tab键和焦点的切换：在测试的页面中使用Tab键可以在全页面的所有元素进行焦点切换、并且要将相邻元素的 tab键切换顺序做到关联。如： a. 用户打开登录首页，则焦点应该默认显示在用户名输入框中 b. 在用户名输入框输入用户名之后，按下tab 键后，焦点应该切换到密码输入框中，而不是切换到其他元素上。 c. 输入密码后，按下tab键可将焦点切换到“保存密码”的复选框或者登录按钮以上操作，均对偏好使用快捷键的用户给于更友好的支持。预防方法：  产品：考虑页面的默认焦点设定位置，设定tab键在界面上切换焦点的顺序  开发：依据产品人员的要求实现默认焦点位置，和tab键的切换顺序  测试：验证默认焦点位置和tab切换的顺序 5F 前进、后退和刷新 IE 有一个特性：就是允许前进、后退到某一个页面或在当前页面刷新，在某些特殊业务场景的要求下，用户进行前进、后退和刷新当前页面的操作，会造成数据不完整、校验失败或者重复提交的情况。 预防方法：  产品：明确哪些敏感页面不允许前进、后退和刷新，一般情况下充值和支付等相关的页面或者其他数据提交页面禁止后退和刷新后提交。  开发：从技术层面考虑后退和前进操作是否会造成系统漏洞，让用户重复充值或者支付。如果用户尝试后退，则让页面强制失效或者禁止后退。  测试：和产品确认禁止后退的操作限制页面，进行针对性测试 6F 页面/JS/程序提示语言 通常情况下，产品人员并不会将产品需求细化到某句话应该如何提示用户，所以不同的程序员会根据自己的语言特点来提示用户，这就造成了不同程序员提示的语言风格完全不一样，造成产品友好度下降。 预防方法：  产品：产品人员和开发人员一起制定尽可能大而全的产品提示语言规范，并且作为规范说明提供给开发人员进行使用。  开发：遵守语言说明规范，并且针对各种系统的要求不断补充和规范提示  测试：测试过程中，验证语言是否符合指定的语言规范 语言文字提示： a. 全角字符和半角字符都要使用一个空格分开 b. 英文和数字之间要有空格分开 c. 汉字和英文、数字要有空格分开 d. 带有汉字的话要使用全角字符 e. 语言中不要混用全角和半角标点 f. 在语言中，永远不要用“你”这个字，要做一些操作步骤描述的时候，要多用“请”字 7F 文字缩略和折行 输入框提交很长的纯英文字母或者数字（不带任何全角字符和中文），并且不换行，则提交数据后，页面可能被此相关字符拉伸的特别长。 预防方法：  开发：提交公共处理字符的程序，解决上述问题，在所有输入框中增加相关处理  测试：所有输入框需要进行此输入测试，保证页面不会被用户的恶意输入拉长 8F 图片的显示和链接 图片是否增加链接通常会被开发人员忽略掉图片的显示位置通常会显示不同像素大小和比例的图，所以需要明确定义大图片如何缩减成为小图片的策略，以及小图片如何拉伸显示为大的图片。 预防方法：  产品：提供的需求中明确图片是否需要链接以及链接的url地址以及点击后实在当前页打开，还是弹出新页面打开。明确用户上传图片的显示方法，采用等比缩放，还是原大小显示，还是自适应显示  开发： 按照产品要求进行开发，针对图像的显示开发统一显示模块  测试：点击图片链接，验证图片链接的正确性和打开方式是否符合产品设计要求。传不同格式的图片（长方形图、正方形的图、原型图、超大图和超小图)，验证图片显示策略符合产品 9F 重复提交 用户提交数据页面，用户有可能连续多次点击提交按钮，造成数据的重复提交。 黑客或者不良用户通过抓包可以获取提交的url，进行尝试重复提交。 预防方法：  开发：点击“提交”后，将按钮变为Disable状态，禁止用户再次点击。针对每条提交的数据需要增加校验参数，方式不良用户通过其他工具恶意提交。  测试：通过页面验证按钮点击后的状态，通过工具发送重复提交的请求，验证系统是否可以处理重复提交的问题（金融系统需重点测试） 10F 输入判断问题  所有键盘输入的特殊字符，均可以正常保存  需要特别处理英文单引号、英文双引号等引起程序错误的问题  需要处理“ &lt;”、“ /”和“ \\”等容易保存出错的字符  数字框只能输入数字的内容  日期框需要判断日期是否合法  文本框需要判断字段长是否限制了  对于空格的处理，如果系统想trim掉字符串最开头和最后的空格，则需要整个儿系统都使用此策略，否则会造成数据传递不一致的问题  需要前台页面使用js来判断输入的合法性，同时后台逻辑也要添加判断输入合 预防方法：  开发：开发公共处理特殊字符的模块，在系统中进行规范应用  测试：对所有输入字段，进行输入判断测试，超长、空、特殊字符、 utf8字符等，并验证其他页面输入有效性，验证前台和后台均加有输入判断逻辑 产品测试规范纲要 目录 第1章 产品测试规范 1.1 产品测试流程 1.1.1 测试流程图 1.1.2 测试流程说明 1.2 需求梳理 1.2.1 需求梳理 1.3 测试计划 1.3.1 测试工具选取 1.3.2 测试人员分配 1.3.3 测试业务场景选取 1.3.4 测试环境梳理 1.3.5 测试数据梳理 1.4 测试准备 1.4.1 代码管理 1.4.2 测试环境搭建 1.4.3 测试数据脚本编写 1.5 测试用例编写(功能测试框架) 1.5.1 界面友好性测试 1.5.2 功能测试 1.5.3 业务流程测试(主要功能测试) 1.5.4 链接测试 1.5.5 容错测试 1.5.6 稳定性测试 1.5.7 常规性能测试 1.5.8 易用性测试 1.5.9 兼容性测试 1.6 测试执行 1.6.1 接口自动化测试 1.6.2 探索式测试 1.6.3 传统测试用例测试 1.6.4 Bug跟踪 1.7 测试结果分析 1.7.1 结果收集 1.7.2 结果分析 1.7.3 测试分析报告 1.8 上线准备 1.8.1 版本发布 1.8.2 数据准备 1.9 上线测试跟踪 1.9.1 跟踪测试 [TOC] 1.10.2 app常见产品问题及预防 1.11 BUG管理规范 1.11.1 bug提交规范 1.11.2 bug级别定义","categories":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/categories/产品测试/"}],"tags":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/tags/产品测试/"}]},{"title":"产品测试规范（七:测试结果分析）","slug":"产品测试规范七测试结果分析","date":"2016-10-02T08:47:35.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/10/02/产品测试规范七测试结果分析/","link":"","permalink":"http://wysh.site/2016/10/02/产品测试规范七测试结果分析/","excerpt":"","text":"1.7 测试结果分析1.7.1 结果收集包括测试脚本测试结果，测试用例执行结果、服务器操作系统资源监控结果、数据库资源监控、web服务器监控、中间件服务器监控等结果的收集，如：功能测试测试用例数目，成功失败数，性能测试结果，各服务器资源监控结果，磁盘，io，内存消耗进程图等。 这些收集的结果能帮助测试、产品进行测试结果的分析，哪些问题放到下一个版本中进行解决也可以通过这个来进行规划。 1.7.2 结果分析根据收集的测试结果，分析系统的稳定性，健壮性，功能测试可以通过结果分析得到版本的bug率，严重bug数、bug返工率等，对于系统后续优化有很大帮助；性能测试通过结果分析知道系统的性能指标，来判断本次系统迭代性能是否有提高，或者对于一个从无到有的系统来说，能预估系统在未来的某段时间能否承受住那么大的业务量。 1.7.3 测试分析报告根据分析的结果，生成测试分析报告，给定系统的稳定性指标，让系统相关人员知道该版本的质量情况，提供项目上线的风险评估，如果技术可以，还可以提供针对项目问题的改进计划，帮助提高产品质量。如果系统的性能不达标还需考虑后续系统的调优工作，可以找项目相关负责人，dba等相关专家，一起来做性能调优工作，因为性能调优是一项复杂的工作，仅靠测试人员自己之力一般很难做好调优工作，所以可以借助集体的力量共同完成，调优工作完成后，还需回环在进行一次测试工作，验证调优的效果。 1.8 上线准备1.8.1 版本发布测试合格的代码可以进行版本发布工作，版本发布需要给出：发布包、发布文档、数据库脚本等材料，发布文档包括：用户手册、管理员手册、版本发布说明、对于首次发布还需提供产品发布说明、部署手册、测试分析报告等相关文档，这样每次的版本迭代都有相应的文档等材料一一对应，为项目更长远的发展打下基础。 1.8.2 数据准备上线测试跟踪需要做好测试的准备工作，如线上数据准备，版本回退方案准备等，所有测试可能用到的脚本都应提前准备好，避免测试时手忙脚乱，影响效率。 1.9 上线测试跟踪1.9.1 跟踪测试系统上线后，可以做接口自动化的快速轮询测试，保证系统常用接口功能正常；对于版本迭代的功能要进行局部功能重点验证，看功能是否正常；常规的测试可以按照探索式测试+传统测试用例测试来进行，更全面的检查系统功能点；在跟踪测试过程中应该做好bug的记录工作，对于严重性bug需要开发修改后进行在一轮的验证测试，对于业务影响不大，如界面某个友好性提示问题，需做好问题记录，务必在下一次版本中优化掉，提高用户体验度的同时兼顾项目的实际情况。 可能用到的脚本都应提前准备好，避免测试时手忙脚乱，影响效率。 1.10 Bug预防体系1.10.1 web常见产品问题及预防测试人员在每次版本迭代中，会对项目的整体质量有一个把控，对于项目常见的问题，开发经常犯的错误都会有所了解，为了避免或者减少这样的错误或不规范的事情在发生，测试人员可以整理构建属于产品的bug预防体系，总结项目经常出现bug的种类、位置、以及可以提出针对性的规避措施，提高产品质量。 1F 分辨率兼容性 产品的网页通常保证在1024768的分辨率下显示正常，但是常常忽略800600分辨率下的显示情况，还有其他特殊要求的分辨率 如果页面设计明确只考虑1024768的需求，则只在1024768下验证各个产品页面的显示正确无误 预防方法： 产品：需要明确产品需要兼容的常见屏幕分辨率 开发：网页页面的设计需要针对多种屏幕分辨率制定设计规范，并依据设计规范进行开发 测试：在不同分辨率下验证页面显示的兼容正确性 2F 浏览器兼容性 目前市场上的主流浏览器如下： a. IE 6.0-11 b. 360 浏览器 c. 猎豹浏览器 d. QQ 浏览器 e. Chrome 浏览器 f. FireFox 浏览器 通常情况下要保证IE 6-11和360 浏览器下的兼容性，需要保证页面不变型，Js执行均正确 预防方法： 产品：依据主流的浏览器市场占比，评估你需要兼容的浏览器 开发：针对需要兼容的浏览器类型和版本，指定浏览器兼容设计开发规（ CSS和Js 为主），并不断总结兼容性的经验教训 测试：在产品要求兼容的浏览器类型和版本下，进行兼容性测试 产品测试规范纲要 目录 第1章 产品测试规范 1.1 产品测试流程 1.1.1 测试流程图 1.1.2 测试流程说明 1.2 需求梳理 1.2.1 需求梳理 1.3 测试计划 1.3.1 测试工具选取 1.3.2 测试人员分配 1.3.3 测试业务场景选取 1.3.4 测试环境梳理 1.3.5 测试数据梳理 1.4 测试准备 1.4.1 代码管理 1.4.2 测试环境搭建 1.4.3 测试数据脚本编写 1.5 测试用例编写(功能测试框架) 1.5.1 界面友好性测试 1.5.2 功能测试 1.5.3 业务流程测试(主要功能测试) 1.5.4 链接测试 1.5.5 容错测试 1.5.6 稳定性测试 1.5.7 常规性能测试 1.5.8 易用性测试 1.5.9 兼容性测试 1.6 测试执行 1.6.1 接口自动化测试 1.6.2 探索式测试 1.6.3 传统测试用例测试 1.6.4 Bug跟踪 [TOC] 1.10.2 app常见产品问题及预防 1.11 BUG管理规范 1.11.1 bug提交规范 1.11.2 bug级别定义","categories":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/categories/产品测试/"}],"tags":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/tags/产品测试/"}]},{"title":"产品测试规范（六: 测试执行）","slug":"产品测试规范六测试执行","date":"2016-09-19T00:27:52.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/09/19/产品测试规范六测试执行/","link":"","permalink":"http://wysh.site/2016/09/19/产品测试规范六测试执行/","excerpt":"","text":"1.6 测试执行1.6.1 接口自动化测试搭建好的接口自动化流程，可以方便快速构建一次接口测试，这样能很快定位版本接口是不是基本没有问题，提高版本质量。 目前接口自动化测试在测试工具选取中也谈到了，主要有：jmeter、robotframework、自定义框架等，自动化测试的执行可以版本上线后手动触发执行，也可以用定时任务自动触发，或者用工具来进行自动化构建，不变的初衷是用程序或者工具来替代掉一部分的人力操作，让节省出来的人力更好的投入到测试当中。 如：一套自定义的测试框架，java+testng+maven+jenkins，版本测试时，Jenkins自动构建运行java+testng+maven框架脚本，去运行事先编写好的接口脚本，生成测试报告，对于测试接口异常的点进行邮件或者短信告警等，这样运维人员能在第一时间知道版本的质量，异常的接口是哪些，减少人工去一个一个核查接口正确性的时间消耗，有更快或更多的时间去处理异常和维护接口。而且一般项目对于接口的变动不会太大，不会全盘重构一般都是新增某些接口，或者修改一些接口，这样接口脚本只需跟着稍微调整即可，复用性很强，在很多项目上的实验都证明接口自动化测试带来的收益很大。 1.6.2 探索式测试探索性测试强调测试人员的主观能动性，抛弃繁杂的测试计划和测试用例设计过程，没有很多实际的测试方法、技术和工具，强调在碰到问题时及时改变测试策略。 探索性测试强调测试设计和测试执行同时性，完全抛开测试用例，使用定义的比较笼统的测试用例，则称之为探索式测试。 测试人员可以根据收集到的信息，天马行空，自由发挥；测试结果、测试实例和测试文档在测试执行时创建；探索式测试适用于“敏捷开发过程”。 在用传统的测试用例执行测试的同时，可以使用探索性测试来让测试用例更加的丰富和富有变化，提高测试代码的覆盖率，发现产品更多的问题。 1.6.3 传统测试用例测试传统用例的设计方式有：等价类划分法、边界值、正交实验、因果图、功能图、场景法、错误推测、随机测试、对象属性分析测试等方法，根据这些方法可以选取一种或者多种适合系统的设计方法来编写和设计我们的测试用例，让自己的测试有条理，尽可能多的覆盖测试点，提高产品的质量。 这里给出一个等价类划分法结合边界值方法的测试用例设计例子： 某报表处理系统要求用户输入处理报表的日期，日期限制在2001年1月至2008年12月，即系统只能对该段期间内的报表进行处理，如日期不在此范围内，则显示输入错误信息。系统日期规定由年、月的6位数字字符组成，前四位代表年，后两位代表月： 分析输入条件有：200101到200812；6位；数字 等价类表： 测试用例： 那么根据这些测试用例我们就能很好的测试这个“用户输入处理报表的日期”的功能，其他的功能点类推，我们根据1.4中准备好的功能测试框架进行套用，每个模块都按预期设计的方案来进行测试，这样就能保证一些常规部分的功能点更多的被覆盖到。 1.6.4 Bug跟踪测试人员在测试过程中对于遇到的bug需要进行记录和跟踪，不要觉得不严重的bug口头上说一声或者其他形式表达一下就可以不用记录了，因为bug的记录有利于产品领导了解产品的质量情况，有很多bug管理工具，如：readmine、禅道等，从测试用例到bug生成，指派给开发，返工次数，每次解决的理由到最后关闭即整个的bug生命周期都能做到很好的管控，帮助产品经理或项目经理进行下一步的产品优化、以及对产品质量做一个把控。 产品测试规范纲要 目录 第1章 产品测试规范 1.1 产品测试流程 1.1.1 测试流程图 1.1.2 测试流程说明 1.2 需求梳理 1.2.1 需求梳理 1.3 测试计划 1.3.1 测试工具选取 1.3.2 测试人员分配 1.3.3 测试业务场景选取 1.3.4 测试环境梳理 1.3.5 测试数据梳理 1.4 测试准备 1.4.1 代码管理 1.4.2 测试环境搭建 1.4.3 测试数据脚本编写 1.5 测试用例编写(功能测试框架) 1.5.1 界面友好性测试 1.5.2 功能测试 1.5.3 业务流程测试(主要功能测试) 1.5.4 链接测试 1.5.5 容错测试 1.5.6 稳定性测试 1.5.7 常规性能测试 1.5.8 易用性测试 1.5.9 兼容性测试 [TOC] 1.7 测试结果分析 1.7.1 结果收集 1.7.2 结果分析 1.7.3 测试分析报告 1.8 上线准备 1.8.1 版本发布 1.8.2 数据准备 1.9 上线测试跟踪 1.9.1 跟踪测试 1.10 BUG预防体系 1.10.1 web常见产品问题及预防 1.10.2 app常见产品问题及预防 1.11 BUG管理规范 1.11.1 bug提交规范 1.11.2 bug级别定义","categories":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/categories/产品测试/"}],"tags":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/tags/产品测试/"}]},{"title":"产品测试规范（五: 测试用例编写-功能测试框架）","slug":"产品测试规范五测试用例编写-功能测试框架","date":"2016-09-11T23:57:16.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/09/12/产品测试规范五测试用例编写-功能测试框架/","link":"","permalink":"http://wysh.site/2016/09/12/产品测试规范五测试用例编写-功能测试框架/","excerpt":"","text":"功能测试框架可以包括：界面友好性测试、功能测试、链接测试、容错测试、稳定性测试、常规性能测试、配置测试、算法测试等等。 1.5.6 稳定性测试 系统不间断运行（7*24），验证是否内存泄露、系统其他资源是否存在泄露 如果很紧急上线，可以跑一晚上或者周末跑两天。 一般压力很大的情况下，数据库连接数问题、内存泄露问题会曝露的比较快但是死锁可能不能体现，所以要看系统重要性，如12306稳定性则最好7*24小时 1.5.7 常规性能测试 连接速度测试 用户连接到Web应用系统的速度根据上网方式的变化而变化，他们或许是电话拨号，或是宽带上网。当下载一个程序时，用户可以等较长的时间，但如果仅仅访问一个页面就不会这样。如果Web系统响应时间太长（例如超过5秒钟），用户就会因没有耐心等待而离开。 另外，有些页面有超时的限制，如果响应速度太慢，用户可能还没来得及浏览内容，就需要重新登陆了。而且，连接速度太慢，还可能引起数据丢失，使用户得不到真实的页面。 负载测试 负载测试是为了测量Web系统在某一负载级别上的性能，以保证Web系统在需求范围内能正常工作。负载级别可以是某个时刻同时访问Web系统的用户数量，也可以是在线数据处理的数量。例如：Web应用系统能允许多少个用户同时在线？如果超过了这个数量，会出现什么现象？Web应用系统能否处理大量用户对同一个页面的请求？ 压力测试 负载测试应该安排在Web系统发布以后，在实际的网络环境中进行测试。因为一个企业内部员工，特别是项目组人员总是有限的，而一个Web系统能同时处理的请求数量将远远超出这个限度，所以，只有放在Internet上，接受负载测试，其结果才是正确可信的。 进行压力测试是指实际破坏一个Web应用系统，测试系统的反映。压力测试是测试系统的限制和故障恢复能力，也就是测试Web应用系统会不会崩溃，在什么情况下会崩溃。黑客常常提供错误的数据负载，直到Web应用系统崩溃，接着当系统重新启动时获得存取权。 压力测试的区域包括表单、登陆和其他信息传输页面等。 1.5.8 易用性测试 系统界面的控件是否可以通过tab键遍历，并且顺序合理 主要功能的入口和操作是否易于理解 界面是否布局合理，功能是否易于查找和使用 操作步骤 操作习惯 有足够的提示信息，且信息文字描述准确 1.5.9 兼容性测试兼容性测试不只是指界面在不同操作系统或浏览器下的兼容，有些功能方面的测试，也要考虑到兼容性，包括操作系统兼容和应用软件兼容，可能还包括硬件兼容。 比如涉及到ajax、jquery、javascript等技术的，都要考虑到不同浏览器下的兼容性问题。 除了上面所说的这些测试以外，还有算法测试、配置测试、安全性测试等等，在工作中不断总结和分析，形成自己的功能测试框架，当你把这份工作做起来以后，对于你自己对于测试团队而言都是一份很有价值的事情，你的测试思路也会变得更全面。 产品测试规范纲要 目录 第1章 产品测试规范 1.1 产品测试流程 1.1.1 测试流程图 1.1.2 测试流程说明 1.2 需求梳理 1.2.1 需求梳理 1.3 测试计划 1.3.1 测试工具选取 1.3.2 测试人员分配 1.3.3 测试业务场景选取 1.3.4 测试环境梳理 1.3.5 测试数据梳理 1.4 测试准备 1.4.1 代码管理 1.4.2 测试环境搭建 1.4.3 测试数据脚本编写 1.5 测试用例编写(功能测试框架) 1.5.1 界面友好性测试 1.5.2 功能测试 1.5.3 业务流程测试(主要功能测试) 1.5.4 链接测试 1.5.5 容错测试 [TOC] 1.6 测试执行 1.6.1 接口自动化测试 1.6.2 探索式测试 1.6.3 传统测试用例测试 1.6.4 Bug跟踪 1.7 测试结果分析 1.7.1 结果收集 1.7.2 结果分析 1.7.3 测试分析报告 1.8 上线准备 1.8.1 版本发布 1.8.2 数据准备 1.9 上线测试跟踪 1.9.1 跟踪测试 1.10 BUG预防体系 1.10.1 web常见产品问题及预防 1.10.2 app常见产品问题及预防 1.11 BUG管理规范 1.11.1 bug提交规范 1.11.2 bug级别定义","categories":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/categories/产品测试/"}],"tags":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/tags/产品测试/"}]},{"title":"产品测试规范（四：测试用例编写-功能测试框架）","slug":"产品测试规范四测试用例编写-功能测试框架","date":"2016-08-15T09:37:49.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/08/15/产品测试规范四测试用例编写-功能测试框架/","link":"","permalink":"http://wysh.site/2016/08/15/产品测试规范四测试用例编写-功能测试框架/","excerpt":"","text":"功能测试框架可以包括：界面友好性测试、功能测试、链接测试、容错测试、稳定性测试、常规性能测试、配置测试、算法测试等等。 1.5.2 功能测试 使用所有默认值进行测试 根据所有产品文档、帮助文档中描述的内容要进行遍历测试 输入判断 所有界面出现是和否的逻辑，要测试 异常处理 敏感词 根据需求文档的流程图遍历所有流程图路径 根据程序内容，遍历if elif else switch的逻辑点要遍历 界面各种控件测试 如对于输入框测试： 字符型输入框： 字符型输入框： 英文全角、英文半角、数字、空或者空格、特殊字符“~！@#￥%……&amp;*？[]{}”特别要注意单引号和&amp;符号。禁止直接输入特殊字符时，使用“粘贴、拷贝”功能尝试输入。 长度检查： 最小长度、最大长度、最小长度-1、最大长度+1、输入超工字符比如把整个文章拷贝过去。 空格检查： 输入的字符间有空格、字符前有空格、字符后有空格、字符前后有空格 多行文本框输入： 允许回车换行、保存后再显示能够保存输入的格式、仅输入回车换行，检查能否正确保存（若能，检查保存结果，若不能，查看是否有正常提示）、 安全性检查： 输入特殊字符串 （null,NULL, ,javascript,, 数值型输入框： 边界值： 最大值、最小值、最大值+1、最小值-1 位数： 最小位数、最大位数、最小位数-1最大位数+1、输入超长值、输入整数 异常值、特殊字符： 输入空白（NULL）、空格或”~!@#$%^&amp;*()_+{}|[]\\:”&lt;&gt;?;’,./?;:’-=等可能导致系统错误的字符、禁止直接输入特殊字符时，尝试使用粘贴拷贝查看是否能正常提交、word中的特殊功能，通过剪贴板拷贝到输入框，分页符，分节符类似公式的上下标等、数值的特殊符号如∑，㏒，㏑，∏，+，-等、输入负整数、负小数、分数、输入字母或汉字、小数（小数前0点舍去的情况，多个小数点的情况）、首位为0的数字如01、02、科学计数法是否支持1.0E2、全角数字与半角数字、数字与字母混合、16进制，8进制数值、货币型输入（允许小数点后面几位）、 安全性检查： 不能直接输入就 copy 日期型输入框： 合法性检查： (输入0日、1日、32日)、月输入[1、3、5、7、8、10、12]、日输入[31]、月输入[4、6、9、11]、日输入[30][31]、输入非闰年，月输入[2]，日期输入[28、29]、输入闰年，月输入[2]、日期输入[29、30]、月输入[0、1、12、13] 考虑开始日期与结束日历的比较，特别是在查询的时候. 异常值、特殊字符： 输入空白或NULL、输入~！@#￥%……&amp;*（）{}[]等可能导致系统错误的字符 安全性检查： 不能直接输入，就copy，是否数据检验出错？ 1.5.3 业务流程测试(主要功能测试)业务流程，一般会涉及到多个模块的数据，所以在对业务流程测试时，首先要保证单个模块功能的正确性，其次就要对各个模块间传递的数据进行测试，这往往是容易出现问题的地方，测试时一定要设计不同的数据进行测试。 如某一功能模块具有最基本的增删改查功能，则需要进行以下测试： 单项功能测试（增加、修改、查询、删除） 增加——&gt;增加——&gt;增加 （连续增加测试） 增加——&gt;删除 增加——&gt;删除——&gt;增加 （新增加的内容与删除内容一致） 增加——&gt;修改——&gt;删除 修改——&gt;修改——&gt;修改 （连续修改测试） 修改——&gt;增加（新增加的内容与修改前内容一致） 修改——&gt;删除 修改——&gt;删除——&gt;增加 （新增加的内容与删除内容一致） 删除——&gt;删除——&gt;删除 （连续删除测试） 1.5.4 链接测试主要是保证链接的可用性和正确性，它也是网站测试中比较重要的一个方面。 可以使用特定的工具如XENU来进行链接测试。 1.5.5 容错测试 输入系统不允许的数据作为输入 把某个相关模块或者子系统停掉，验证对当前系统的影响 配置文件删除或者配置错误 数据库注入错误数据 产品测试规范纲要 目录 第1章 产品测试规范 1.1 产品测试流程 1.1.1 测试流程图 1.1.2 测试流程说明 1.2 需求梳理 1.2.1 需求梳理 1.3 测试计划 1.3.1 测试工具选取 1.3.2 测试人员分配 1.3.3 测试业务场景选取 1.3.4 测试环境梳理 1.3.5 测试数据梳理 1.4 测试准备 1.4.1 代码管理 1.4.2 测试环境搭建 1.4.3 测试数据脚本编写 1.5 测试用例编写(功能测试框架) 1.5.1 界面友好性测试 [TOC] 1.5.6 稳定性测试 1.5.7 常规性能测试 1.5.8 易用性测试 1.5.9 兼容性测试 1.6 测试执行 1.6.1 接口自动化测试 1.6.2 探索式测试 1.6.3 传统测试用例测试 1.6.4 Bug跟踪 1.7 测试结果分析 1.7.1 结果收集 1.7.2 结果分析 1.7.3 测试分析报告 1.8 上线准备 1.8.1 版本发布 1.8.2 数据准备 1.9 上线测试跟踪 1.9.1 跟踪测试 1.10 BUG预防体系 1.10.1 web常见产品问题及预防 1.10.2 app常见产品问题及预防 1.11 BUG管理规范 1.11.1 bug提交规范 1.11.2 bug级别定义","categories":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/categories/产品测试/"}],"tags":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/tags/产品测试/"}]},{"title":"产品测试规范（三：测试准备）","slug":"产品测试规范三测试准备）","date":"2016-08-15T09:37:47.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/08/15/产品测试规范三测试准备）/","link":"","permalink":"http://wysh.site/2016/08/15/产品测试规范三测试准备）/","excerpt":"","text":"1.4 测试准备1.4.1 代码管理所有的产品代码应该统一管理起来，开发人员提交代码应与测试代码地址进行分离，做到高效管理代码，当开发人员提交代码到开发的代码库中，需要进行测试时，测试人员可去开发的代码中进行提取代码到测试基线库中，每提取一次就建立一个测试基线，直到此次版本测试合格，在把合格的测试基线提取到正式基线中用于版本发布，这样每个版本都有清晰的界限和记录，使得产品代码清晰一目了然，可以借助代码管理工具，如svn创建基线来帮助管理产品代码： 1.4.2 测试环境搭建这个需要配合1.3测试计划的1.3.4测试环境梳理文档和部署文档来进行，根据事先规划好的服务器部署应用策略来搭建测试环境，能让你搭建思路更加清晰，以后维护环境也更加方便。 接触很多公司的测试关于环境这块的梳理工作，有的是有专门的服务器管理人员来管理这些环境，有的是由测试人员自己管理，但需要保证的是测试环境应当与开发环境分离开，让测试更加规范减少不必要的麻烦，遇到一些事情如：开发人员很懒，功能开发完成后让他在服务器上验证一下是不是对的，因为开发环境没人去管理部署上去弄得不好应用就报错无法进行调试，所以有的开发就会为方便起见把自己的验证测试直接弄到测试环境上进行，这样带来一个后果就是，你也来部署一个应用，他也来部署一个应用，久而久之测试环境就会特别乱，对测试人员梳理该环境增加不必要的负担，所以建议测试环境的账号应当只有dba或者测试人员自己知道，与开发环境进行分离。 1.4.3 测试数据脚本编写功能性测试数据脚本一般为辅助性测试脚本，如：为了验证分页功能，写一个造数据的脚本让界面出现分页效果，帮助自己测试，减少手动一条一条增加数据的时间。 接口测试需要编写接口测试脚本，目前接口测试比较受欢迎的几款工具有：postman、loadrunner、jmeter、soupui、自定义框架，postman工具可以模拟发送http请求，用来做一些简单的接口验证测试比较方便，测试结果需要人眼去核查是否正确；loadrunner和jmeter工具更加智能化，接口测试支持断言/检查点设置，工具自己校验测试结果，支持参数化以及请求间参数关联，可以做一些复杂的场景流程测试；自定义框架可以结合项目适合进行扩展，比工具要灵活，但是需要测试人员有一定的代码基础才能开发出适合项目的接口自动化框架，如：unittest、testng技术等。 性能测试需要编写性能测试脚本，如loadrunner脚本、jmeter脚本等，脚本涉及参数化的地方也需提前构建好，如果系统并发登录需要大量的登录账户，则需要提前造好数据，可以让用户按规则进行，这样脚本中用户就可以用正则编写一定吻合的规则即可，省去大数据参数化的性能损耗。 测试工具层出不穷，在学习各种测试工具、测试技术的同时，不要忘记基本功，编程能力的提升才是重中之重。 1.5 测试用例编写(功能测试框架)测试用例的编写需要按照一定的思路进行，而不是想到哪写到哪，一般测试机制成熟的公司都会有公司自己自定义的测试用例模板，以及一整套的测试流程关注点，当然我们自己在测试生涯中也应当积累一套自己的测试框架，所有功能性的测试都可以依据框架的思路来进行，达到事半功倍的效果。 功能测试框架可以包括：界面友好性测试、功能测试、链接测试、容错测试、稳定性测试、常规性能测试、配置测试、算法测试等等。 1.5.1 界面友好性测试 风格、样式、颜色是否协调 界面布局是否整齐、协调（保证全部显示出来的，尽量不要使用滚动条 界面操作、标题描述是否恰当（描述有歧义、注意是否有错别字） 操作是否符合人们的常规习惯（有没有把相似的功能的控件放在一起，方便操作） 提示界面是否符合规范（不应该显示英文的cancel、ok，应该显示中文的确定等） 界面中各个控件是否对齐 日期控件是否可编辑 日期控件的长度是否合理，以修改时可以把时间全部显示出来为准 查询结果列表列宽是否合理、标签描述是否合理 查询结果列表太宽没有横向滚动提示 对于信息比较长的文本，文本框有没有提供自动竖直滚动条 数据录入控件是否方便 有没有支持Tab键，键的顺序要有条理，不乱跳 有没有提供相关的热键 控件的提示语描述是否正确 模块调用是否统一，相同的模块是否调用同一个界面 用滚动条移动页面时，页面的控件是否显示正常 日期的正确格式应该是XXXX-XX-XX或XXXX-XX-XX XX:XX:XX 页面是否有多余按钮或标签 窗口标题或图标是否与菜单栏的统一 窗口的最大化、最小化是否能正确切换 对于正常的功能，用户可以不必阅读用户手册就能使用 执行风险操作时，有确认、删除等提示吗 操作顺序是否合理 正确性检查：检查页面上的form, button, table, essay-header, footer,提示信息，还有其他文字拼写，句子的语法等是否正确。 系统应该在用户执行错误的操作之前提出警告，提示信息. 页面分辨率检查，在各种分辨率浏览系统检查系统界面友好性。 合理性检查：做delete, update, add, cancel, back等操作后，查看信息回到的页面是否合理。 检查本地化是否通过：英文版不应该有中文信息，英文翻译准确，专业。 背景灰度冻结 产品测试规范纲要 目录 第1章 产品测试规范 1.1 产品测试流程 1.1.1 测试流程图 1.1.2 测试流程说明 1.2 需求梳理 1.2.1 需求梳理 1.3 测试计划 1.3.1 测试工具选取 1.3.2 测试人员分配 1.3.3 测试业务场景选取 1.3.4 测试环境梳理 1.3.5 测试数据梳理 [TOC] 1.5.2 功能测试 1.5.3 业务流程测试(主要功能测试) 1.5.4 链接测试 1.5.5 容错测试 1.5.6 稳定性测试 1.5.7 常规性能测试 1.5.8 易用性测试 1.5.9 兼容性测试 1.6 测试执行 1.6.1 接口自动化测试 1.6.2 探索式测试 1.6.3 传统测试用例测试 1.6.4 Bug跟踪 1.7 测试结果分析 1.7.1 结果收集 1.7.2 结果分析 1.7.3 测试分析报告 1.8 上线准备 1.8.1 版本发布 1.8.2 数据准备 1.9 上线测试跟踪 1.9.1 跟踪测试 1.10 BUG预防体系 1.10.1 web常见产品问题及预防 1.10.2 app常见产品问题及预防 1.11 BUG管理规范 1.11.1 bug提交规范 1.11.2 bug级别定义","categories":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/categories/产品测试/"}],"tags":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/tags/产品测试/"}]},{"title":"产品测试规范（一：测试流程）","slug":"产品测试规范一测试流程）","date":"2016-08-09T07:46:19.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/08/09/产品测试规范一测试流程）/","link":"","permalink":"http://wysh.site/2016/08/09/产品测试规范一测试流程）/","excerpt":"","text":"第1章 产品测试规范产品测试流程 1.1.1 测试流程图 1.1.2 测试流程说明 需求阶段： 测试人员了解项目需求及需求变更，包括需求规格说明书、功能结构及模块划分，根据需求梳理测试点。 测试计划阶段： 测试计划环节需要考虑测试工具选取，考虑需要测试的业务点，涉及到多业务量测试团队测试，需考虑人员分配问题，如：哪些人准备测试执行，哪些人准备测试过程中数据的收集与整理为后面统一分析做准备。 测试环境梳理为测试需要部署哪些应用，应用是单节点部署还是分布式部署，每个应用分配几台机器进行部署，以及测试工具及监控工具的部署等。 测试数据梳理为测试过程中需要考虑可能用到哪些数据如同时登陆的场景需要不同的用户，测试翻页功能需要的数据量，通过测试数据梳理能够理清可能需要编写哪些辅助脚本来进行测试。 测试场景梳理为根据选取的测试业务点来设计需要测试的场景。 测试准备阶段： 代码管理为分为开发代码、测试基线、正式基线等，测试代码应在测试基线中进行即与开发的代码管理库分离，测试合格的代码才可以分支到正式基线中。 测试环境的搭建工作也需要进行管理，哪些服务器用来搭建哪些应用应当有对应的部署文档以及部署架构图，即测试环境需心中有数且有文档记录，让人一目了然。 测试用例编写可以根据功能测试框架来进行，覆盖到所需测试的模块以及需求中指出的测试点。 测试数据准备为在系统正式测试前就准备好测试时需要的数据，如移动查单需提前准备好手机号码用来测试查询。 测试脚本准备为测试过程中通过手工无法进行或者效率很低可以通过代码来实现的环节，如：登录用户的准备，千万条用户性能测试同时登录系统，需要编写sql脚本来批量生成用户账号数据，又如：接口测试根据接口测试文档预先编写好所有的接口测试脚本。 测试执行阶段： 功能测试可以通过传统测试用例测试+探索式测试一起执行，提高测试产品的质量，性能测试将测试准备阶段准备好的脚本和数据以及部署好的工具，按照写好的测试方案来进行测试，接口测试按照接口测试方案来运行已编写好的脚本。即让所有的测试有条不紊的运行，不是想到哪是哪，而且所有的测试不是一蹴而就的，测试过程中需要进行bug的跟踪，指派给对应的负责人，把握项目的测试进度。 测试结果分析阶段： 根据测试的结果、日志收集结果、资源收集结果、异常跟踪结果等汇总分析生成测试分析报告并给出可行性的建议，如果涉及到调优工作，还需对调优结果进行验证，需要对上线的风险进行评估。 上线准备阶段： 测试人员需要准备线上测试需要用到的数据，需结合生产环境进行，如系统生成订单测试环境是不需要uim卡号的，但是真实的线上环境需要用到uim卡号，这就需要提前准备好线上测试的数据。 上线准备需要提供测试合格的发布资料(包括：发布包、数据库脚本、用户手册、部署文档、维护手册等)、还需要考虑好回滚方案。 上线后测试跟踪阶段： 可以持续构建接口自动化，快速进行一轮接口测试，保证常规接口正常运行，功能测试可以根据测试用例+探索式测试来进行，如果是更新补丁等，需要重点对上线更新的功能进行验证测试，当然测试过程中必不可少要进行bug的跟踪。 项目总结阶段： 对于项目整体的质量做总结分析，给出总结报告，测试人员需要根据每次的测试、上线等积累符合项目的bug预防体系，总结项目经常出现bug的种类、位置、以及可以提出针对性的规避措施，提高产品质量。（待续） 产品测试规范纲要 目录 [TOC] 1.2 需求梳理 1.2.1 需求梳理 1.3 测试计划 1.3.1 测试工具选取 1.3.2 测试人员分配 1.3.3 测试业务场景选取 1.3.4 测试环境梳理 1.3.5 测试数据梳理 1.4 测试准备 1.4.1 代码管理 1.4.2 测试环境搭建 1.4.3 测试数据脚本编写 1.5 测试用例编写(功能测试框架) 1.5.1 界面友好性测试 1.5.2 功能测试 1.5.3 业务流程测试(主要功能测试) 1.5.4 链接测试 1.5.5 容错测试 1.5.6 稳定性测试 1.5.7 常规性能测试 1.5.8 易用性测试 1.5.9 兼容性测试 1.6 测试执行 1.6.1 接口自动化测试 1.6.2 探索式测试 1.6.3 传统测试用例测试 1.6.4 Bug跟踪 1.7 测试结果分析 1.7.1 结果收集 1.7.2 结果分析 1.7.3 测试分析报告 1.8 上线准备 1.8.1 版本发布 1.8.2 数据准备 1.9 上线测试跟踪 1.9.1 跟踪测试 1.10 BUG预防体系 1.10.1 web常见产品问题及预防 1.10.2 app常见产品问题及预防 1.11 BUG管理规范 1.11.1 bug提交规范 1.11.2 bug级别定义","categories":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/categories/产品测试/"}],"tags":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/tags/产品测试/"}]},{"title":"产品测试规范（二：需求梳理）","slug":"产品测试规范二需求梳理","date":"2016-08-09T07:46:19.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/08/09/产品测试规范二需求梳理/","link":"","permalink":"http://wysh.site/2016/08/09/产品测试规范二需求梳理/","excerpt":"","text":"1.2 需求梳理1.2.1 需求梳理根据需求文档、需求规格说明书来对需要测试的功能点进行梳理，而且通过需求文档能够更加了解项目的业务场景，一般情况下，在项目中需求文档有3种现状： 有详细的需求文档： 比较严谨负责的团队项目的实施是有详细的需求文档的，我们就可以详阅需求文档来进行测试点的梳理工作，对于需求中你认为不明确的地方可以找项目领导人进行沟通，做到对需求整体把握和理解，利于测试更好的进行。 需求文档不明确即有文档但是文档很粗糙： 一般有两种办法，如果开发团队很配合，可以要求开发或者需求分析人员完善需求文档，如果因为各种原因比如时间紧张或者开发就是不愿意，那么就需要自己去沟通对于文档中不明确的点问清楚，切记不要含糊不清的测试，于人于己都没有好处。 没有需求文档： 如果你运气很不好遇到了，虽然我很同情你，但是貌似同情没啥用，我们知道做测试很重要的一点是：我有一个预期，我要把软件运行的实际值跟我的预期去比对，如果达到了预期，那么就没问题，如果跟预期不一致那就是有问题。那么如果没有需求，我们该怎么办： 第一种靠嘴去问，大家去协调，协商沟通，然后大家都回答没问题了，我会自己写一个概要的需求描述，然后让他去确认，他说可以，那咱们就这样测，有问题就不断的口头沟通； 第二种要基于用户使用的场景和行业的经验来去做判断它是不是合理的。 1.3 测试计划1.3.1 测试工具选取 测试工具说明： 以上列出了自己在测试过程中所用过的一些工具，每种都有自己的利弊和自适应的测试场景，可以进行参考和根据实际需求来进行分析选取。 1.3.2 测试人员分配测试场景敲定以后，对于大业务量的测试工作或者团队合作测试的任务需要分配好各自的任务，让大家各司其职，如：测试环境梳理和搭建人员、测试数据准备人员、测试脚本编写人员、测试执行人员、测试日志收集人员、测试结果汇总分析人员，每个人可以负责一个模块或者多个模块，更甚者有的项目任务量不多，一个人搞定这么多部分也是大有人在，即一个人搭建环境、一个人准备数据写测试用例准备测试、收集日志进行分析，这对测试人员的要求比较高才能更好保证产品的输出质量。 1.3.3 测试业务场景选取根据需求说明文档，梳理需要测试的业务点和场景，比如应用系统的性能测试，需测试nginx负载节点的性能情况，是否可支撑1000/s的业务能力，极限环境下支撑2万/s并发，节点接收报文常规为几byte，大报文可达到8k，节点支持分布式部署。则我们根据这些信息可以梳理我们需要测试的场景有：直接压测一台节点观察性能峰值、nginx负载一台节点的性能、nginx负载两台节点的性能、nginx负载三台节点的性能、报文场景为500字节、1KB、8KB、并发数为依次递增至1500并发(保证1000/s并发是否可以)，看是否满足常规业务处理能力，极限测试下并发数为2万，测试7*24小时，观察极限处理能力。 1.3.4 测试环境梳理根据测试场景以及梳理的被测系统、压力系统、压力机情况、给定的服务器数量，绘制测试环境搭建图谱即每个应用系统搭建数量、各节点所在机器，如下图梳理了整个系统部署的流程及每个应用、监控工具、测试工具应该部署的机器情况，让人一目了然。 1.3.5 测试数据梳理这里的测试数据内容很广，可包括测试准备和测试执行阶段所需要的一切数据来源，如：测试脚本、测试参数化文件、测试账号、辅助性测试程序等，即让测试工作更加有条不紊的进行，而不是等到测试时才发现这个东西要去找，那个东西又没有弄得自己手忙脚乱，降低测试效率。比如，下面是一段造数据的存储过程脚本： 产品测试规范纲要 目录 第1章 产品测试规范 1.1 产品测试流程 1.1.1 测试流程图 1.1.2 测试流程说明 [TOC] 1.4 测试准备 1.4.1 代码管理 1.4.2 测试环境搭建 1.4.3 测试数据脚本编写 1.5 测试用例编写(功能测试框架) 1.5.1 界面友好性测试 1.5.2 功能测试 1.5.3 业务流程测试(主要功能测试) 1.5.4 链接测试 1.5.5 容错测试 1.5.6 稳定性测试 1.5.7 常规性能测试 1.5.8 易用性测试 1.5.9 兼容性测试 1.6 测试执行 1.6.1 接口自动化测试 1.6.2 探索式测试 1.6.3 传统测试用例测试 1.6.4 Bug跟踪 1.7 测试结果分析 1.7.1 结果收集 1.7.2 结果分析 1.7.3 测试分析报告 1.8 上线准备 1.8.1 版本发布 1.8.2 数据准备 1.9 上线测试跟踪 1.9.1 跟踪测试 1.10 BUG预防体系 1.10.1 web常见产品问题及预防 1.10.2 app常见产品问题及预防 1.11 BUG管理规范 1.11.1 bug提交规范 1.11.2 bug级别定义","categories":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/categories/产品测试/"}],"tags":[{"name":"产品测试","slug":"产品测试","permalink":"http://wysh.site/tags/产品测试/"}]},{"title":"性能测试方案设计思路总结","slug":"性能测试方案设计思路总结","date":"2016-07-07T01:48:28.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/07/07/性能测试方案设计思路总结/","link":"","permalink":"http://wysh.site/2016/07/07/性能测试方案设计思路总结/","excerpt":"","text":"[TOC] 一、需求分析 测试目的 为什么测？目的在于测试系统相关性能能否满足业务需求。通常分以下两种情况： 1）新项目上线 2）老项目优化 如果是老项目优化，可考虑是否存有历史测试方案，如果有可以参考，或许可以省事很多。 测试对象 要测啥？ 测试对象可以归结为“业务功能”。测试前，需要了解我们需要测试的业务功能（不深入细节）有哪些，比如“购买商品”、“寄送快递”。 有没有必要测？ 需求来源哪里？，有没有数据支撑测试这个需求的必要性？ 通常，可以从以下几个方面考虑： 1）是否核心功能，是否要求严格的质量 2）是否常用、高频使用的功能 3）可能占用系统较多资源的功能 4）使用人数多还是少 5）在线人数多还是少 拆分对象 先从业务上来分，实现这个完整的功能包含哪些流程、环节 举例：购买商品 登录-&gt;搜索商品-&gt;提交订单-&gt;支付订单-&gt;退出 然后从功能实现上来看，怎么实现这个完整功能的。通常这些业务功能操作都对应着一个或多个请求(可能能是不同类型的请求，比如http, mysql等)，我们要做的是找出这些操作对应的请求，请求之间的顺序是怎么样的。 指标分析 分析性能需求指标（如“支持300人并发登录”）是否合理 有必要测试这个需求，考虑需求指标是否合理？有没有数据支撑？ 通常，支撑数据可以从以下方面考虑： 1）采样时间段内系统使用人数 2）采样时间段内系统在线人数 3）采样时间段内系统(页面)访问量 4）采样时间段内请求数…. 常用分析思路： 1）2/8法则 2/8法则：80%的业务量在20%的时间里完成。这里，业务量泛指访问量，请求数，数据量等 2）正态分布 3）按比例倍增 4）响应时间2-5-8原则 就是说，一般情况下，当用户能够在2秒以内得到响应时，会感觉系统的响应很快；当用户在2-5秒之间得到响应时，会赶紧系统的响应速度还可以；当用户在5-8秒以内得到响应时，会赶紧系统的速度很慢，但是还可以接受；而当用户在超过8秒后仍然无法得到响应时，会感觉系统糟糕透了，或者认为系统已经失去响应。 注意：这个要根据实际情况，有些情况下时间长点也是可以接受的，好比12306 举例： 某公司后台监控，根据一段时间的采样数据，分析得出日高峰时段(11:00-14:00)用户下单请求数平均为1000，峰值为1500，根据这个计算并发请求数 时段：3个小时 -&gt; 3 x 60 x 60 = 1080s 业务量：1500 吞吐量：1500 80% / (1080 20%) = 5.56请求数/s 假设用户下单遵循正态分布，那么并发请求数峰值会肯定大于上述估算的吞吐量 注意: 1、2/8原则计算的结果并非在线并发用户数，是系统要达到的处理能力（吞吐量） 2、如果要求更高系统性能，根据实际情况，也可以考虑1/9原则或其它更严格的算法 3、以上估值只是大致的估算，不是精确值 举例： 想了下，暂时没想到啥好的例子，大致就说一些涉及到数据量的性能测试，比如报表统计，或者是大数据挖掘，查询等，怎么去估算数据量？ 数据生命周期： 一般来说，数据都是有一定的生命周期的，时间的选取需要结合数据周期考虑。这里假设3年后系统性能仍然需要满足业务需求。 数据增长率： 如果是老项目，可以考虑对应功能主表历史数据存放情况 这里假设按年统计，比如第一年 10000，第二年 15000，第三年 20000，第四年25000，那么我们得出，以第一年为基准，数据增长率分别为 0.5，1，1.5，每年在上一年的基础上，以5000的速度增长 预估3年后，数据增长率为 3，需要测试数据量为 （1+3）x 10000 = 40000 注意： 1、实际数据一般是没上面举例那么规律的，只能大致估算数据增长率。2、一些大数据量的性能测试除了和数据量相关，还涉及到数据分布等，比如查询，构造数据时需要结合实际，尽量贴近实际。 3、不同业务模块，涉及表不一样，数据量要求也是不一样的，需要有区别的对待。 如果是新项目，那就比较不确定了，除非能收集相关数据。 二、系统分析结合需求分析中第3点，分析系统架构。 1）请求顺序、请求之间相互调用关系 2）数据流向，数据是怎么走的，经过哪些组件、服务器等 3）预测可能存在性能瓶颈的环节（组件、服务器等） 4）明确应用类型 IO型，还是CPU消耗性、内存消耗型-&gt;弄清楚重点监控对象 5）关注应用是否采用多进程、多线程架构-&gt; 多线程容易造成线程死锁、数据库死锁，数据不一致等 6）是否使用集群/是否使用负载均衡 了解测试环境部署和生产环境部署差异，是否按1:1的比例部署 通常建议测试时先不考虑集群，采用单机测试，测试通过后再考虑使用集群，这样有个比较，比较能说明问题 参考阅读“浅谈web网站架构演变过程 ”：http://blog.csdn.net/qiaqia609/article/details/50809383 三、业务分析1）明确要测试的功能业务中，功能业务占比，重要程度。 目的在于 明确重点测试对象，安排测试优先级 建模，混合场景中，虚拟用户资源分配，针对不同业务功能施加不同的负载。 2）明确下“需求分析-指标分析”中相关业务功能所需基础数据及数据量问题，因为那块需求分析时可能只是大致估算下，评估指标是否合理，需要认真再分析下 四、用例设计1）用例设计 通常是基于场景的测试用例设计 单业务功能场景 运行测试期间，所有虚拟用户只执行同一种业务功能某个环节、操作 混合业务功能场景 运行测试期间，部分虚拟用户执行某种业务的某个环节操作，部分虚拟用户执行该业务功能的其它环节或者运行测试期间，部分虚拟用户执行某种业务功能，部分虚拟用户执行其它业务功能 注：这里用例没说到多少用户去跑，跑多久等，这里只是把他当作相同场景用例下的的一组组测试数据了。 2）事务定义 根据用例合理的定义事务，方便分析耗时（特别是混合业务功能场景测试），进而方便分析瓶颈。 比如，购买商品，我们可以把下订单定义为一个事务，把支付也定义为一个事务。 3）场景监控对象 针对每条用例，结合“系统分析”第4）点，明确可能的压力点（比如数据库、WEB服务器），需要监控的对象，比如tps，耗时，CPU，内存，I/O等 五、测试策略1）先进行混合业务功能场景的测试，在考虑进行测试单业务功能场景的测试 2）负载测试 -&gt; 压力测试-&gt; 稳定性测试-&gt; 强度测试 注：如果测试稳定性，时间建议至少8小时； 3）逐步加压 比如开始前5分钟，20个用户，然后每隔5分钟，增加20个用户。 好处：不仅比较真实的模拟现实环境，而且在性能指标比较模糊，且不知道服务器处理能力的情况下，可以帮我们确定一个大致基准，因为通常情况下，随着用户数的不断增加，服务器压力也会随着增加，如果服务器不够强大，那么就会出现不能及时处理请求、处理请求失败的情况下，对应的运行结果图形中，运行曲线也会出现对应的形态，比如从原本程一条稳定直线的情况，到突然极限下降、开始上下波动等，通过分析我们就能得出服务器大致处理能力，供后续测试参考。4）单点并发 比如使用集合点，单独针对某个环节的并发测试，通常是针对某个环节的性能调优时使用。 常识： a) 负载测试 保证系统能正常运行(通常是满足某些系统性能指标)的前提下，让被测对象承担不同的工作量，以评估被测对象的最大处理能力及存在缺陷而进行的测试 b) 压力测试 不保证系统能否正常运行的前提下，让被测对象承担不同工作量，以评估被测对象能提供的最大处理能力及存在缺陷而进行的测试 c) 稳定性测试 测试系统的长期稳定运行的能力。同疲劳强度测试的区别是，稳定性测试的压力强度较小，一般趋向于客户现场日常状态下的压力强度，当然在通过时间不能保证稳定性的状态下，需要加大压力强度来测试，此时的压力强度则会高于正常值。 d) 强度测试 通常模拟系统在较差、异常资源配置下运行，如人为降低系统工作环境所需要的资源,如网络带宽,系统内存,数据锁等等,以评估被测对象在资源不足的情况下的工作状态 注：疲劳强度测试是一类特殊的强度测试，主要测试系统长时间运行后的性能表现，例如7x24小时的压力测试。 六、工具选取1）协议分析 一般性能测试工具都是基于协议开发的，所以先要明确应用使用的协议 2）工具选取 1）类型 开源工具、收费工具、自研工具 2）分析工具 理解工具实现原理 采用用异步还是同步 常识： 同步请求：发出一个调用请求，在没有得到结果之前，该调用就不返回。 异步请求：发出一个调用请求，在没有得到请求结果之前，该调用可立即返回。该调用请求的处理者在处理完成后通过状态、通知和回调等来通知调用者。 使用长连接还是短连接 七、 软件配置1）操作系统 内核版本、32 or 64位? 2）应用版本 应用版本要和线上保持一致，特别是中间件、组件等的版本，因为不同版本，其性能可能不一样 3）参数配置 负载均衡、反向代理参数配置 Web服务器参数配置 数据库服务器参数配置 八、网络分析1）网络路由 通常为了排除网络型瓶颈，通常建议在局域网下进行测试。 通常，这里我的分析思路是这样的： 检查hosts文件的配置 从终端压测机(负载生成机)开始，到请求目的服务器器，机器的hosts文件配置 通常，hosts文件位于如下： Windows：C:\\Windows\\System32\\drivers\\etc\\hostsUnix/Linux：/etc/hosts 小常识： 1、通常域名访问站点，首先要通过DNS域名服务器把网络域名（形如www.xxx.com）解析成XXX.XXX.XXX.XXX的IP地址，然后继续后续访问。2、hosts存放了域名和ip地址的映射关系，如下 性能测试方案设计思路总结 使用hosts可以加快域名解析，在进行DNS请求以前，系统会先检查自己的hosts文件中是否有这个地址映射关系，如果有则把域名解析为映射的IP地址，不请求网络上的DNS服务器，如果没有再向已知的DNS 服务器提出域名解析。也就是说hosts的请求级别比DNS高，可加快域名解析。 检查DNS配置 不同DNS，其速度和准确率是不一样的，比如114.114.114.114速度远比8.8.8.8快，如果有用到DNS（特别是压测机），需要考虑下是否适当 确保路由正确设置 2）网络带宽 如果没条件在局域网下测试，可能需要估算所需大致带宽。 如果测试时是基于UI层操作的操作，那么得估算页面平均大小，这个可以通过浏览器自带工具查看打开单个页面服务器返回的请求数据大小。如果是测试时是基于接口层的请求测试，可以通过工具查看服务器响应数据大小。 然后根据采集的页面PV峰值、请求数峰值进行计算。 假设在 PV峰值、请求数峰值 = 1000，峰值时段：8:00 - 12:00，平均页面、请求大小 200k 带宽 = 1000 x 80% / (20% x 4 x 3600s) x 200KB x /1024 x 8bit ,单位MBps 注意： 这里涉及到浏览器缓存等因素，估值可能不准，大致估算。 九、硬件配置1) CPU 型号，频率，核数 2) 内存 3) 磁盘 不同磁盘类型，读写速率不一样 4) 网卡 不同网卡，其传输速率也不一样 注意：硬件配置最好和生产环境的配置保持一致 十、性能监控略 注意： 1） 这里监控不仅仅是服务器自身性能指标监控，如cpu，还包括事务耗时监控等 2） 需要记录测试前各个性能指标数据，方便后续测试对比 十一、 实施测试略 十二、 结果分析如果是性能调优，还需同上一个版本的性能测试结果对比","categories":[{"name":"性能测试","slug":"性能测试","permalink":"http://wysh.site/categories/性能测试/"}],"tags":[{"name":"性能测试","slug":"性能测试","permalink":"http://wysh.site/tags/性能测试/"}]},{"title":"性能测试体系的知识分享","slug":"性能测试体系的知识分享","date":"2016-07-06T14:28:47.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/07/06/性能测试体系的知识分享/","link":"","permalink":"http://wysh.site/2016/07/06/性能测试体系的知识分享/","excerpt":"","text":"开始性能测试前需要了解的内容： 项目具体需求。 指标：响应时间在多少以内，并发数多少，tps多少，总tps多少，稳定性交易总量多少，事务成功率，交易波动范围，稳定运行时长，资源利用率，测哪些交易，哪些接口,测试哪些场景。 环境：生产环境服务器数量，测试环境服务器数量，按照资源配比得出测试指标。 协议：系统用什么协议进行通讯。 压力机数量：如果并发用户数太多，需要把压力发到不同的压力机，不然可能会存在压力机瓶颈问题，导致tps和响应时间抖动。 交易占比：分析线上日志得出tps占比。 系统架构：请求流经过哪些环节，压测时监控这些环节。 测试： 基准：一个用户迭代100次，关注响应时间，事务成功率100%。 负载：10个用户跑10分钟，关注响应时间，事务成功率100%。 容量：估算一个总tps，根据公式计算出每个交易的pacing和vu，获取系统最大处理能力（最优容量），再令外测出三个梯度作为对比（两组小于最优容量，一组大于最优容量），四组容量VU等差，tps等差，对比每组容量实际占比和测试占比（越接近越能模拟真实场景），关注响应时间，总tps，tps，事务成功率，AP cpu利用率，DB cpu利用率，线程死锁，数据库死锁。 其中响应时间应小于负载测试时间，总tps应约等于预估总tps（相差不超过10是正常的），每个交易的tps应接近预估总tps*占比，事务成功率100%，AP cpu小于60%，DB cpu小于80%。dump线程栈检测是否有线程死锁，查看数据库日志看是否有数据库死锁。 稳定性：采取最优容量的80%作为压力持续运行24小时，观察系统长时间运行的性能表现，关注响应时间，tps，总tps，事务成功率，交易总数，观察是否有内存溢出（堆溢出，栈溢出，持久代溢出），cpu利用率是否达标，mem是否不持续增长，是否能正常触发fullgc，gc时间，gc频率， fullgc时间，fullgc频率（重点关注，JVM调优就是为了减少fullgc频率）。 监控：容量测试和稳定性测试时启动nmon监控。 压测中遇到的性能问题及解决办法：一、容量测试过程中cpu过高 用vmstat实时监控cpu使用情况。很小的压力AP cpu却到了80%多，指标是不能超过60%。 分析是use cpu过高还是sys cpu过高，常见的是use cpu使用过高。 如果是sys cpu使用过高，先把消耗cpu最多的进程找出来（top命令），再找到该线程下消耗cpu过高的是哪几个线程，再把该线程转换成16进制，再用jstack命令来dump线程栈，看这个线程栈在调用什么东西导致use cpu过高。 二、内存溢出（堆溢出、栈溢出、持久代溢出） 堆内存溢出 1)稳定性压测一段时间后，LR报错，日志报java.lang.OutOfMemoryError.Java heap space。 2)用jmap -histo pid命令dump堆内存使用情况，查看堆内存排名前20个对象，看是否有自己应用程序的方法，从最高的查起，如果有则检查该方法是什么原因造成堆内存溢出。 3)如果前20里没有自己的方法，则用jmap -dump来dump堆内存，在用MAT分析dump下来的堆内存，分析导出内存溢出的方法。 4)如果应用程序的方法没有问题，则需要修改JVM参数，修改xms，xmx，调整堆内存参数，一般是增加堆内存。 栈内存溢出 1)稳定性压测一段时间后，LR报错，日志报Java.Lang.StackOverflowError。 2)修改jvm参数，将xss参数改大，增加栈内存。 3)栈溢出一定是做批量操作引起的，减少批处理数据量。 持久代溢出 1)稳定性压测一定时间后，日志报Java.Lang.OutOfMenoryError.PermGen Space。 2)这种原因是由于类、方法描述、字段描述、常量池、访问修饰符等一些静态变量太多，将持久代占满导致持久代溢出。 3)修改jvm配置，将XX:MaxPermSize=256参数调大。尽量减少静态变量。 三、线程死锁 容量测试压测一段时间后，LR报连接超时。 造成这种现象的原因很多，比如带宽不够，中间件线程池不够用，数据库连接池不够，连接数占满等都会造成连接不上而报超时错误。 jstack命令dump线程栈，搜索线程栈里有没有block，如果有的话就是线程死锁，找到死锁的线程，分析对应的代码。 四、数据库死锁 容量测试压测一段时间后，LR报连接超时。 造成这种现象的原因很多，比如带宽不够，中间件线程池不够用，数据库连接池不够，连接数占满等都会造成连接不上而报超时错误。 数据库日志中搜索block，能搜到block的话就是存在数据库死锁，找到日志，查看对应的sql，优化造成死锁的sql。 五、数据库连接池不释放 容量测试压测一段时间后，LR报连接超时。 造成这种现象的原因很多，比如带宽不够，中间件线程池不够用，数据库连接池不够，连接数占满等都会造成连接不上而报超时错误。 去数据库查看应用程序到数据库的连接有多少个（ show full processlist），假如应用程序里面配置的数据库连接为30，在数据库查看应用程序到数据库的连接也是30，则表示连接池占满了。将配置改成90试试，去数据库看如果连接到了90，则可以确定是数据库连接池不释放导致的。查看代码，数据库连接部分是不是有创建连接但是没有关闭连接的情况。基本就是这种情况导致的，修改代码即可。 六、TPS上不去 压力大的时候tps频繁抖动，导致总tps上不去。查看是否有fullgc（tail -f gc_mSrv1.log | grep full）。 pacing设置太小也会导致tps上不去，对抖动大的交易多增加点用户即可。 tps抖动，单压抖动大的交易，发现很平稳，这时怀疑是不是压力太大导致，所以发容量的时候把压力最大的那只交易分到其他压力机，然后发现tps不抖动了。注意：多台压力机只影响tps抖动，不会影响服务器的cpu。 看响应时间有没有超时，看用户数够不够。 七、服务器压力不均衡（相差1%-2%是正常的） 跑最优容量的时候，四台AP只有一台cpu超过60%，其他三台都在60%以下。 查看服务器是否有定时任务。 查看是否存在压力机瓶颈。 是否存在带宽瓶颈（局域网不存在此问题）。 查看部署的版本，配置是否一样。 可能别人也在用这些AP，因为同一台物理机上有很多虚拟机，因为别人先用，资源被别人先占了。 八、fullgc时间太长 跑容量和稳定性的时候，出现LR报请求超时错误，查看后台日志是fullgc了，看LR几点报的错和日志里fullgc的时间是否对应，fullgc会暂停整个应用程序，导致LR前端没响应，所以报错，这时可以减少old代内存，从而减少fullgc时间，减少fullgc时间LR就不会报错，让用户几乎感觉不到应用程序暂停。 四台AP轮流着full gc（部分server fullgc，其他server也会fullgc），这时可以制定策略让不同的server不同时fullgc，或者等夜间交易量少时写定时任务重启服务。 注意： 服务器日志为error下测试。 服务启动后几分钟内发压压力会很大，最好是服务启动两三分钟后再开始跑压力。","categories":[{"name":"性能测试","slug":"性能测试","permalink":"http://wysh.site/categories/性能测试/"}],"tags":[{"name":"性能测试","slug":"性能测试","permalink":"http://wysh.site/tags/性能测试/"}]},{"title":"如何写一个好的缺陷（Defect）报告","slug":"如何写一个好的缺陷（Defect）报告","date":"2016-05-15T08:26:34.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/05/15/如何写一个好的缺陷（Defect）报告/","link":"","permalink":"http://wysh.site/2016/05/15/如何写一个好的缺陷（Defect）报告/","excerpt":"","text":"编写缺陷报告是测试人员的日常工作，好的缺陷报告能够让开发人员更容易理解，更快速的定位问题；不好的缺陷报告可能会误导调查方向，增加沟通成本。那么一个好的缺陷报告应该包括哪些方面呢？ 请看我的mindmap： 标题 首先要做一个“标题党”（此标题党非彼标题党）。标题一定要清晰简洁易理解，不应该臃长 尽量前缀要规范，例如模板： [Product][Version][Feature][Title]，这样描述会很清晰，也方便查找 缺陷的标题一定要描述在什么情况下发生了什么问题 尽量避免使用人称（比如you, I等等） 缺陷标题的例子： DemoApp 1.0_Login_Cannot enter username by copy/paste enternal string 这个标题包含了产品名，版本号，模块，发生了什么（cannot enter username),什么情况下(copy/paste enternal string) 描述或总结描述或总结这个模块可以用来描述标题不能容纳的更详细的内容，它可以包括很多方面，比如相关、历史版本是否重现、用户操作等。目的是更清晰详细的描述缺陷。 影响这部分用以描述该缺陷对用户实际应用中的影响。 前置条件 用以描述在重现缺陷之前环境、数据或者其他的一些特殊需求。 重现步骤 从用户角度出发来描述重现步骤，步与步之间不应该有太大的业务跳跃，最好是连贯的。 例如： Repro Steps： Open DemoApp to enter Login screen Copy username from enternal file Paster username to username field of Login Screen 结果 结果可以分为“期望结果”和“实际结果”，结果可以有多个，也可以穿插在重现步骤之间（比如重现步骤中有多个缺陷的问题） 优先级凡事都有轻重缓急，缺陷也是，需要标明缺陷优先级和紧急程度，以便开发团队决定先做还是延后。 重现频率 当然，大部分的缺陷是可以100%重现的，对于少数缺陷可能很难重现，或者不太容易重现，这就要标明重现的几率，比如50%。往往这种缺陷需要提供详细的日志文件，以便从日志角度获取重现或者解决突破口。 附件 附件非常重要！附件的格式可以多种多样，图片，日志文件，视频等。除了可以提供直观的认识（图片，视频），还可以有更多的信息（缺陷讨论邮件，日志等）。 变通方案（Workaround） 变通方案是提供一种绕过当前问题而使用其它的产品功能的一种方式。这样客户就可以在缺陷未解决的情况下继续使用产品。 发生原因分析（Root Cause Analysis） 描述从代码角度，该缺陷是如何发生的。能做到这一步的测试人员需要有较高的读写代码的能力。 环境配置用以描述测试环境的配置，比如OS，相应产品版本等。 那么，问题来了！缺陷包括这么多方面，如果每个缺陷都这么写，要耗费多少effort啊！！！（毕竟测试时最忙的！） 个人认为没有必要每个都这么写，毕竟写缺陷报告对客户来说没有value。缺陷报告是缺陷的信息载体，它存在的意义是用于更好、更清楚的进行开发团队之间的沟通和以后的回顾，写到什么程度还是需要根据实际情况有所取舍。（比如Root cause analysis在时间不富裕的情况下可以忽略等） 综合以上的方面，下边是一个模板，希望对大家有所帮助。 123456789101112131415161718192021222324252627282930313233343536373839Title: [Product][Version]_[Feature]_TitleDescription/Summary:Impact:Priority/Severity: CriticalFrequency: 100%Precondition:Repro Steps: step 1: step 2: Expected Result: Actual Result: step 3:Expected Result:Actual Result:Root Cause Analysis:Workaround:Environment:Attchment:","categories":[{"name":"测试文档","slug":"测试文档","permalink":"http://wysh.site/categories/测试文档/"}],"tags":[{"name":"测试文档","slug":"测试文档","permalink":"http://wysh.site/tags/测试文档/"}]},{"title":"如何制定测试策略","slug":"如何制定测试策略","date":"2016-05-13T23:47:26.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/05/14/如何制定测试策略/","link":"","permalink":"http://wysh.site/2016/05/14/如何制定测试策略/","excerpt":"","text":"测试策略是描述测试项目和测试任务之间的关系。它用来说明要测什么，如何测，如何协调测试资源和测试时间等。测试策略制定的是否合理高效会对测试项目的进度产生很大的影响。那么，如何制定一个好的测试策略并且能防止遗漏呢？一个好的测试策略又包含哪些方面呢？下面我给出一个平时经常使用的一个模板供大家参考 大将测试策略分为了一下几个模块： 1. 测试安排、发布计划这个模块用来罗列测试项目本身重要的里程碑，每个里程碑都需要有明确的结束时间，这个时间可以指导我们后续的测试。如果测试时间安排不足，我们就可以在后续的测试范围中挑选优先级比较高的特性来执行测试，这样可以最大限度的保证产品的质量。 2. 测试范围（按优先级排列）这一部分分为InScope和Out Of Scope.这一部分需要说明哪些产品模块是在测试范围中的，哪些是本阶段测试不考虑的。对于在测试范围中的模块，需要给出优先级以便相应测试时间不足的情况；对于不在测试范围中的模块，需要给出原因（为什么在本测试阶段不考虑测）。 3. 测试资源测试资源在测试策略中也是很重要的一环，它分为人力和工具两部分。人力资源主要说明参与测试的人员，当然可以包括很多的角色，如何专业测试人员，客户，产品经理等。工具主要是指可能用到其他软件（可能需要license）。 4. 测试环境测试环境主要包括推荐环境解决方案，操作系统要求，软硬件要求。 对于推荐解决方案，需要陈述的是对测试项目对其他软件的依赖，比如测试项目对.Net有依赖，这时我们可能给出的推荐版本可能就是4.5.2，在之后的测试中主要是针对4.5.2进行验证，而对其他版本进行简单验证，这样在产品文档中给出4.5.2的推荐方案，主要是为了说明4.5.2是没问题的，其他版本不保证。 操作系统主要是说明对windows或者其他操作系统的版本的支持情况。 5. 测试方法测试方法的罗列主要是为了说明针对测试项目我们要开展哪些类型的测试，功能测试是必须的，非功能测试是可选的。（相信各位童鞋对测试方法都已经倒背如流了，就不一一介绍了） 6. 用例设计方法用例设计大家也很清楚了，不再介绍了。 7. 文档管理对于一个完整的产品来说，文档是很重要的一环。它一般包括安装、升级文档，用户指南等。文档不单单是一个文件，它需要经过完整的测试才能发布给客户。差的文档很可能会误导用户，从而使他们对测试项目失去信心（虽然客户很少看文档……：）） 8. 风险管理风险管理模块需要罗列出来现在已知的可能会出现不确定性的因素，这些因素可能来自技术，资源或者其他方面的。 9. 发布包验证这部分有一定的特殊性，并不适用于所有的产品。这部分主要是对测试项目安装包进行验证，防止在制作ISO文件的过程中产生变动。 就写这些吧，希望大家在看了这9个模块后能找到文章开头两个问题的答案。也非常欢迎大家提出改进意见。","categories":[{"name":"用例设计","slug":"用例设计","permalink":"http://wysh.site/categories/用例设计/"}],"tags":[{"name":"用例设计","slug":"用例设计","permalink":"http://wysh.site/tags/用例设计/"}]},{"title":"测试设计中需要考虑的22种测试类型","slug":"测试设计中需要考虑的22种测试类型","date":"2016-05-09T13:04:25.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/05/09/测试设计中需要考虑的22种测试类型/","link":"","permalink":"http://wysh.site/2016/05/09/测试设计中需要考虑的22种测试类型/","excerpt":"","text":"黑盒测试：不基于内部设计和代码的任何知识，而是基于需求和功能性。 白盒测试：基于一个应用代码的内部逻辑知识，测试是基于覆盖全部代码、分支、路径、条件。 单元测试：最微小规模的测试；以测试某个功能或代码块。典型地由程序员而非测试员来做，因为它需要知道内部程序设计和编码的细节知识。这个工作不容易作好，除非应用系统有一个设计很好的体系结构;还可能需要开发测试驱动器模块或测试套具。 累积综合测试：当一个新功能增加后，对应用系统所做的连续测试。它要求应用系统的不同形态的功能能够足够独立以可以在全部系统完成前能分别工作，或当需要时那些测试驱动器已被开发出来;这种测试可由程序员或测试员来做。 集成测试：一个应用系统的各个部件的联合测试，以决定他们能否在一起共同工作。部件可以是代码块、独立的应用、网络上的客户端或服务器端程序。这种类型的测试尤其与客户服务器和分布式系统有关。 功能测试：用于测试应用系统的功能需求的黑盒测试方法。这类测试应由测试员做，这并不意味着程序员在发布前不必检查他们的代码能否工作(自然他能用于测试的各个阶段)。 系统测试：基于系统整体需求说明书的黑盒类测试；应覆盖系统所有联合的部件。 端到端测试：类似于系统测试；测试级的“宏大”的端点；涉及整个应用系统环境在一个现实世界使用时的模拟情形的所有测试。例如与数据库对话，用网络通讯，或与外部硬件、应用系统或适当的系统对话。 健全测试：典型地是指一个初始化的测试工作，以决定一个新的软件版本测试是否足以执行下一步大的测试努力。例如，如果一个新版软件每5分钟与系统冲突，使系统陷于泥潭，说明该软件不够“健全”，目前不具备进一步测试的条件。 衰竭测试：软件或环境的修复或更正后的“再测试”。可能很难确定需要多少遍再次测试。尤其在接近开发周期结束时。自动测试工具对这类测试尤其有用。 接受测试：基于客户或最终用户的规格书的最终测试，或基于用户一段时间的使用后，看软件是否满足客户要求。 负载测试：测试一个应用在重负荷下的表现，例如测试一个Web站点在大量的负荷下，何时系统的响应会退化或失败。 强迫测试：在交替进行负荷和性能测试时常用的术语。也用于描述象在异乎寻常的重载下的系统功能测试之类的测试，如某个动作或输入大量的重复，大量数据的输入，对一个数据库系统大量的复杂查询等。 性能测试：在交替进行负荷和强迫测试时常用的术语。理想的“性能测试”(和其他类型的测试)应在需求文档或质量保证、测试计划中定义。 可用性测试：对“用户友好性”的测试。显然这是主观的，且将取决于目标最终用户或客户。用户面谈、调查、用户对话的录象和其他一些技术都可使用。程序员和测试员通常都不宜作可用性测试员。 安装/卸载测试：对软件的全部、部分或升级安装/卸载处理过程的测试。 恢复测试：测试一个系统从如下灾难中能否很好地恢复，如遇到系统崩溃、硬件损坏或其他灾难性问题。 安全测试：测试系统在防止非授权的内部或外部用户的访问或故意破坏等情况时怎么样。这可能需要复杂的测试技术。 兼容测试：测试软件在一个特定的硬件/软件/操作系统/网络等环境下的性能如何。 比较测试：与竞争伙伴的产品的比较测试，如软件的弱点、优点或实力。 Alpha测试：在系统开发接近完成时对应用系统的测试；测试后，仍然会有少量的设计变更。这种测试一般由最终用户或其他人员员完成，不能由程序员或测试员完成。 Beta测试：当开发和测试根本完成时所做的测试，而最终的错误和问题需要在最终发行前找到。这种测试一般由最终用户或其他人员员完成，不能由程序员或测试员完成。","categories":[{"name":"用例设计","slug":"用例设计","permalink":"http://wysh.site/categories/用例设计/"}],"tags":[{"name":"用例设计","slug":"用例设计","permalink":"http://wysh.site/tags/用例设计/"}]},{"title":"Web端测试和移动端测试的区别","slug":"Web端测试和移动端测试的区别","date":"2016-05-09T04:58:05.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/05/09/Web端测试和移动端测试的区别/","link":"","permalink":"http://wysh.site/2016/05/09/Web端测试和移动端测试的区别/","excerpt":"","text":"之前参加的项目有涉及Web端测试和移动端测试，简单的记录下他们之间的区别： 1、记录bug在Web端可以通过系统自带的截图和QQ截图等方式来截取bug的图片，对于错误的地方可以用工具自带的标识来重点标记。 对于移动端设备可以用手机自带的截图工具来截图然后传到电脑上，个人一般习惯安装微信的windows版本，通过文件传输助手发送到PC端。还有一种比较便捷的方式，将手机用数据线连接到电脑，本地配置android的运行环境，下载asm.jar,在cmd运行java -jar asm.jar，即可实时同步手机端画面，对有bug的页面直接使用PC端的截图工具进行截图（该工具在另一篇文章中会具体介绍）。IOS可以在PC安装itools，要额可以同步画面。 对于记录bug建议： bug主题尽量的言简意骇，在bug描述中可以详细描述，对于操作步骤比较复杂的bug要详细的写上操作步骤。必要时附带上相关的log，记录上测试的环境，手机版本等等。对于必现喝非必现的bug也要详细说明，减少不必要的沟通成本。 2、测试环境Web端的测试环境很多时候是通过hosts进行切换，switchhosts工具可以方便的切换需要的host，但是移动端设置起来比较复杂，比较简单的方式是电脑端设置代理，手机端直接连接代理。注意，手机和电脑必须连接同一个网络。 设置代理推荐使用Fiddle，可以抓到手机端的数据包。 3、兼容性web端的测试一般都是主要使用一种浏览器，待系统基本稳定的时候，再去专门测试浏览器的兼容性。 但是，对移动端来说，这样的方式是行不通的，因为移动端主要分为安卓和IOS，而这两端出现的问题一般是不一致的，一致的问题主要是数据问题，这时候是需要后台处理的，所以我们测试的时候需要两端都重点测试，而不会出现先着重测试某一端的问题。 注：一般方式是在测试一端时，出现问题则立马查看另一端是否也有这个问题。 4、移动端的特性移动端与web端相比较来说，移动端有很多自己的特性： 网络种类多 移动端有多种网络：无线网络、2G、3G、4G等，断网、网速较差及网络之间的切换时页面的显示等，这些对于移动端来说很重要。此外，在非wifi下，还需要注意网络使用量问题。 间断问题 移动端有一个很重要的问题，一般情况下在使用软件的过程并不是长久的，这中间可能发生很多中断，如电话、短信、通知、断电等等，软件需要特殊处理这些特殊情况。 打开一个页面，或在操作的过程中（点击一个按钮后），将手机屏幕锁住，再打开时，应用能否正常处理。 屏幕的限制 图片及文字的显示；上传不同的图片尺寸显示是否正常；图片和文字一起显示时，效果如何。 操作区域；web端的应用，一般不会受到屏幕的限制，而且通过鼠标操作更加准确。但是移动端由于屏幕较小，页面及按钮会受到屏幕大小的限制，再加上用户都是通过手指进行操作，一些按钮、选择框等是否容易点击，多个可点区域位置较近时，点击部位稍微偏移，也许就会造成不同的结果，这种情况下是否可以达到预先的效果。 软件启动运行 移动端启动、卸载、升级几个特性，这是比较常见、也很重要的，比如升级时用户的数据怎么办，卸载后用户的数据怎么处理，卸载再安装用户登录数据的显示等。 手势 移动端还有一大特性，就是移动端有自己比较简单的手势，用户可以通过手势进行一个操作，比如左滑删除、右滑返回上一个页面、左右滑动图片等，软件需要对这个手势进行适配。 分享 移动端一般会装有很多软件，用户下单或者产品有活动时，用户都会进行分享，但是分享时的权限、软件是否存在等问题，需要特殊处理测试。一般的软件或应用，都会开放一部分页面，允许用户不登录时即可访问，而有些页面是必须要求用户登录的，主要针对这两种权限不同的页面做分享，然后通过分享进入本页面，查看权限的控制是否正常。 web和移动端的同步 用户在web端的操作，在移动端是否可以正常的进行同步、显示；在移动端的操作，用户登录web账号，信息是否同步等。","categories":[{"name":"测试理论","slug":"测试理论","permalink":"http://wysh.site/categories/测试理论/"}],"tags":[{"name":"测试理论","slug":"测试理论","permalink":"http://wysh.site/tags/测试理论/"}]},{"title":"软件测试缺陷度量分析","slug":"软件测试缺陷度量分析","date":"2016-05-08T13:25:46.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/05/08/软件测试缺陷度量分析/","link":"","permalink":"http://wysh.site/2016/05/08/软件测试缺陷度量分析/","excerpt":"","text":"对缺陷的度量有助于测试过程监控，例如：缺陷密度分析，发现和修复的缺陷数目等。另外，缺陷度量应包括追踪过程控制信息的过程改进活动所需的缺陷信息，并引入缺陷来源分析、缺陷趋势分析等作为风险减轻策略的输入。本文介绍了几种常见的缺陷度量指标，在实际项目中，缺陷度量指标通常要和其他指标共同使用以达到测度的目的。 1、缺陷发现进度1）度量目标缺陷发现进度度量（累计缺陷）可以显示每个星期累计发现缺陷的数量，帮助评估测试的状态、测试进度和软件系统的质量。 2）度量定义缺陷发现进度度量（累计缺陷）的X轴为星期，以yww形式表示，其中y表示年份的后两位，ww表示星期，例如：815指的是2008年的第15周。Y轴表示在测试阶段发现的缺陷数目，如图1所示。 3）度量分析对缺陷修复进度进行度量分析的时候，可以从以下几个方面着手，分析和评估开发人员修复缺陷的进度、评估后续的测试资源分配和软件系统的质量： 结合缺陷发现进度度量数据，分析发现缺陷和修复缺陷数目之间的差异，从整个项目的层面帮助项目团队进行合理的资源分配。 根据测试缺陷修复进度度量数据，分析开发人员修复缺陷的速率是否在正常范围之内，并分析产生较大差异的原因。 根据开发人员修复缺陷的情况，可以有针对性地更新测试计划和测试资源的分配。 3、缺陷优先级1）度量目标缺陷优先级度量（累计缺陷）有助于识别不同优先级的缺陷在所有缺陷中的比重，从整体上把握不同优先级缺陷的分布，有助于开发和测试资源的计划和分配。 2）度量定义缺陷优先级度量（累计缺陷）的X轴为时间，以yww形式表示，其中y表示年份的后两位，ww表示星期，例如：815指的是2008年的第15周。Y轴表示测试数目，如图3所示。需要收集的测试数据包括： 缺陷优先级为1的数目（累计缺陷）。 缺陷优先级为2的数目（累计缺陷）。 缺陷优先级为3的数目（累计缺陷）。 缺陷优先级为4的数目（累计缺陷）。 图3 缺陷优先级（累计缺陷） 3）度量分析在进行缺陷优先级度量分析的时候，可以从以下几个方面分析测试进展和测试质量： 发现的各个优先级的缺陷数目。 优先级1和优先级2的缺陷有没有突然发生大的变化，高优先级的缺陷大量增加通常意味着产品质量出现了较大的问题。 和缺陷发现进度度量（累计缺陷）数据相结合，分析各个优先级缺陷的发现趋势，判断产品质量是否趋于稳定。 $$ 4、缺陷严重程度 1）度量目标缺陷严重程度度量（累计缺陷）有助于识别不同严重程度的缺陷在所有缺陷中的比重，从整体上把握不同严重程度缺陷的分布，有助于开发人员资源的计划和分配，以及测试人员资源的计划和分配。 2）度量定义缺陷严重程度度量（累计缺陷）的X轴为星期，以yww形式表示，其中y表示年份的后两位，ww表示星期，例如：815指的是2008年的第15周。Y轴表示测试数目，如图4所示。需要收集的测试数据包括： 缺陷严重程度为1的数目（累计缺陷）。 缺陷严重程度为2的数目（累计缺陷）。 缺陷严重程度为3的数目（累计缺陷）。 缺陷严重程度为4的数目（累计缺陷）。 3）度量分析在进行缺陷严重程度度量分析的时候，可以从以下几个方面分析测试进展和测试质量： 各个缺陷严重程度发现的缺陷数目。 严重程度1和严重程度2的缺陷有没有突然发生大的变化，高严重程度的缺陷大量增加通常意味着产品质量出现问题。 和缺陷发现进度度量（累计缺陷）相结合，分析各个严重程度缺陷的发现趋势，判断产品质量是否趋于稳定。","categories":[{"name":"测试理论","slug":"测试理论","permalink":"http://wysh.site/categories/测试理论/"}],"tags":[{"name":"测试理论","slug":"测试理论","permalink":"http://wysh.site/tags/测试理论/"}]},{"title":"作为测试人员一般经历的几个阶段","slug":"作为测试人员一般经历的几个阶段","date":"2016-05-07T14:43:31.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/05/07/作为测试人员一般经历的几个阶段/","link":"","permalink":"http://wysh.site/2016/05/07/作为测试人员一般经历的几个阶段/","excerpt":"","text":"一、执行测试用例作为一个测试新手来说，最主要的工作应该就是执行测试用例，最基本的要求当然就是不能够出现执行漏测了。是的，达到这个要求毕竟简单，只要严格按照用例来执行就可以了，这里主要考验的就是一个测试人员的执行力和细心的能力。另外，这个阶段测试人员能够学习到自己测试模块的一些基本业务知识，以及如何去执行用例，提交和跟踪bug等等，这个阶段也很容易达到，甚至可能会跟第2个阶段一起进行，但是该阶段虽然简单却很重要 二、发现bug经历第一个阶段后，这个时候测试人员可以开始在执行用例的基础上开始一些自己的发散测试（更好的叫法是探索性测试），来更好的发现一些通过执行用例无法测试到的bug。这个阶段就比较考察一个测试人员的发散思维了（个人觉得测试人员的发散思维恩能力是测试人员非常重要的一个能力，这也是软件测试的魅力所在），有的测试人员就是能够通过自己的一些想法来发现一些bug（甚至是隐藏很深的bug），并且很享受在其中。个人觉得这些能力（也可以叫对bug的敏感程度）是天生的，当然，并不是说这块能力不强的人在测试里面发展不好，因为测试也有技术（确定的因素）的成分，比如：自动化等。但是，这样的测试人员不太容易享受到发现bug给自己带来的成就感（即软件测试的艺术性）。当然，要达到这样的程度对于模块本身的业务逻辑也需要非常熟悉 三、保证质量质量是一个测试人员的生命，当我们将一个功能模块交给一个测试人员负责的时候，我们肯定是希望对方能够给保证质量的。但是实际上要达到这个要求是很难的。首先，我们对于保证质量是如何定义的，是保证该模块到客户那里不会影响客户的业务还是在客户那里不出现问题？实际上2者是很难区分的，目前我们对于测试人员的要求大概都是能够发现所有的bug吧，虽然实际上不可能。其次，作为一个测试人员，我们自己对于该模块的测试策略该如何把握呢（因为测试时间是一定的）？根据个人经验，要达到这个目标，至少需要做到以下几点吧！ 需求覆盖：对于的整个需求的理解程度：对于客户来说，为什么需要这个功能？主要是用使用习惯是怎样的？客户哪里可能会出现的一些异常情况等等 业务逻辑覆盖：对于整个业务逻辑的理解程度：通过看研发的设计文档和具体的实现逻辑图，来分析如何覆盖到所有的逻辑以及异常逻辑（这个时候还需要提前发现一些研发没有考虑到的地方），并且设计对应的逻辑测试用例，保证我们的测试业务逻辑的覆盖率（代码覆盖率非逻辑覆盖率）。当然，这个阶段是能够通过一些技术手段做的更好的，比如：接口测试、单元测试、代码的静态走读等等，后面再讨论… 性能压力覆盖（为什么就不说了）：主要是在前面对业务逻辑非常熟悉的基础上，对于每个业务逻辑的性能测试点进行分析，看下这些逻辑是否可能有压力点，比如：当资源不足的时候是否有影响？当并发数或者流量很大的情况下是否会有影响？是否会涉及到多线程通信或者进程之间抢占资源等等，分析完成后还需要考虑如何去覆盖到这些地方（包括压力是否足够等等） 关联覆盖：大部分情况下一个模块总是会和整个系统的其他模块存在关联的地方，那么我们除了要分析出和哪些模块有关联，还需要分析具体的关联点是什么？这其实就要求我们对于与之关联的模块也足够的熟悉，这样才能够更好的分析到对应点上 当然，还会需要涉及到其他的，比如：模块的可靠性，安全性等等 发散测试：一般情况下，前面的几点很难分析到非常的全面和充分，这个时候就需要依赖自己的分析能力和发散思维能力了，如果这方面比较好的话应该是有意外惊喜的，而且前面的一些测试也需要依赖自己的发散思维能力如果能够达到这个阶段，相信你已经是一个比较让人放心的测试人员了，这些阶段一个非常重要的就是对被测模块的业务熟悉程度。 四、提高测试效率测试是有成本的，而且测试的周期越长，成本越大，甚至可能影响整个产品在市场的占有情况或客户的满意程度。所以，对于测试人员一个很重要的要求当然是如何在更短的时间内保证质量。要做到这个程度，主要依靠两个手段吧！ 一、前期缺陷预防：测试人员通过前期和开发人员配合，共同的将很多bug直接扼杀在摇篮，避免bug在后面被发现。下面可以从每个阶段来分析测试测试人员需要做好哪些事情。 需求阶段：测试和研发一起将该功能的所有需求点全部列出来，并且分析所有的需求点是否明确和合理。另外，是否还有没有考虑到的客户的隐藏需求等等，通过不断的检视来完善。需要的能力：测试经验、对于需求的理解能力和思考问题的全面性 设计阶段：加深对于设计的理解，多跟开发进行交流，能够根据自己的测试经验以及对于该模块的理解程度对研发的设计进行评审，并能够发现设计存在的一些问题（比如：一些场景可能没有考虑到，一些异常情况可能没有考虑到等等）。并且将自己后面可能会怎么测试提前告诉开发（这个时候心里应该大概知道该如何去测试该模块，可能的风险是什么灯）。需要的能力：对于模块的理解程度，对于用户场景的理解程度，对于整个业务的理解程度（参考测试人员的第3个阶段）。 编码阶段：这个时候可以通过一些改进，比如：对研发的代码进行静态走读，通过工具覆盖，思考单元测试或者借口测试，对用例实现自动化等，目的就是在黑盒测试前就能够提前发现该模块存在的代码逻辑问题，减少后面的手工测试时间。需要的能力：代码理解能力、代码测试工具的使用能力，一定的编码能力，自动化开发能力等 二、测试分析能力进入测试后，需要对我们前期的缺陷预防进行分析和总结，并且分析下该模块的质量：看下哪些地方是存在风险的，哪些地方是前面做的比较充分的，从而来制定我们的测试策略，减少一些没有必要的测试点或增加一些有效的测试点，让我们的整个测试更加的有效，并且通过不断的测试和分析，来及时调整测试策略，来减少我们的测试时间（比如：以前需要测试500个测试用例，后面通过分析后能够减少到300个，并且证明测试结果是一样的），当然我们需要对我们的测试结果负责，要达到这个程度应该是比较难的！ 以上纯属个人观点，欢迎大家讨论","categories":[{"name":"测试理论","slug":"测试理论","permalink":"http://wysh.site/categories/测试理论/"}],"tags":[{"name":"测试理论","slug":"测试理论","permalink":"http://wysh.site/tags/测试理论/"}]},{"title":"什么是性能瓶颈？","slug":"什么是性能瓶颈","date":"2016-05-03T12:12:37.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/05/03/什么是性能瓶颈/","link":"","permalink":"http://wysh.site/2016/05/03/什么是性能瓶颈/","excerpt":"","text":"在性能测试中，总会用到“性能瓶颈”这个词，也就把它当成基本的一个词汇了，从没想过它到底是个什么东西。今天忽然有人问道什么是“性能瓶颈”，虽然勉强能列举一些例子来说明它，但总不是太令人满意，所以就总结一下。 首先得说明“瓶颈”的意思。瓶颈，通俗地说，就是一个瓶子的脖子，就是整个系统最薄弱的环节。好比一个桶，能装多少水取决于最短的那块木板，其它的木板再长也没用，那么我们就可以认为这块短的木板就是这个桶的瓶颈了。 所以性能瓶颈，就是说指限制系统性能的关键因素。 一般包括： （1）硬件上的性能瓶颈 主要指的是CPU、RAM方面的问题。例如，在进行软件需求分析、概要设计时，确定了在数据库服务器上需要6个CPU、12G内存但是在测试时，发现CPU的持续利用率超过95%，这时可以认为在硬件上出现了性能瓶颈。 （2）应用软件上的性能瓶颈 一般指的是应用服务器、WEB服务器等应用软件，还包括数据库系统。例如，在WEBLogic平台上配置了JDBC连接池的参数，最大连接数为50，最小连接数为5，增加量为10。在测试时发现，当负载增加时，现有的连接数不足，系统会动态生成10个新的连接数，这样导致了交易处理的响应时间大大的增加。 这时可以认为在应用软件上出现了性能瓶颈。 （3）应用程序上的性能瓶颈 一般指的是开发人员新开发出来的应用程序。例如，用Java或者C开发出来的部署在应用服务器上用于用户交易请求处理的应用程序。例如，某个开发员开发了一个缴费处理程序，在测试时发现，这个缴费处理程序在处理用户发过来的并发缴费请求时，只能串行处理，无法并行处理，导致缴费交易的处理响应时间非常长，这时可以认为在应用程序上出现了性能瓶颈。 （4）操作系统上的性能瓶颈 一般指的是Windows、Unix、Linux这些操作系统。例如，在windows系统中，虚拟内存设置的不合理，都指定为C驱提供虚拟内存，在测试时发现当出现物理内存不足时，虚拟内存的交换效果非常不理想，导致交易的响应时间大大增加。这时可以认为在操作系统上出现了性能瓶颈。 （5）网络设备上的性能瓶颈 一般指的是防火墙、动态负载均衡器、交换机等设备。例如，在动态负载均衡器上设置了动态分发负载的机制，当发现某个应用服务器上的硬件资源已经到达极限时，动态负载均衡器将后续的交易请求发送到其它负载较轻的应用服务器上。在测试时发现，动态负载均衡机制没有起到相应的作用，这时可以认为在网络设备上出现了性能瓶颈。","categories":[{"name":"测试理论","slug":"测试理论","permalink":"http://wysh.site/categories/测试理论/"},{"name":"性能测试","slug":"测试理论/性能测试","permalink":"http://wysh.site/categories/测试理论/性能测试/"}],"tags":[{"name":"性能测试","slug":"性能测试","permalink":"http://wysh.site/tags/性能测试/"}]},{"title":"性能测试指标+案例","slug":"性能测试指标+案例","date":"2016-05-01T22:57:40.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/05/02/性能测试指标+案例/","link":"","permalink":"http://wysh.site/2016/05/02/性能测试指标+案例/","excerpt":"","text":"通用指标（指Web应用服务器、数据库服务器必需测试项) Web服务器指标 数据库服务器性能指标 系统的瓶颈定义 稳定系统的资源状态 通俗理解： 日访问量 常用页面最大并发数 同时在线人数 访问相应时间 案例： 最近公司一个项目，是个门户网站，需要做性能测试，根据项目特点定出了主要测试项和测试方案： 一种是测试几个常用页面能接受的最大并发数(用户名参数化，设置集合点策略) 一种是测试服务器长时间压力下，用户能否正常操作(用户名参数化，迭代运行脚本) 一种则需要测试服务器能否接受10万用户同时在线操作，如果是用IIS做应用服务器的话，单台可承受的最大并发数不可能达到10万级，那就必须要使用集群，通过多台机器做负载均衡来实现；如果是用websphere之类的应用服务器的话，单台可承受的最大并发数可以达到10万级，但为性能考虑还是必须要使用集群，通过多台机器做负载均衡来实现；通常有1个简单的计算方式，1个连接产生1个session，每个session在服务器上有个内存空间大小的设置，在NT上是3M，那么10万并发就需要300G内存，当然实际使用中考虑其他程序也占用内存，所以准备的内存数量要求比这个还要多一些。还有10万个用户同时在线，跟10万个并发数是完全不同的2个概念。这个楼上已经说了。但如何做这个转换将10万个同时在线用户转换成多少个并发数呢？这就必须要有大量的历史日志信息来支撑了。系统日志需要有同时在线用户数量的日志信息，还需要有用户操作次数的日志信息，这2个数据的比例就是你同时在线用户转换到并发数的比例。另外根据经验统计，对于1个JAVA开发的WEB系统（别的我没统计过，给不出数据），一般1台双CPU、2G内存的服务器上可支持的最大并发数不超过500个（这个状态下大部分操作都是超时报错而且服务器很容易宕机，其实没什么实际意义），可正常使用（单步非大数据量操作等待时间不超过20秒）的最大并发数不超过300个。假设你的10万同时在线用户转换的并发数是9000个，那么你最少需要这样的机器18台，建议不少于30台。当然，你要是买个大型服务器，里面装有200个CPU、256G的内存，千兆光纤带宽，就算是10万个并发用户，那速度，也绝对是嗖嗖的。 另外暴寒1下，光设置全部进入运行状态就需要接近6个小时。具体的可以拿1个系统来压一下看看，可能会出现以下情况： 服务器宕机； 客户端宕机； 从某个时间开始服务器拒绝请求，客户端上显示的全是错误； 勉强测试完成，但网络堵塞或测试结果显示时间非常长。假设客户端和服务器之间百兆带宽，百兆/10000=10K，那每个用户只能得到10K，这个速度接近1个64K的MODEM上网的速度；另外以上分析全都没考虑系统的后台，比如数据库、中间件等。 1、服务器方面：上面说的那样的PC SERVER需要50台； 2、网络方面：按每个用户50K，那至少5根百兆带宽独享，估计仅仅网络延迟就大概是秒一级的； 3、如果有数据库，至少是ORACLE，最好是SYSBASE，SQL SERVER是肯定顶不住的。数据库服务器至少需要10台4CPU、16G内存的机器； 4、如果有CORBA，那至少再准备10台4CPU、16G内存的机器；再加上负载均衡、防火墙、路由器和各种软件等，总之没个1000万的资金投入，肯定搞不定。 这样的门户系统，由于有用户权限，所以并不象jackie所说大多是静态页面。但只要是多服务器的集群，那么我们就可以通过1台机器的测试结果来计算多台机器集群后的负载能力的，最多额外考虑一下负载均衡和路由上的压力，比如带宽、速度、延迟等。但如果都是在1台机器上变化，那我们只能做一些指标上的计算，可以从这些指标上简单判断一下是否不可行，比如10万并发用户却只有1根百兆带宽，那我们可以计算出每个用户只有1K带宽，这显然是不可行的。但实际的结果还是需要测试了才知道，毕竟系统压力和用户数量不是线性变化的。 这一类系统的普遍的成熟的使用，以及很多软件在方案设计后就能够大致估算出系统的性能特点，都导致了系统在软件性能方面调优的比例并不大（当然不完全排除后期针对某些代码和配置进行优化后性能的进一步提高），更多的都是从硬件方面来考虑，比如增加内存、硬盘做RAID、增加带宽、甚至增加机器等。 网络技术中的10M带宽指的是以位计算，就是10M bit/秒，而下载时的速度看到的是以字节（Byte）计算的，所以10M带宽换算成字节理论上最快下载速度为：1.25MByte/秒!","categories":[{"name":"测试理论","slug":"测试理论","permalink":"http://wysh.site/categories/测试理论/"},{"name":"性能测试","slug":"测试理论/性能测试","permalink":"http://wysh.site/categories/测试理论/性能测试/"}],"tags":[{"name":"性能测试","slug":"性能测试","permalink":"http://wysh.site/tags/性能测试/"}]},{"title":"性能测试如何定位瓶颈","slug":"性能测试如何定位瓶颈","date":"2016-04-26T23:44:34.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/04/27/性能测试如何定位瓶颈/","link":"","permalink":"http://wysh.site/2016/04/27/性能测试如何定位瓶颈/","excerpt":"","text":"性能测试这种测试方式在发生过程中，其中一个过渡性的工作，就是对执行过程中的问题，进行定位，对功能的定位，对负载的定位，最重要的，当然就是问题中说的“瓶颈”，接触性能测试不深，更非专家，自己的理解，瓶颈产生在以下几方面： 网络瓶颈，如带宽，流量等形成的网络环境 应用服务瓶颈，如中间件的基本配置，CACHE等 系统瓶颈，这个比较常用：应用服务器，数据库服务器以及客户机的CPU，内存，硬盘等配置 数据库瓶颈，以ORACLE为例，SYS中默认的一些参数设置 应用程序本身瓶颈，这个是测试过程中最需要去关注的，需要测试人员和开发人员配合执行，然后定位 逐步细化分析，先可以监控一些常见衡量CPU，内存，磁盘的性能指标，进行综合分析，然后根据所测系统具体情况，进行初步问题定位，然后确定更详细的监控指标来分析。 怀疑内存不足时： 方法1： 【监控指标】：Memory Available MBytes ，Memory的Pages/sec， page read/sec， Page Faults/sec 【参考值】： 如果 Page Reads/Sec 比率持续保持为 5，表示可能内存不足。 Page/sec 推荐00-20（如果服务器没有足够的内存处理其工作负荷，此数值将一直很高。如果大于80，表示有问题）。 方法2： 根据Physical Disk 值分析性能瓶颈 【监控指标】：Memory Available MBytes ，Pages read/sec，%Disk Time 和 Avg.Disk Queue Length 【参考值】：%Disk Time建议阈值90% 当内存不足时，有点进程会转移到硬盘上去运行，造成性能急剧下降，而且一个缺少内存的系统常常表现出很高的CPU利用率，因为它需要不断的扫描内存，将内存中的页面移到硬盘上。 怀疑内存泄漏时 【监控指标】：Memory Available MBytes ，Process\\Private Bytes和Process\\Working Set，PhysicalDisk/%Disk Time 【说明】： Windows资源监控中，如果Process\\Private Bytes计数器和Process\\Working Set计数器的值在长时间内持续升高，同时Memory\\Availablebytes计数器的值持续降低，则很可能存在内存泄漏。内存泄漏应该通过一个长时间的，用来研究分析当所有内存都耗尽时，应用程序反应情况的测试来检验。 CPU分析 【监控指标】： System %Processor Time CPU，Processor %Processor Time CPU Processor%user time 和Processor%Privileged Time system\\Processor Queue Length Context Switches/sec 和%Privileged Time 【参考值】： System\\%Total processor time不持续超过90%，如果服务器专用于SQL Server，可接受的最大上限是80-85% ，合理使用的范围在60%至70%。Processor %Processor Time小于75% system\\Processor Queue Length值，小于CPU数量的总数+1 CPU瓶颈问题 System\\%Total processor time如果该值持续超过90%，且伴随处理器阻塞，则说明整个系统面临着处理器方面的瓶颈. 注：在某些多CPU系统中，该数据虽然本身并不大，但CPU之间的负载状况极不均衡，此时也应该视作系统产生了处理器方面的瓶颈. 排除内存因素，如果Processor %ProcessorTime计数器的值比较大，而同时网卡和硬盘的值比较低，那么可以确定CPU 瓶颈。（内存不足时，有点进程会转移到硬盘上去运行，造成性能急剧下降，而且一个缺少内存的系统常常表现出很高的CPU利用率，因为它需要不断的扫描内存，将内存中的页面移到硬盘上。） 造成高CPU使用率的原因： 频繁执行程序，复杂运算操作，消耗CPU严重 数据库查询语句复杂，大量的 where 子句，order by， group by 排序等，CPU容易出现瓶颈 内存不足，IO磁盘问题使得CPU的开销增加 磁盘I/O分析 【监控指标】： PhysicalDisk/%Disk time，PhysicalDisk/%Idle Time，Physical Disk\\ Avg.Disk Queue Length， Disk sec/Transfer 【参考值】： %Disk Time建议阈值90% Windows资源监控中，如果% Disk Time和Avg.Disk Queue Length的值很高，而Page Reads/sec页面读取操作速率很低，则可能存在磁盘瓶径。 Processor%Privileged Time该参数值一直很高，且如果在 Physical Disk 计数器中，只有%Disktime 比较大，其他值都比较适中，硬盘可能会是瓶颈。若几个值都比较大，那么硬盘不是瓶颈。若数值持续超过80%，则可能是内存泄露。如果Physical Disk 计数器的值很高时该计数器的值（Processor%Privileged Time）也一直很高，则考虑使用速度更快或效率更高的磁盘子系统。 Disk sec/Transfer 一般来说，该数值小于15ms为最好，介于15-30ms之间为良好，30-60ms之间为可以接受，超过60ms则需要考虑更换硬盘或是硬盘的RAID方式了. Average Transaciton Response Time（事务平均响应时间）随着测试时间的变化，系统处理事务的速度开始逐渐变慢，这说明应用系统随着投产时间的变化，整体性能将会有下降的趋势 Transactions per Second（每秒通过事务数/TPS）当压力加大时，点击率/TPS曲线如果变化缓慢或者有平坦的趋势，很有可能是服务器开始出现瓶颈 Hits per Second（每秒点击次数）通过对查看“每秒点击次数”，可以判断系统是否稳定。系统点击率下降通常表明服务器的响应速度在变慢，需进一步分析，发现系统瓶颈所在。 Throughput（吞吐率）可以依据服务器的吞吐量来评估虚拟用户产生的负载量，以及看出服务器在流量方面的处理能力以及是否存在瓶颈。 Connections（连接数）当连接数到达稳定状态而事务响应时间迅速增大时，添加连接可以使性能得到极大提高（事务响应时间将降低） Time to First Buffer Breakdown（Over Time）（第一次缓冲时间细分（随时间变化））可以使用该图确定场景或会话步骤运行期间服务器或网络出现问题的时间。 碰到过的性能问题： 在高并发的情况下，产生的处理失败（比如：数据库连接池过低，服务器连接数超过上限，数据库锁控制考虑不足等） 内存泄露（比如：在长时间运行下，内存没有正常释放，发生宕机等） CPU使用偏离（比如：高并发导致CPU使用率过高） 日志打印过多，服务器无硬盘空间 如何定位这些性能问题： 查看系统日志，日志是定位问题的不二法宝，如果日志记录的全面，很容易通过日志发现问题。 比如，系统宕机时，系统日志打印了某方法执行时抛出out of memory的错误，我们就可以顺藤摸瓜，很快定位到导致内存溢出的问题在哪里。 利用性能监控工具，比如：JAVA开发B/S结构的项目，可以通过JDK自带的Jconsole，或者JProfiler，来监控服务器性能，Jconsole可以远程监控服务器的CPU，内存，线程等状态，并绘制变化曲线图。 利用Spotlight可以监控数据库使用情况。 我们需要关注的性能点有：CPU负载，内存使用率，网络I/O等 工具和日志只是手段，除此之外，还需要设计合理的性能测试场景 具体场景有：性能测试，负载测试，压力测试，稳定性测试，浪涌测试等 好的测试场景，能更加快速的发现瓶颈，定位瓶颈 了解系统参数配置，可以进行后期的性能调优 最后要说的是：做性能测试的时候，我们一定要确保瓶颈不要发生在我们自己的测试脚本和测试工具上。","categories":[{"name":"测试理论","slug":"测试理论","permalink":"http://wysh.site/categories/测试理论/"},{"name":"性能测试","slug":"测试理论/性能测试","permalink":"http://wysh.site/categories/测试理论/性能测试/"}],"tags":[{"name":"性能测试","slug":"性能测试","permalink":"http://wysh.site/tags/性能测试/"}]},{"title":"软常见的性能测试方法","slug":"常见的性能测试方法","date":"2016-04-25T01:46:06.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/04/25/常见的性能测试方法/","link":"","permalink":"http://wysh.site/2016/04/25/常见的性能测试方法/","excerpt":"","text":"1．负载测试在这里，负载测试指的是最常见的验证一般性能需求而进行的性能测试，在上面我们提到了用户最常见的性能需求就是“既要马儿跑，又要马儿少吃草”。因此负载测试主要是考察软件系统在既定负载下的性能表现。我们对负载测试可以有如下理解： （1）负载测试是站在用户的角度去观察在一定条件下软件系统的性能表现。 （2）负载测试的预期结果是用户的性能需求得到满足。此指标一般体现为响应时间、交易容量、并发容量、资源使用率等。 2．压力测试压力测试是为了考察系统在极端条件下的表现，极端条件可以是超负荷的交易量和并发用户数。注意，这个极端条件并不一定是用户的性能需求，可能要远远高于用户的性能需求。可以这样理解，压力测试和负载测试不同的是，压力测试的预期结果就是系统出现问题，而我们要考察的是系统处理问题的方式。比如说，我们期待一个系统在面临压力的情况下能够保持稳定，处理速度可以变慢，但不能系统崩溃。因此，压力测试是能让我们识别系统的弱点和在极限负载下程序将如何运行。例子：负载测试关心的是用户规则和需求，压力测试关心的是软件系统本身。对于它们的区别，我们可以用华山论剑的例子来更加形象地描述一下。如果把郭靖看做被测试对象，那么压力测试就像是郭靖和已经走火入魔的欧阳峰过招，欧阳锋蛮打乱来，毫无套路，尽可能地去打倒对方。郭靖要能应对住，并且不能丢进小命。而常规性能测试就好比郭靖和黄药师、洪七公三人约定，只要郭靖能分别接两位高手一百招，郭靖就算胜。至于三百招后哪怕郭靖会输掉那也不用管了。他只要能做到接下一百招，就算通过。 思考 我们在做软件压力测试时，往往要增加比负载测试更多的并发用户和交易，这是为什么？ 3. 并发测试验证系统的并发处理能力。一般是和服务器端建立大量的并发连接，通过客户端的响应时间和服务器端的性能监测情况来判断系统是否达到了既定的并发能力指标。负载测试往往就会使用并发来创造负载，之所以把并发测试单独提出来，是因为并发测试往往涉及服务器的并发容量，以及多进程/多线程协调同步可能带来的问题。这是要特别注意，必须测试的。 4．基准测试当软件系统中增加一个新的模块的时候，需要做基准测试，以判断新模块对整个软件系统的性能影响。按照基准测试的方法，需要打开/关闭新模块至少各做一次测试。关闭模块之前的系统各个性能指标记下来作为基准（Benchmark），然后与打开模块状态下的系统性能指标作比较，以判断模块对系统性能的影响。 5. 稳定性测试“路遥知马力”，在这里我们要说的是和性能测试有关的稳定性测试，即测试系统在一定负载下运行长时间后是否会发生问题。软件系统的有些问题是不能一下子就暴露出来的，或者说是需要时间积累才能达到能够度量的程度。为什么会需要这样的测试呢？因为有些软件的问题只有在运行一天或一个星期甚至更长的时间才会暴露。这种问题一般是程序占用资源却不能及时释放而引起的。比如，内存泄漏问题就是经过一段时间积累才会慢慢变得显著，在运行初期却很难检测出来；还有客户端和服务器在负载运行一段时间后，建立了大量的连接通路，却不能有效地复用或及时释放。 6. 可恢复测试测试系统能否快速地从错误状态中恢复到正常状态。比如，在一个配有负载均衡的系统中，主机承受了压力无法正常工作后，备份机是否能够快速地接管负载。可恢复测试通常结合压力测试一起来做。 提示：每种测试有其存在的空间和目的。当我们接手一个软件项目后，在有限的资源条件下，选择去做哪一种测试，这应该根据当前软件过程阶段和项目的本身特点来做选择。比如，在集成测试的时候要做基准测试，在软件产品每个发布点要做性能测试。","categories":[{"name":"测试理论","slug":"测试理论","permalink":"http://wysh.site/categories/测试理论/"},{"name":"性能测试","slug":"测试理论/性能测试","permalink":"http://wysh.site/categories/测试理论/性能测试/"}],"tags":[{"name":"性能测试","slug":"性能测试","permalink":"http://wysh.site/tags/性能测试/"}]},{"title":"软件测试面试题 - 怎么测电梯？（测杯子/椅子/ 雨伞/电话）","slug":"软件测试面试题 - 怎么测电梯（测杯子 椅子 雨伞 电话）","date":"2016-03-30T11:57:16.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/03/30/软件测试面试题 - 怎么测电梯（测杯子 椅子 雨伞 电话）/","link":"","permalink":"http://wysh.site/2016/03/30/软件测试面试题 - 怎么测电梯（测杯子 椅子 雨伞 电话）/","excerpt":"","text":"1. 破题： 问题是：怎么测电梯 前提条件是：这是一道软件测试工程师面试题，而非真正的电梯测试人员的面试题 第二个前提：我没有需求文档，但我了解电梯的基本业务功能 思路：把电梯当作一个我了解基本业务功能，却没有需求文档的软件来进行测试。也就是说这里考察两点: 第一，你能不能测没有需求文档，或者需求文档不完整的东西 第二，你能不能把测试用例设计方法应用到实际工作上去 还隐含第三点，你的测试思维是否完整，测试范围能想得比较全面吗。 2. 确定测试范围以下是黑盒角度的: 功能：关注电梯的基本功能是否实现 性能：关注电梯的性能指标，如负重多少kg 安全性：关注电梯的安全性，如超重报警，下坠制动 用户体验：关注电梯的舒适性 以下是白盒角度的或其他的: 效率：关注电梯控制逻辑的内部算法 接口：电梯和电梯控制器，电梯和大楼，电梯和摄像头，电梯和对讲机（报警装置）的接口测试 零件：电梯的零件的单元测试 兼容性：电梯和其他东西的兼容性 3. 具体测试用例的设计3.1 功能测试： 思路一：基于用户界面，如按钮，分电梯内的按钮和电梯外的按钮；电梯内分楼层键、开关门键、报警键。然后对这些键，一个一个测过来。同时关注显示屏，电梯内外的显示屏均显示电梯当前所在楼层和运行方向。 思路一就是典型的单元测试。 思路二：单个功能测好之后，再把单个的功能组合起来进行测试（集成测试），集成测试时可以根据电梯当前状态是上行、下行还是停止（状态机）来设计测试用例，以保证覆盖率。比如上行时按XX按钮会怎么样。此时可以向面试官提出等价类划分思想，为何我要测这些按钮，如何划分等价类。 思路三：集成测试完毕后，开始测试真实用户场景（确认测试/验收测试/工作流测试），此时可以设计常见的用户场景（场景设计）并进行测试。如大量用户从1楼进入，并去不同楼层。又或者大量用户从不同楼层下到1楼。 思路四：不同品牌电梯的比较，电梯和电梯国际标准的比较，电梯和安装电梯的大楼用户需求的比较等等思路五：特殊需求的测试，如摩天大楼可能要求高速电梯。百货大楼可能要求观光电梯。 3.2 性能测试： 思路一：测试电梯负载单人时的运行情况（基准测试）、多人时的运行情况（负载测试）、一定人数下较长时间的运作（稳定性测试）、更长时间运作时的运行情况（疲劳测试）、不断增加人数导致电梯报警（拐点压力测试） 思路二：不同层次的性能，如零部件性能等 3.3 安全性测试：软件的安全性测试我也不了解。只能瞎说了。比如，暴力破坏电梯，下坠制动测试，超重警报、超时警报的测试，报警功能的测试，监控摄像头测试，火灾时应该不让用户使用，但又要让里面的人能出来等等。 3.4 用户体验：电梯是否有地毯，夏天是否有空调，通风条件，照明条件。等等 3.5 效率：调度算法是否合理，是否最优，按错键是否可以取消3.6 零件: 零部件是否合格3.7 接口：电梯和其他设备的交互，如报警装置、中央空调、监控室等等如何交互，是否工作正常3.8 兼容性：电梯的整体和其他设备的兼容性以上，是建议的答案。一般把整体思路说一下，再把3.1功能测试部分重点讲一讲就ok了，面试官应该会满意的。如果把电梯换成电话，测试思路还是这个，顶多就是换一些具体用例。或者电梯换成其他任何东西都一样的，关键是，把它当作软件，展示测试思维。","categories":[{"name":"测试理论","slug":"测试理论","permalink":"http://wysh.site/categories/测试理论/"},{"name":"用例设计","slug":"测试理论/用例设计","permalink":"http://wysh.site/categories/测试理论/用例设计/"}],"tags":[{"name":"用例设计, 测试理论","slug":"用例设计-测试理论","permalink":"http://wysh.site/tags/用例设计-测试理论/"}]},{"title":"记一次性能测试实践","slug":"记一次性能测试实践","date":"2016-03-21T03:18:45.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/03/21/记一次性能测试实践/","link":"","permalink":"http://wysh.site/2016/03/21/记一次性能测试实践/","excerpt":"","text":"1. 测试对象这次测了一些http接口和几个网页。 2. 测试策略 2.1 基准测试：单个调用各接口循环100次计算平均响应时间 2.2 性能测试：单个接口调用以50并发用户数为单位，逐步加压直到预估的实际负载300并发用户，观察测试指标变化 2.3 压力测试：单个接口调用以50并发用户数为单位，逐步加压直到错误率过高或服务器资源使用率过高，观察测试指标变化 2.4 负载测试：预估实际负载为300并发用户数，在此基础上持续测试5分钟左右，观察测试指标是否达标 2.5 稳定性测试：预估实际负载为300并发用户数，在此基础上持续测试60分钟左右，观察测试指标是否达标，重点观察错误率 2.6 疲劳性测试：预估实际负载为300并发用户数，在此基础上持续测试240分钟左右，观察测试指标是否达标，重点观察错误率 2.7 组合测试：对2.2-2.5的测试采用不同接口同时调用（即系统不同模块同时测试） 2.8 其他：以不同ip地址加压，测试服务器负载均衡效果。以上，本次只做了2.2、2.3、2.4、2.8 3. 测试指标 测响应时间、错误率；同时专人监控服务器硬件资源使用状况、监控tomcat应用服务器等。 计算和监控吞吐量（测试工具自动计算测试执行过程中的吞吐量（每秒钟处理请求数），同时服务器监控软件业监控到了测试执行时服务器的吞吐量） 本次实际测试得到吞吐量距离预估有较大差距；错误率超出预期；且测试数据准备有一定问题。 4. 测试工具 本次选用Jmeter，因为便宜且灵活。 需设置语言为英文，默认中文翻译不完整。 5. 测试脚本编写、调试 5.1 提前对接口、网页进行录制。每个待测接口、网页需要加断言。 断言多采用JQuery断言和Regular Expression断言 5.2 重点在测试数据的准备。 5.3 采用了本地web应用提供数据，jmeter获取这些数据，再发送给服务器的方法（这次发现这个本地应用生成的数据在较高并发时有重复，导致了不必要的错误率） 5.4 测试结果监听器： assertion results, summary report, aggregate report, result tree, result table 5.5 测试接口调用时，可用网页、数据库等其他方法确认接口调用成功。观察接口调用是否生效，是否和网页同样效果。 6. 测试执行 6.1 一台电脑加压300-600并发用户。如果需要更多则需要增加电脑。 6.2 以不同ip地址加压，测试服务器负载均衡效果。 6.3 机房测试，排除internet网络延迟问题 6.4 数据备份和还原，排除性能测试对数据的改变 6.5 生产环境测试（系统未上线），排除测试环境的影响 7. 测试报告 7.1 截取了jmeter监听器的结果，可以截取服务器监控的截图 8. 调优 本次测试结果不理想，服务器因硬件强大，几乎无负载，但应用本身有java出错。并发现接口调用结果未正确影响网页的bug。 后续需要等开发修复、优化之后再次测试","categories":[{"name":"Web测试","slug":"Web测试","permalink":"http://wysh.site/categories/Web测试/"},{"name":"性能测试","slug":"Web测试/性能测试","permalink":"http://wysh.site/categories/Web测试/性能测试/"}],"tags":[{"name":"性能测试","slug":"性能测试","permalink":"http://wysh.site/tags/性能测试/"}]},{"title":"移动APP测试用例设计的关注点","slug":"移动APP测试用例设计的关注点","date":"2016-02-28T05:49:37.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/02/28/移动APP测试用例设计的关注点/","link":"","permalink":"http://wysh.site/2016/02/28/移动APP测试用例设计的关注点/","excerpt":"","text":"在我们的测试工作中，对于某个APP的测试其实有很多东西都是类似的可以抽象出来的，这里总结一下大部分APP测试的时候都要考虑到的方面。如果漏下了其他方面，欢迎大家补充。1 应用的启动和停止1.1 首次启动 是否出现欢迎界面，欢迎界面的停留时间合理，欢迎界面后是否正常进入应用; 首次启动时间是否合理; 该拉取的信息是否正确; 桌面图标是否创建成功，功能启动快捷键创建是否成功（某些安卓手机会有在桌面创建应用内某个功能的快捷键的需求） 1.2 二次启动 启动时间是否符合预期； 从各个启动入口进入应用是否可以正常进入：程序启动主图标，某个功能的快捷键，widget； 启动后状态检查：如初始化信息、初始状态、启动对网络 启动进程服务检查：进程名、进程数、服务名、服务数、第三方调用的SDK如GPS 带登陆的应用是否二次启动的时候正常登录 1.3 程序异常退出后的启动 操作出现crash后再启动：如空指针、内存溢出等 手动停止进程：多进程的情况停止所有或者停止其中一个后重启 手动停止服务：多服务的情况，停止所有或者停止部分服务后，未重启直接使用 管家软件一键清理进程后重启 其他系统软件工具停止进程、清理软件数据 2 程序功能模块 这个一般是根据需求来对应用的所有模块所以功能的触发事件逐一验证。这个最基本的要从两个方面考察，一方面是顺从需求来对模块进行操作，是否达到需求规定的预期；另一方面就是与需求背道而驰是否程序会有相应异常控制等等。廖叔提出了Google正在使用的测试建模的概念，这个方法可以可以帮助我们更好的结合需求分析应用的架构，设计更完善的功能模块用例。 2.1 文本框输入功能正常输入，输入越界，特殊字符集(\\n,\\r等等),利用复制粘贴向文本输入内容，输入程序规定不让输入的字符 2.2 事件触发 每一个按钮、每一个可点击项是否能够完成需求规定的功能 尝试点击页面上不可点击的区域，来验证在测试过程当中的预留测试后门是否关闭 3 权限安全 需要用户确认的权限没有授权，权限默认关闭 联网权限被管家、系统安全类软件限制情况下的联网操作 权限敏感度，如通讯录等为系统的绝密权限谨慎获取 使用安全软件进行安全漏洞、病毒扫描，看被测APP是否会被这些安全软件提示有问题而影响用户的对被测APP的使用或者印象 4 文件存储 APP使用过程中产生的临时文件存储路径、命名方式等 APP中涉及的下载操作产生的文件存储方式 存储的文件被锁、占用 有外置SD、内置SD卡都要考察APP产生的文件是否正确 APP被安装在SD卡或者手机存储空间 磁盘空间不足、磁盘无权限（如读、写） 5 网络与流量 网络信号，尤其是弱网络环境下应用的表现 不同运营商网络：电信、联通、移动，2G/3G/4G 网络中断、网络恢复场景的逻辑处理（如重试），以及网络提示 首次启动应用的流量是否符合预期 统计、异常上报对流量的影响 APP中图片大小、尺寸是否有考虑对网络流量的影响 基于流量安全的特殊业务，如仅wifi联网 6 接口容错 请求网络层错误：http response返回非200的状态 请求业务层错误：接口返回内容为空、超长、字段类型不匹配 7 中断测试 锁屏中断：停留在程序操作界面进行锁屏，恢复后检查操作是否正常 前后台切换：停留在程序操作界面，通过Home键，进行程序的前后台切换 加载中断：页面接口请求、界面框架加载时，通过Home键、返回键、快速切换操作进行中断 系统异常中断：如关机、断电、来电 8 机型适配8.1 分辨率适配UI结构、对话框基于分辨率、屏幕大小进行适配 8.2 OS版本适配涉及API调用如获取SIM卡信息、外置SD卡设置（4.4外置SD卡不具备写的权限） 8.3 CPU硬件配置X86机型、V5、V6、V7、V8 9 系统配置 进程管理：省电管理、后台进程驻留管理 显示管理：字体大小、字体类型 语言环境：语言环境 横竖屏配置：是否支持横竖屏自适应处理 10 升级 覆盖安装 逐步升级：用户数据、设置、状态的保留，特步注意新版本已去掉的状态或设置 跳级：即隔开版本覆盖安装 降级：覆盖安装更低版本 卸载安装，安装目录清理，SD卡存储数据不被清理 省流量升级：有些助手提供省流量升级的方式 在没有更新或者网络时，需要给予用户正确的信息表达 如果升级有忽略本次版本升级，那么当有新的升级版本时，是否还有提示升级 强制升级 不升级无法使用 11 性能测试11.1 性能核心操作的性能指标：如CPU/内存、响应时长、电量、流量 11.2 稳定性 选择某些场景做持续反复操作 Monkey稳定性操作，持续多个小时 11.3 流畅度列表滑动、返回进入、快速点击（这个肉眼不好评判，可以借助GT，一般打分在90分以上是比较好的） 11.4 软件兼容 通用软件 输入法 安全软件 通信类 竞品软件 同类软件，是否出现冲突 12 竞品对比测试 功能方面：与同类竞品软件在UI设计，交互体验等方面进行对比 性能方面：同类竞品软件在性能、耗电、流量等方面至少与对方持平，最好不要低于对方太多","categories":[{"name":"APP测试","slug":"APP测试","permalink":"http://wysh.site/categories/APP测试/"},{"name":"用例设计","slug":"APP测试/用例设计","permalink":"http://wysh.site/categories/APP测试/用例设计/"}],"tags":[{"name":"用例设计","slug":"用例设计","permalink":"http://wysh.site/tags/用例设计/"}]},{"title":"完美组合:用例精简+精准测试","slug":"完美组合用例精简精准测试","date":"2016-02-23T09:36:29.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/02/23/完美组合用例精简精准测试/","link":"","permalink":"http://wysh.site/2016/02/23/完美组合用例精简精准测试/","excerpt":"","text":"一、 为什么要做用例精简和精准测试1、 测试用例越来越多，测试效率低下这是因为在目前的快速迭代开发模式下，测试人员需要不停覆盖不断调整的产品逻辑需求，因此测试用例也越来越庞大了，以病毒查杀为例，目前用例已达500多条用例，导致全量测试时间很长，同时发现的问题并不和用例数成正比 2、 以往迭代测试用例更多是功能“点”的覆盖，而不是用户场景“线”、“面”的覆盖目前产品经理给出的需求都是增量文档，也就是很难有某个产品的完整需求文档，因此，每次用例更多是功能点的覆盖，比如需求文档里面提到点击某个按钮会有什么变化，那某次用例编写时可能只是简单的覆盖，但这种用例并不完全符合用户实际场景，因此还是可能出现覆盖不完全问题。 3、 用例精简是精准测试的基础之一精准测试的本质是在有代码变更时可以快速并精确地挑选出所影响的用例，在不影响质量的同时降低工作成本，理论上精准测试已经可以提高工作效率，但如果同时再加上精简后的用例，那就可以在精准测试的基础上再次降低工作成本。 因此用例精简可以是精准测试的基础之一。 4、 用例精简降低用例执行的多次投入成本测试中的成本按其时间跨度可以分为：单次投入成本和多次投入成本。例如：编写测试用例可以看作是单次投入成本，因为编写测试用例一般是在测试的计划阶段进行（Scrum每个Sprint的开始阶段）的，虽然后期会有小的改动，但绝大多数是在一开始的设计阶段就基本上成型了；测试用例（包括：手工和自动化测试用例）的执行则是多次投入成本，因为每出一个新版本Build时都要执行所有的测试用例。 因此，我们虽然增加了用例精简的单次投入成本，但总体降低了后续用例执行时的多次投入成本降低。 二、 用例精简原则概要用例精简本质是，用更少的用例发现相同或更多的问题。因此，精简原则重点是： 从用户场景出发，合并、删除功能点用例，从“线”、“面”整体考虑用户场景，对已有用例进行精简和重构。 首先，先筛选出正常跟异常用例。异常用例是对测试的补充，也是探索性测试在用例方面的表现，所以对异常类的用例精简应跟正常用例进行结合，这部分下面会提到精简方案： 场景原则精简用例： 假如有一个功能A，有三个子功能A1，A2。A2的测试路径基于A1的功能操作，那么就用TestA1_A2来贯穿这个测试用例。每一个测试动作都是一个点，但是把这些测试要点结合起来连成一条线，甚至是面的覆盖，这就是一个测试场景。测试场景可以覆盖的面是比较广的，也可以较好的模范用户操作行为。 对需求的理解精简用例： 在最开始的时候，我们写用例都是根据需求文档的内容进行场景划分，用例编写。当需求越来越完善的时候，我们需要及时地精简我们的用例，明确需求是什么，场景是什么。将需求转化为对用户场景的模拟，实际的去重构用例。例如：第一阶段：当产品出了需求，有AB两个模块，A能调用起B模块。这个时候用例可能就关注功能需求，A调用B。第二阶段，需求二：加入C模块，A模块能调用起C模块；第三阶段，加入D模块，A能调用起D模块…。这个时候我们要关注的不再是需求功能，而是转化为对需求的理解：A模块能够成功调用其他模块的所有内容。 对比手段精简用例： 一开始我们把用例分为异常类型跟正常类型的用例之后，就是为了在对比测试这里用到。假如正常类型用例A，异常类型用例A+，当我们把执行顺序是TestA+, A的时候，就是一种对比类型的精简用例方法。 合并、降低缺陷出现率低的用例优先级原则： 按照无线的测试指南，正常逻辑的用例应该标为一级用例，同时占比不超过30%，但是在实际测试工作中我们可能需要再进行细化优先级，比如病毒查杀有500多条用例，那么理论上一级用例就多于150条。如果把这150条作为上线前测试的话，一个下午光做病毒模块的上线前都没法做完。 同时，虽然一级用例都是正常逻辑，但是从测试数据、用户反馈数据来看，功能模块的缺陷可能只集中在几个场景中，因此我们需要重视这几个场景，把相关的用例精简重构作为上线前用例，合并或降低缺陷出现率低的一级用例。 补充用户反馈跟缺陷的bug补充用例： 软件开发中有个80：20的原则，80%的效果是由20%的原因导致的，或者是80%的结果来自于20%的努力。同理，80%的缺陷主要集中在20%的代码中，90%的停机是由10%（甚至更少）的缺陷造成的。 所以，注重用户反馈，注重缺陷的场景，显得特别的重要。这部分看起来虽然不在精简范围内，但是这部分却应该是在用例设计补充里面。精简的用例要能做到很好的覆盖，有更好的效果，就需要重视缺陷的集中地带。同时用户真实的场景跟需求的场景也是可以在这里得到反映。 三、 用例精简实践根据上面的几个原则，我们抽取了广告拦截模块用例进行精简实践。首先我们先对以往缺陷的集中点进行了梳理，随后按照以上几个原则进行了精简和重构： 精简效果反馈： 力度：精简前的用例数据达到了223条用例，经过精简之后，目前用例数目只剩150条用例，减少了73条用例。 时间：73条用例一般需要1天/人去执行，这对效率上面是有很大提升。 四、 用例精简效果粗略验证这里只提到粗略验证，是因为我们认为用例精简有效果验证有两种方式：功能点检查和全量覆盖率结果，功能点的完善性可以通过内部和外部评审实现，但比较耗时，也可以通过标注新旧用例之间的对应关系来确保精简后用例覆盖的功能点不遗漏。 全量覆盖率验证，主要是使用同一个版本测试包对精简前后用例进行覆盖率测试。目前覆盖率测试有行覆盖、方法覆盖、块覆盖，行覆盖率相对方法覆盖精度比较高，虽然不能百分之百的精确，但已经可以作为基础的衡量标准，因此我们抽取了部分精简后的用例进行覆盖率测试。 五、 精准测试方案和实践上面提到用例精简是精准测试的基础之一，因此在用例精简之后我们也做了精准测试的实践。 精准测试的本质是，代码有变更时用更少的用例去覆盖变更涉及的功能点，以达到提高测试效率的目的。 代码变更到用例挑选，这就很自然地引出一个问题，那就是代码变更时如何快速找到相应的测试用例，目前解决这问题的思路有两种，一种是通过阅读代码从而梳理出软件架构，这种方式的输出是代码文件和功能逻辑的对应关系，当代码有变更时找到对应功能逻辑从而挑选出需要执行的用例，这种思路是一次投入长期收益，但比较耗时，而且要求测试同学的代码功底比较强，同时需要开发同学的帮忙；另一种是执行某个功能模块用例集时输出代码日志，从代码日志里面也可以找到功能模块的对应关系，这种方案对测试同学的代码能力要求不高，比较适合外包同学的执行，但也比较耗时，同时需要不停维护对应关系库。 同时，版本间的变更识别也有两种思路，一种是成都同学的查看SVN日志，再从日志详细代码中识别出变更逻辑，找到对应用例；还有一种是通过SVN diff，直接从版本间的SVN diff来获取代码变更；这两种方法的思路是想通的，都是利用SVN来找到版本间的差异，但是相比SVN日志，SVN diff结果没有那么直观，只看到差异部分，而且差异部分不一定能够看到前后代码，只有零散的增删改，没有整体概念，这是因为SVN diff结果只会对有增删改的代码往前追溯7行，比如以下变更代码就很难看出是在哪个功能点有变更： 我们的方案和以上两种思路不完全相同，但也采用SVN diff作为技术基础，需要对SVN diff结果再做处理，我们采用类似迭代的思路去逐步求精，也就是开始选择用例时是根据经验尽可能找到对应的用例集，在执行结束时根据此轮覆盖率情况再决定是否需要再挑选更小的用例集。这思路相比预先阅读全部代码和通过日志方案更敏捷，也无需对软件代码细节非常了解，如果结合开发同学的讲解，效率会更高，对架构理解更深。 在给出具体的实践之前，也插播下精准测试的背景：作为测试人员，在版本提测时你是否也会经常收到开发这样的信息，那就是开发人员说某个功能没什么大改动，简单过下就好了，但是就这么一句话测试人员可能就要忙半天了，具体改啥了？“简单过下”到什么程度？这些都是头痛的问题，比如广告拦截在手机管家5.1版本中有变更，开发的提测建议如下： 广告拦截除了第一点有涉及之外，其他变更内容都浓缩在第四点里面，对这样的一句话你放心吗？作为测试人员怀疑精神是要有的，因此我们要想既保证质量又不做无用功就得考虑精准测试了。 我们做了个比较粗糙展示系统，这系统可以实践我们“迭代的思路去逐步求精”想法： 1、 分析版本间变更内容变更内容的基础是SVN diff,但是因为SVN diff原始结果不直观，因此我们需要对结果再进行解析处理，处理后可以在源码中用不同颜色标明变更内容，而不是直接的SVN diff结果。 初次结果报告覆盖率基本上为0： 2、 根据源码中的变更，判断大概变更涉及模块，挑选出此轮用例。根据代码变更内容(快速的办法是找开发同学一起过下变更内容)，我们可以得知5.1和5.0版本之间广告拦截主要有以下功能点变更： 监测到系统通知栏新消息 调用临时root失败 卸载安装包时清除通知栏消息； 启动通知栏监控； 获取广告行为表 修改广告软件配置 广告详情页面展示 广告页面刷新 由此可以看到通过查看变更代码可以了解到需要覆盖的测试点 3、 执行此轮用例，利用覆盖率执行结果ec文件再次生成用例执行覆盖情况。4、 根据此轮用例执行覆盖率情况，再决定是否需要做下一轮的用例挑选。根据以上粗略挑选用例，发现覆盖率仍比较偏低，因此通过查看具体代码再增加测试用例覆盖，逐步提高覆盖率： 5、 最终完成变更的覆盖，实现精准测试的目的。最终逐步挑选出约50条用例覆这两个版本间的实际代码变更，不再盲目测试。由于变更代码中有大概10%的保护代码，因此90%的行覆盖率已经比较理想了。同时，由于测试人员逐步分析变更代码，也使得大家对广告拦截的代码逻辑更清晰，增加了发布质量的信心。","categories":[{"name":"测试理论","slug":"测试理论","permalink":"http://wysh.site/categories/测试理论/"},{"name":"用例设计","slug":"测试理论/用例设计","permalink":"http://wysh.site/categories/测试理论/用例设计/"}],"tags":[{"name":"用例设计","slug":"用例设计","permalink":"http://wysh.site/tags/用例设计/"}]},{"title":"负载测试、压力测试和性能测试的区别","slug":"负载测试压力测试和性能测试的区别","date":"2016-02-18T07:14:54.000Z","updated":"2018-09-16T18:35:02.000Z","comments":true,"path":"2016/02/18/负载测试压力测试和性能测试的区别/","link":"","permalink":"http://wysh.site/2016/02/18/负载测试压力测试和性能测试的区别/","excerpt":"","text":"负载测试（Load testing）、压力测试（Stress Test，应称为强度测试）和性能测试，这三个概念常常引起混淆，难以区分，从而造成不正确的理解和使用。 目前对性能测试没有明确的定义，一般地，它主要是针对系统的性能指标制定性能测试方案，执行测试用例，得出测试结果来验证系统的性能指标是否满足既定值。性能指标里可能包括系统各个方面的能力，如系统并发处理能力，批量业务处理能力等。 负载测试、压力测试和性能测试的测试目的不同，但其手段和方法在一定程度上比较相似，通常会使用相同的测试环境和测试工具，而且都会监控系统所占用资源的情况以及其它相应的性能指标，这也是造成人们容易产生概念混淆的主要原因。 我们知道，软件总是运行在一定的环境下，这种环境包括支撑软件运行的软硬件环境和影响软件运行的外部条件。为了让客户使用软件系统感到满意，必须确保系统 运行良好，达到高安全、高可靠和高性能。其中，系统是否具有高性能的运行特征，不仅取决于系统本身的设计和程序算法，而且取决于系统的运行环境。系统的运 行环境会依赖于一些关键因素，例如： 系统架构，如分布式服务器集群还是集中式主机系统等。 硬件配置，如服务器的配置，CPU、内存等配置越高，系统的性能会越好。 网络带宽，随着带宽的提高，客户端访问服务器的速度会有较大的改善。 支撑软件的选定，如选定不同的数据库管理系统（Oracle、MySQL等）和web应用服务器（Tomcat、GlassFish、Jboss、WebLogic等），对应用系统的性能都有影响。 外部负载，同时有多少个用户连接、用户上载文件大小、数据库中的记录数等都会对系统的性能有影响。一般来说，系统负载越大，系统的性能会降低。 从上面可以看出，使系统的性能达到一个最好的状态，不仅通过对处在特定环境下的系统进行测试以完成相关的验证，而且往往要根据测试的结果，对系统的设计、代码和配置等进行调整，提高系统的性能。许多时候，系统性能的改善是测试、调整、再测试、再调整、……一个持续改进的过程，这就是我们经常说的性能调优 （perormance tuning）。 在了解了这样一个背景之后，就比较容易理解为什么在性能测试中常常要谈负载测试。从测试的目的出发、从用户的需求出发，就比较容易区分性能测试、负载测试和压力测试。性能测试是为了获得系统在某种特定的条件下（包括特定的负载条件下）的性能指标数据，而负载测试、压力测试是为了发现软件系统中所存在的问题，包括性能瓶颈、内存泄漏等。通过负载测试，也是为了获得系统正常工作时所能承受的最大负载，这时负载测试就成为容量测试。通过压力测试，可以知道在什么极限情况下系统会崩溃、系统是否具有自我恢复性等，但更多的是为了确定系统的稳定性。 那么，如何给负载测试、压力测试下个定义呢？根据上述讨论，我们可以给出如下的定义： 负载测试是模拟实际软件系统所承受的负载条件的系统负荷，通过不断加载（如逐渐增加模拟用户的数量）或其它加载方式来观察不同负载下系统的响应时间和数据吞吐量、系 统占用的资源（如CPU、内存）等，以检验系统的行为和特性，以发现系统可能存在的性能瓶颈、内存泄漏、不能实时同步等问题。负载测试更多地体现了一种方 法或一种技术。 压力测试是在强负载（大数据量、大量并发用户等）下的测试，查看应用系统在峰值使用情况下操作行为，从而有效地发现系统的某项功能隐患、系统是否具有良好的容错能力 和可恢复能力。压力测试分为高负载下的长时间（如24小时以上）的稳定性压力测试和极限负载情况下导致系统崩溃的破坏性压力测试。 负载测试及压力测试特点： 性能测试方法通过模拟生产运行的业务压力量和使用场景组合测试性能是否能够满足需要。具备三个特点： 这种方法的目的是验证系统是否具有系统宣称具有的能力。 这种方法需要事先了解被测试系统典型场景、并确定性能目标。 这种方法要求在已确定的环境下运行 负载测试用来测定系统饱和状态、确定阀值。其特点有： 这种方法的目的是找到系统处理能力的极限；通过“检测、加压、阀值”手段找到如“响应时间不超过10秒”，“服务器平均CPU利用率低于65%”等指标。 这种性能测试方法需要在给定的测试环境下进行，通常也需要考虑被测系统的业务压力量和典型场景、另外HP Mercury LoadRuner在使用该方法进行“加压”的时候必须选择典型场景。 这种性能测试方法一般用来了解系统的性能容量，或者是配合性能调优的时候来使用。 压力测试方法测试目标系统在一定饱和状态下，例如CPU、内存等在饱和状态下、系统能够处理的session的能力，以及系统是否会出现错误。该方法需要在系统cache调优与pool优化方面着手。该方法具备以下特点： 该方法的目的是检查系统处于压力情况下的，应用的表现。如增加VU数量、节点数量、并发用户数量等使应用系统的资源使用保持一定的水平，这种方法的主要目的是检验此时的应用表现，重点在于有无错误信息产生，系统对应用的响应时间等。 该方法通过模拟负载在实现压力。这种模拟需要考虑的层面很多、首先、模拟必须是有效的，我的经验是需要结合业务系统和软件架构来定制模拟指标、我 测试过一些国内生产的压力测试工具、他们使用通用的指标来考量、造成很多信息反馈有很大的水分。需要考虑的层面如：Oracle I/O、JVM GC、Conn Pool等。 该方法还可以测试系统的稳定性。这里的技巧在于“什么样的平台定义一个多长的压力测试时间让其稳定运行才是科学的？”","categories":[{"name":"测试理论","slug":"测试理论","permalink":"http://wysh.site/categories/测试理论/"},{"name":"性能测试","slug":"测试理论/性能测试","permalink":"http://wysh.site/categories/测试理论/性能测试/"}],"tags":[{"name":"性能测试","slug":"性能测试","permalink":"http://wysh.site/tags/性能测试/"}]}]}